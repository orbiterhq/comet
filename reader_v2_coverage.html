
<!DOCTYPE html>
<html>
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
		<title>comet: Go Coverage Report</title>
		<style>
			body {
				background: black;
				color: rgb(80, 80, 80);
			}
			body, pre, #legend span {
				font-family: Menlo, monospace;
				font-weight: bold;
			}
			#topbar {
				background: black;
				position: fixed;
				top: 0; left: 0; right: 0;
				height: 42px;
				border-bottom: 1px solid rgb(80, 80, 80);
			}
			#content {
				margin-top: 50px;
			}
			#nav, #legend {
				float: left;
				margin-left: 10px;
			}
			#legend {
				margin-top: 12px;
			}
			#nav {
				margin-top: 10px;
			}
			#legend span {
				margin: 0 5px;
			}
			.cov0 { color: rgb(192, 0, 0) }
.cov1 { color: rgb(128, 128, 128) }
.cov2 { color: rgb(116, 140, 131) }
.cov3 { color: rgb(104, 152, 134) }
.cov4 { color: rgb(92, 164, 137) }
.cov5 { color: rgb(80, 176, 140) }
.cov6 { color: rgb(68, 188, 143) }
.cov7 { color: rgb(56, 200, 146) }
.cov8 { color: rgb(44, 212, 149) }
.cov9 { color: rgb(32, 224, 152) }
.cov10 { color: rgb(20, 236, 155) }

		</style>
	</head>
	<body>
		<div id="topbar">
			<div id="nav">
				<select id="files">
				
				<option value="file0">github.com/orbiterhq/comet/client.go (23.7%)</option>
				
				<option value="file1">github.com/orbiterhq/comet/consumer.go (0.0%)</option>
				
				<option value="file2">github.com/orbiterhq/comet/debug.go (50.0%)</option>
				
				<option value="file3">github.com/orbiterhq/comet/index_binary.go (0.0%)</option>
				
				<option value="file4">github.com/orbiterhq/comet/logger.go (18.2%)</option>
				
				<option value="file5">github.com/orbiterhq/comet/metrics_interface.go (0.0%)</option>
				
				<option value="file6">github.com/orbiterhq/comet/metrics_mmap.go (0.0%)</option>
				
				<option value="file7">github.com/orbiterhq/comet/mmap_writer.go (0.0%)</option>
				
				<option value="file8">github.com/orbiterhq/comet/reader.go (0.0%)</option>
				
				<option value="file9">github.com/orbiterhq/comet/reader_v2.go (77.2%)</option>
				
				<option value="file10">github.com/orbiterhq/comet/retention.go (5.6%)</option>
				
				<option value="file11">github.com/orbiterhq/comet/state.go (37.1%)</option>
				
				<option value="file12">github.com/orbiterhq/comet/state_recovery.go (0.0%)</option>
				
				<option value="file13">github.com/orbiterhq/comet/test_helpers.go (0.0%)</option>
				
				</select>
			</div>
			<div id="legend">
				<span>not tracked</span>
			
				<span class="cov0">not covered</span>
				<span class="cov8">covered</span>
			
			</div>
		</div>
		<div id="content">
		
		<pre class="file" id="file0" style="display: none">package comet

import (
        "bufio"
        "context"
        "encoding/binary"
        "fmt"
        "hash/fnv"
        "os"
        "path/filepath"
        "slices"
        "sort"
        "strings"
        "sync"
        "sync/atomic"
        "syscall"
        "time"
        "unsafe"

        "github.com/klauspost/compress/zstd"
)

// Wire format for each entry:
// [uint32 length][uint64 timestamp][byte[] data]

// NOTE: MmapSharedState and SequenceState have been replaced by CometState
// which provides comprehensive metrics and coordination in a single memory-mapped structure

// DURABILITY SEMANTICS:
//
// 1. Write Acknowledgment:
//    - Add() returns successfully after data is written to OS buffers
//    - Direct I/O bypasses page cache when available (Linux)
//    - Not guaranteed durable until explicit Sync() or checkpoint
//
// 2. Checkpointing (automatic durability):
//    - Index persisted every 10,000 writes OR 5 seconds
//    - Data file sync'd during checkpoint
//    - Consumer offsets persisted immediately on ACK
//
// 3. Crash Recovery:
//    - Files scanned from last checkpoint to find actual EOF
//    - Partial writes at EOF are discarded
//    - Consumer offsets recovered from last persisted index
//
// 4. Consistency Guarantees:
//    - Entry-based addressing ensures compression compatibility
//    - Atomic index updates prevent torn reads
//    - Memory barriers ensure proper ordering
//
// 5. Performance vs Durability Trade-offs:
//    - Fast writes: Data buffered, periodic sync
//    - Crash safety: Index checkpoints provide recovery points
//    - Consumer safety: ACKs are immediately durable

const (
        headerSize              = 12      // 4 bytes length + 8 bytes timestamp
        defaultBufSize          = 1 &lt;&lt; 20 // 1MB buffer
        checkpointWrites        = 10000
        maxFileSize             = 1 &lt;&lt; 30   // 1GB per file
        preallocSize            = 128 &lt;&lt; 20 // 128MB preallocation
        minCompressSize         = 256       // Don't compress entries smaller than this
        vectorIOBatch           = 64        // Number of entries to batch for vectored I/O
        defaultBoundaryInterval = 100       // Store boundaries every N entries for memory efficiency
        compressionWorkers      = 4         // Number of parallel compression workers
        compressionQueueSize    = 256       // Buffer size for compression queue
        defaultShardCount       = 16        // Default number of shards for load distribution
)

// Smart Sharding Helpers

// ShardStreamName generates a stream name for a specific shard
func ShardStreamName(namespace string, version string, shardID uint32) string <span class="cov0" title="0">{
        return fmt.Sprintf("%s:%s:shard:%04d", namespace, version, shardID)
}</span>

// PickShard selects a shard based on a key using consistent hashing
// Returns a shard ID in the range [0, shardCount)
func PickShard(key string, shardCount uint32) uint32 <span class="cov0" title="0">{
        if shardCount == 0 </span><span class="cov0" title="0">{
                shardCount = defaultShardCount
        }</span>

        <span class="cov0" title="0">h := fnv.New32a()
        h.Write([]byte(key))
        return h.Sum32() % shardCount</span>
}

// PickShardStream combines PickShard and ShardStreamName for convenience
// Example: PickShardStream("user123", "events", "v1", 16) -&gt; "events:v1:shard:0007"
func PickShardStream(key, namespace, version string, shardCount uint32) string <span class="cov0" title="0">{
        shardID := PickShard(key, shardCount)
        return ShardStreamName(namespace, version, shardID)
}</span>

// AllShardsRange returns a slice of shard IDs from 0 to shardCount-1
// Useful for consumers that need to read from all shards
func AllShardsRange(shardCount uint32) []uint32 <span class="cov0" title="0">{
        if shardCount == 0 </span><span class="cov0" title="0">{
                shardCount = defaultShardCount
        }</span>

        <span class="cov0" title="0">shards := make([]uint32, shardCount)
        for i := uint32(0); i &lt; shardCount; i++ </span><span class="cov0" title="0">{
                shards[i] = i
        }</span>
        <span class="cov0" title="0">return shards</span>
}

// AllShardStreams returns stream names for all shards in a namespace
func AllShardStreams(namespace, version string, shardCount uint32) []string <span class="cov0" title="0">{
        if shardCount == 0 </span><span class="cov0" title="0">{
                shardCount = defaultShardCount
        }</span>

        <span class="cov0" title="0">streams := make([]string, shardCount)
        for i := uint32(0); i &lt; shardCount; i++ </span><span class="cov0" title="0">{
                streams[i] = ShardStreamName(namespace, version, i)
        }</span>
        <span class="cov0" title="0">return streams</span>
}

// CometStats tracks key metrics for monitoring comet performance
type CometStats struct {
        // Write metrics
        TotalEntries     uint64 `json:"total_entries"`
        TotalBytes       uint64 `json:"total_bytes"`
        TotalCompressed  uint64 `json:"total_compressed_bytes"`
        WriteLatencyNano uint64 `json:"avg_write_latency_nano"`
        MinWriteLatency  uint64 `json:"min_write_latency_nano"`
        MaxWriteLatency  uint64 `json:"max_write_latency_nano"`

        // Compression metrics
        CompressionRatio   uint64 `json:"compression_ratio_x100"` // x100 for fixed point
        CompressedEntries  uint64 `json:"compressed_entries"`
        SkippedCompression uint64 `json:"skipped_compression"`

        // File management
        TotalFiles      uint64 `json:"total_files"`
        FileRotations   uint64 `json:"file_rotations"`
        CheckpointCount uint64 `json:"checkpoint_count"`
        LastCheckpoint  uint64 `json:"last_checkpoint_nano"`

        // Consumer metrics
        ActiveReaders uint64 `json:"active_readers"`
        ConsumerLag   uint64 `json:"max_consumer_lag_bytes"`

        // Error tracking
        ErrorCount         uint64 `json:"error_count"`
        LastErrorNano      uint64 `json:"last_error_nano"`
        IndexPersistErrors uint64 `json:"index_persist_errors"`

        // Compression metrics
        CompressionWaitNano uint64 `json:"compression_wait_nano"`
}

// Health represents the health status of the Comet client
type Health struct {
        Status        string    `json:"status"`    // "healthy", "degraded", "unhealthy"
        Healthy       bool      `json:"healthy"`   // Simple boolean for quick checks
        WritesOK      bool      `json:"writes_ok"` // Can we write data?
        ReadsOK       bool      `json:"reads_ok"`  // Can we read data?
        LastWriteTime time.Time `json:"last_write_time"`
        LastErrorTime time.Time `json:"last_error_time"`
        ErrorCount    uint64    `json:"error_count"`
        ActiveShards  int       `json:"active_shards"`
        TotalFiles    uint64    `json:"total_files"`
        Uptime        string    `json:"uptime"`
        Details       string    `json:"details,omitempty"` // Optional details about issues
}

// ClientMetrics holds atomic counters for thread-safe metrics tracking
type ClientMetrics struct {
        TotalEntries       atomic.Uint64
        TotalBytes         atomic.Uint64
        TotalCompressed    atomic.Uint64
        WriteLatencyNano   atomic.Uint64
        MinWriteLatency    atomic.Uint64
        MaxWriteLatency    atomic.Uint64
        CompressionRatio   atomic.Uint64
        CompressedEntries  atomic.Uint64
        SkippedCompression atomic.Uint64
        TotalFiles         atomic.Uint64
        FileRotations      atomic.Uint64
        CheckpointCount    atomic.Uint64
        LastCheckpoint     atomic.Uint64
        ActiveReaders      atomic.Uint64
        ConsumerLag        atomic.Uint64
        ErrorCount         atomic.Uint64
        LastErrorNano      atomic.Uint64
        CompressionWait    atomic.Uint64 // Time waiting for compression
        IndexPersistErrors atomic.Uint64 // Failed index persistence attempts
}

// RetentionConfig defines data retention policies
type RetentionConfig struct {
        // Time-based retention
        MaxAge time.Duration `json:"max_age"` // Delete files older than this

        // Size-based retention
        MaxTotalSize int64 `json:"max_total_size"` // Total size limit across all shards
        MaxShardSize int64 `json:"max_shard_size"` // Size limit per shard

        // Cleanup behavior
        CleanupInterval   time.Duration `json:"cleanup_interval"`   // How often to run cleanup
        MinFilesToKeep    int           `json:"min_files_to_keep"`  // Always keep at least N files
        ProtectUnconsumed bool          `json:"protect_unconsumed"` // Don't delete unread data
        ForceDeleteAfter  time.Duration `json:"force_delete_after"` // Delete even if unread after this time
}

// CompressionConfig controls compression behavior
type CompressionConfig struct {
        MinCompressSize int `json:"min_compress_size"` // Don't compress entries smaller than this
}

// IndexingConfig controls indexing behavior
type IndexingConfig struct {
        BoundaryInterval int `json:"boundary_interval"` // Store boundaries every N entries
        MaxIndexEntries  int `json:"max_index_entries"` // Max boundary entries per shard (0 = unlimited)
}

// StorageConfig controls file storage behavior
type StorageConfig struct {
        MaxFileSize    int64 `json:"max_file_size"`      // Maximum size per file before rotation
        CheckpointTime int   `json:"checkpoint_time_ms"` // Checkpoint every N milliseconds
}

// ConcurrencyConfig controls multi-process behavior
type ConcurrencyConfig struct {
        EnableMultiProcessMode bool `json:"enable_file_locking"` // Enable file-based locking for multi-process safety
}

// CometConfig holds configuration for comet client
type CometConfig struct {
        // Compression settings
        Compression CompressionConfig `json:"compression"`

        // Indexing settings
        Indexing IndexingConfig `json:"indexing"`

        // Storage settings
        Storage StorageConfig `json:"storage"`

        // Concurrency settings
        Concurrency ConcurrencyConfig `json:"concurrency"`

        // Retention policy
        Retention RetentionConfig `json:"retention"`
        // Logging configuration
        Log LogConfig `json:"log"`
}

// DefaultCometConfig returns sensible defaults optimized for logging workloads
func DefaultCometConfig() CometConfig <span class="cov8" title="1">{
        return CometConfig{
                // Compression - optimized for logging workloads
                Compression: CompressionConfig{
                        MinCompressSize: 4096, // Only compress entries &gt;4KB to avoid latency hit on typical logs
                },

                // Indexing - memory efficient boundary tracking
                Indexing: IndexingConfig{
                        BoundaryInterval: 100,   // Store boundaries every 100 entries
                        MaxIndexEntries:  10000, // Limit index memory growth
                },

                // Storage - optimized for 1GB files
                Storage: StorageConfig{
                        MaxFileSize:    1 &lt;&lt; 30, // 1GB per file
                        CheckpointTime: 2000,    // Checkpoint every 2 seconds
                },

                // Concurrency - disabled by default for performance
                // Enable this for multi-process deployments (e.g., prefork mode)
                Concurrency: ConcurrencyConfig{
                        EnableMultiProcessMode: false, // Single-process mode is faster
                },

                // Retention - defaults for edge observability
                Retention: RetentionConfig{
                        MaxAge:            4 * time.Hour,    // Keep 4 hours of data
                        MaxTotalSize:      10 &lt;&lt; 30,         // 10GB total
                        MaxShardSize:      1 &lt;&lt; 30,          // 1GB per shard
                        CleanupInterval:   15 * time.Minute, // Check every 15 minutes
                        MinFilesToKeep:    2,                // Keep at least 2 files
                        ProtectUnconsumed: true,             // Don't delete unread data by default
                        ForceDeleteAfter:  24 * time.Hour,   // Force delete after 24 hours
                },
        }
}</span>

// HighCompressionConfig returns a config optimized for high compression ratios
// Suitable for text-heavy logs and structured data
func HighCompressionConfig() CometConfig <span class="cov0" title="0">{
        cfg := DefaultCometConfig()
        cfg.Compression.MinCompressSize = 256 // Compress almost everything
        return cfg
}</span>

// MultiProcessConfig returns a config suitable for multi-process deployments
// Enable this for prefork servers or when multiple processes write to the same stream
func MultiProcessConfig() CometConfig <span class="cov0" title="0">{
        cfg := DefaultCometConfig()
        cfg.Concurrency.EnableMultiProcessMode = true
        cfg.Storage.CheckpointTime = 100 // More frequent checkpoints for multi-process coordination
        return cfg
}</span>

// HighThroughputConfig returns a config optimized for high write throughput
// Trades some memory for better performance
func HighThroughputConfig() CometConfig <span class="cov0" title="0">{
        cfg := DefaultCometConfig()
        cfg.Indexing.BoundaryInterval = 1000    // Less frequent indexing
        cfg.Indexing.MaxIndexEntries = 100000   // Allow larger index
        cfg.Storage.CheckpointTime = 5000       // Less frequent checkpoints
        cfg.Compression.MinCompressSize = 10240 // Only compress very large entries
        return cfg
}</span>

// validateConfig ensures configuration values are reasonable
func validateConfig(cfg *CometConfig) error <span class="cov8" title="1">{
        // Storage validation
        if cfg.Storage.MaxFileSize &lt; 100 </span><span class="cov0" title="0">{
                return fmt.Errorf("MaxFileSize must be at least 100 bytes, got %d", cfg.Storage.MaxFileSize)
        }</span>
        <span class="cov8" title="1">if cfg.Storage.MaxFileSize &gt; 10&lt;&lt;30 </span><span class="cov0" title="0">{ // 10GB
                return fmt.Errorf("MaxFileSize cannot exceed 10GB, got %d", cfg.Storage.MaxFileSize)
        }</span>
        <span class="cov8" title="1">if cfg.Storage.CheckpointTime &lt; 0 </span><span class="cov0" title="0">{
                return fmt.Errorf("CheckpointTime cannot be negative, got %d", cfg.Storage.CheckpointTime)
        }</span>
        <span class="cov8" title="1">if cfg.Storage.CheckpointTime &gt; 300000 </span><span class="cov0" title="0">{ // 5 minutes
                return fmt.Errorf("CheckpointTime cannot exceed 5 minutes (300000ms), got %d", cfg.Storage.CheckpointTime)
        }</span>

        // Compression validation
        <span class="cov8" title="1">if cfg.Compression.MinCompressSize &lt; 0 </span><span class="cov0" title="0">{
                return fmt.Errorf("MinCompressSize cannot be negative, got %d", cfg.Compression.MinCompressSize)
        }</span>
        // Note: MinCompressSize can be larger than MaxFileSize to effectively disable compression

        // Indexing validation
        <span class="cov8" title="1">if cfg.Indexing.BoundaryInterval &lt; 1 </span><span class="cov0" title="0">{
                return fmt.Errorf("BoundaryInterval must be at least 1, got %d", cfg.Indexing.BoundaryInterval)
        }</span>
        <span class="cov8" title="1">if cfg.Indexing.BoundaryInterval &gt; 10000 </span><span class="cov0" title="0">{
                return fmt.Errorf("BoundaryInterval cannot exceed 10000, got %d", cfg.Indexing.BoundaryInterval)
        }</span>
        <span class="cov8" title="1">if cfg.Indexing.MaxIndexEntries &lt; 0 </span><span class="cov0" title="0">{
                return fmt.Errorf("MaxIndexEntries cannot be negative, got %d", cfg.Indexing.MaxIndexEntries)
        }</span>
        <span class="cov8" title="1">if cfg.Indexing.MaxIndexEntries &gt; 0 &amp;&amp; cfg.Indexing.MaxIndexEntries &lt; 10 </span><span class="cov0" title="0">{
                return fmt.Errorf("MaxIndexEntries must be 0 (unlimited) or at least 10, got %d", cfg.Indexing.MaxIndexEntries)
        }</span>

        // Retention validation
        <span class="cov8" title="1">if cfg.Retention.MaxShardSize &lt; 0 </span><span class="cov0" title="0">{
                return fmt.Errorf("MaxShardSize cannot be negative, got %d", cfg.Retention.MaxShardSize)
        }</span>
        <span class="cov8" title="1">if cfg.Retention.MaxShardSize &gt; 0 &amp;&amp; cfg.Retention.MaxShardSize &lt; cfg.Storage.MaxFileSize </span><span class="cov0" title="0">{
                return fmt.Errorf("MaxShardSize must be 0 (unlimited) or at least MaxFileSize (%d), got %d",
                        cfg.Storage.MaxFileSize, cfg.Retention.MaxShardSize)
        }</span>
        <span class="cov8" title="1">if cfg.Retention.MaxAge &lt; 0 </span><span class="cov0" title="0">{
                return fmt.Errorf("MaxAge cannot be negative, got %v", cfg.Retention.MaxAge)
        }</span>

        <span class="cov8" title="1">return nil</span>
}

// Client implements a local file-based stream client with append-only storage
type Client struct {
        dataDir     string
        config      CometConfig
        logger      Logger
        shards      map[uint32]*Shard
        metrics     ClientMetrics
        mu          sync.RWMutex
        closed      bool
        retentionWg sync.WaitGroup
        stopCh      chan struct{}
        startTime   time.Time
}

// Shard represents a single stream shard with its own files
// Fields ordered for optimal memory alignment (64-bit words first)
type Shard struct {
        // 64-bit aligned fields first (8 bytes each)
        readerCount    int64     // Lock-free reader tracking
        lastCheckpoint time.Time // 64-bit on most systems
        lastMmapCheck  int64     // Last mmap timestamp we saw (for change detection)

        // Pointers (8 bytes each on 64-bit)
        dataFile          *os.File      // Data file handle
        writer            *bufio.Writer // Buffered writer
        compressor        *zstd.Encoder // Compression engine
        index             *ShardIndex   // Shard metadata
        lockFile          *os.File      // File lock for multi-writer safety
        indexLockFile     *os.File      // Separate lock for index writes
        retentionLockFile *os.File      // Separate lock for retention operations
        rotationLockFile  *os.File      // Separate lock for file rotation operations
        // NOTE: mmapState and sequenceState replaced by state
        mmapWriter *MmapWriter // Memory-mapped writer for ultra-fast multi-process writes
        state      *CometState // NEW: Unified memory-mapped state for all metrics and coordination
        logger     Logger      // Logger for this shard

        // Strings (24 bytes: ptr + len + cap)
        indexPath         string // Path to index file
        indexLockPath     string // Path to index lock file
        retentionLockPath string // Path to retention lock file
        rotationLockPath  string // Path to rotation lock file
        statePath         string // Path to unified state file
        stateData         []byte // Memory-mapped unified state data (slice header: 24 bytes)

        // Mutex (platform-specific, often 24 bytes)
        mu      sync.RWMutex
        writeMu sync.Mutex // Protects DirectWriter from concurrent writes
        indexMu sync.Mutex // Protects index file writes

        // Synchronization for background operations
        wg sync.WaitGroup // Tracks background goroutines

        // Smaller fields last
        writesSinceCheckpoint int    // 8 bytes
        shardID               uint32 // 4 bytes
        // 4 bytes padding will be added automatically for 8-byte alignment
}

// EntryIndexNode represents a node in the binary searchable index
type EntryIndexNode struct {
        EntryNumber int64         `json:"entry_number"` // Entry number this node covers
        Position    EntryPosition `json:"position"`     // Position in files
}

// BinarySearchableIndex provides O(log n) entry lookups
type BinarySearchableIndex struct {
        // Sorted slice of index nodes for binary search
        Nodes []EntryIndexNode `json:"nodes"`
        // Interval between indexed entries (default: 1000)
        IndexInterval int `json:"index_interval"`
        // Maximum number of nodes to keep (0 = unlimited)
        MaxNodes int `json:"max_nodes"`
}

// ShardIndex tracks files and consumer offsets
// Fixed to use entry-based addressing instead of byte offsets
// Fields ordered for optimal memory alignment
type ShardIndex struct {
        // 64-bit aligned fields first (8 bytes each)
        CurrentEntryNumber int64 `json:"current_entry_number"` // Entry-based tracking (not byte offsets!)
        CurrentWriteOffset int64 `json:"current_write_offset"` // Still track for file management

        // Maps (8 bytes pointer each)
        ConsumerOffsets map[string]int64 `json:"consumer_entry_offsets"` // Consumer tracking by entry number (not bytes!)

        // Composite types
        BinaryIndex BinarySearchableIndex `json:"binary_index"` // Binary searchable index for O(log n) lookups

        // Slices (24 bytes: ptr + len + cap)
        Files []FileInfo `json:"files"` // File management

        // Strings (24 bytes: ptr + len + cap)
        CurrentFile string `json:"current_file"`

        // Smaller fields last
        BoundaryInterval int `json:"boundary_interval"` // 8 bytes
}

// AddIndexNode adds a new entry to the binary searchable index
func (bi *BinarySearchableIndex) AddIndexNode(entryNumber int64, position EntryPosition) <span class="cov8" title="1">{
        // Only index entries at the specified interval to limit memory usage
        if bi.IndexInterval == 0 </span><span class="cov0" title="0">{
                bi.IndexInterval = 1000 // Default interval
        }</span>

        <span class="cov8" title="1">if entryNumber%int64(bi.IndexInterval) == 0 || entryNumber == 0 </span><span class="cov8" title="1">{
                node := EntryIndexNode{
                        EntryNumber: entryNumber,
                        Position:    position,
                }

                // Use binary search to find insertion point to maintain sorted order
                idx, found := slices.BinarySearchFunc(bi.Nodes, node, func(a, b EntryIndexNode) int </span><span class="cov0" title="0">{
                        if a.EntryNumber &lt; b.EntryNumber </span><span class="cov0" title="0">{
                                return -1
                        }</span>
                        <span class="cov0" title="0">if a.EntryNumber &gt; b.EntryNumber </span><span class="cov0" title="0">{
                                return 1
                        }</span>
                        <span class="cov0" title="0">return 0</span>
                })

                <span class="cov8" title="1">if !found </span><span class="cov8" title="1">{
                        // Insert at the correct position to maintain sorted order
                        bi.Nodes = slices.Insert(bi.Nodes, idx, node)

                        // Check if we need to prune old nodes
                        if bi.MaxNodes &gt; 0 &amp;&amp; len(bi.Nodes) &gt; bi.MaxNodes </span><span class="cov0" title="0">{
                                // Remove the oldest (first) node
                                bi.Nodes = bi.Nodes[1:]
                        }</span>
                } else<span class="cov0" title="0"> {
                        // Update existing node
                        bi.Nodes[idx] = node
                }</span>
        }
}

// FindEntry uses binary search to locate an entry position
func (bi *BinarySearchableIndex) FindEntry(entryNumber int64) (EntryPosition, bool) <span class="cov0" title="0">{
        if len(bi.Nodes) == 0 </span><span class="cov0" title="0">{
                return EntryPosition{}, false
        }</span>

        // Find the largest indexed entry &lt;= target
        <span class="cov0" title="0">target := EntryIndexNode{EntryNumber: entryNumber}
        idx, found := slices.BinarySearchFunc(bi.Nodes, target, func(a, b EntryIndexNode) int </span><span class="cov0" title="0">{
                if a.EntryNumber &lt; b.EntryNumber </span><span class="cov0" title="0">{
                        return -1
                }</span>
                <span class="cov0" title="0">if a.EntryNumber &gt; b.EntryNumber </span><span class="cov0" title="0">{
                        return 1
                }</span>
                <span class="cov0" title="0">return 0</span>
        })

        <span class="cov0" title="0">if found </span><span class="cov0" title="0">{
                // Exact match
                return bi.Nodes[idx].Position, true
        }</span>

        <span class="cov0" title="0">if idx == 0 </span><span class="cov0" title="0">{
                // Target is before the first indexed entry
                return EntryPosition{}, false
        }</span>

        // Return the position of the largest indexed entry before target
        // The caller will need to scan forward from this position
        <span class="cov0" title="0">return bi.Nodes[idx-1].Position, true</span>
}

// GetScanStartPosition returns the best starting position for scanning to find an entry
func (bi *BinarySearchableIndex) GetScanStartPosition(entryNumber int64) (EntryPosition, int64, bool) <span class="cov0" title="0">{
        if len(bi.Nodes) == 0 </span><span class="cov0" title="0">{
                return EntryPosition{}, 0, false
        }</span>

        // Find the largest indexed entry &lt;= target
        <span class="cov0" title="0">target := EntryIndexNode{EntryNumber: entryNumber}
        idx, found := slices.BinarySearchFunc(bi.Nodes, target, func(a, b EntryIndexNode) int </span><span class="cov0" title="0">{
                if a.EntryNumber &lt; b.EntryNumber </span><span class="cov0" title="0">{
                        return -1
                }</span>
                <span class="cov0" title="0">if a.EntryNumber &gt; b.EntryNumber </span><span class="cov0" title="0">{
                        return 1
                }</span>
                <span class="cov0" title="0">return 0</span>
        })

        <span class="cov0" title="0">if found </span><span class="cov0" title="0">{
                // Exact match
                return bi.Nodes[idx].Position, bi.Nodes[idx].EntryNumber, true
        }</span>

        <span class="cov0" title="0">if idx == 0 </span><span class="cov0" title="0">{
                // Target is before the first indexed entry, start from beginning
                return EntryPosition{FileIndex: 0, ByteOffset: 0}, 0, true
        }</span>

        // Return the position of the largest indexed entry before target
        <span class="cov0" title="0">node := bi.Nodes[idx-1]
        return node.Position, node.EntryNumber, true</span>
}

// EntryPosition tracks where an entry is located
// Fields ordered for optimal memory alignment
type EntryPosition struct {
        ByteOffset int64 `json:"byte_offset"` // Byte position within that file (8 bytes)
        FileIndex  int   `json:"file_index"`  // Which file in Files array (8 bytes)
}

// FileInfo tracks a single data file
// Fields ordered for optimal memory alignment
type FileInfo struct {
        // 64-bit aligned fields first
        StartOffset int64     `json:"start_offset"`
        EndOffset   int64     `json:"end_offset"`
        StartEntry  int64     `json:"start_entry"` // First entry number in this file
        Entries     int64     `json:"entries"`
        StartTime   time.Time `json:"start_time"`
        EndTime     time.Time `json:"end_time"`

        // String last (will use remaining space efficiently)
        Path string `json:"path"`
}

// NewClient creates a new comet client for local file-based streaming with default config
func NewClient(dataDir string) (*Client, error) <span class="cov0" title="0">{
        return NewClientWithConfig(dataDir, DefaultCometConfig())
}</span>

// NewClientWithConfig creates a new comet client with custom configuration
func NewClientWithConfig(dataDir string, config CometConfig) (*Client, error) <span class="cov8" title="1">{
        // Validate configuration
        if err := validateConfig(&amp;config); err != nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("invalid configuration: %w", err)
        }</span>

        <span class="cov8" title="1">if err := os.MkdirAll(dataDir, 0755); err != nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("failed to create data directory: %w", err)
        }</span>

        // Create logger based on config
        <span class="cov8" title="1">logger := createLogger(config.Log)

        // Set debug mode from config
        if config.Log.EnableDebug </span><span class="cov0" title="0">{
                SetDebug(true)
        }</span>

        <span class="cov8" title="1">c := &amp;Client{
                dataDir:   dataDir,
                config:    config,
                logger:    logger,
                shards:    make(map[uint32]*Shard),
                stopCh:    make(chan struct{}),
                startTime: time.Now(),
        }

        // Start retention manager if configured
        c.startRetentionManager()

        return c, nil</span>
}

// Append adds entries to a stream shard (append-only semantics)
func (c *Client) Append(ctx context.Context, stream string, entries [][]byte) ([]MessageID, error) <span class="cov8" title="1">{
        // Extract shard from stream name (e.g., "events:v1:shard:0042" -&gt; 42)
        shardID, err := parseShardFromStream(stream)
        if err != nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("invalid stream name %s: %w", stream, err)
        }</span>

        <span class="cov8" title="1">shard, err := c.getOrCreateShard(shardID)
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>

        <span class="cov8" title="1">return shard.appendEntries(entries, &amp;c.metrics, &amp;c.config)</span>
}

// Len returns the number of entries in a stream shard
func (c *Client) Len(ctx context.Context, stream string) (int64, error) <span class="cov0" title="0">{
        shardID, err := parseShardFromStream(stream)
        if err != nil </span><span class="cov0" title="0">{
                return 0, fmt.Errorf("invalid stream name %s: %w", stream, err)
        }</span>

        <span class="cov0" title="0">shard, err := c.getOrCreateShard(shardID)
        if err != nil </span><span class="cov0" title="0">{
                return 0, err
        }</span>

        <span class="cov0" title="0">shard.mu.RLock()
        defer shard.mu.RUnlock()

        // If file locking is enabled, reload index to get latest state from other processes
        // Only reload if the index file exists and is newer than our last checkpoint
        if c.config.Concurrency.EnableMultiProcessMode &amp;&amp; shard.lockFile != nil </span><span class="cov0" title="0">{
                if indexStat, err := os.Stat(shard.indexPath); err == nil </span><span class="cov0" title="0">{
                        if indexStat.ModTime().After(shard.lastCheckpoint) </span><span class="cov0" title="0">{
                                // Acquire shared lock for reading
                                err := syscall.Flock(int(shard.lockFile.Fd()), syscall.LOCK_SH)
                                if err != nil </span><span class="cov0" title="0">{
                                        return 0, fmt.Errorf("failed to acquire shared lock for Len: %w", err)
                                }</span>
                                <span class="cov0" title="0">defer syscall.Flock(int(shard.lockFile.Fd()), syscall.LOCK_UN)

                                // Reload index to get current state
                                if err := shard.loadIndex(); err != nil </span><span class="cov0" title="0">{
                                        return 0, fmt.Errorf("failed to reload index for Len: %w", err)
                                }</span>
                        }
                }
        }

        // In multi-process mode, check if we need to rebuild index AFTER reloading
        <span class="cov0" title="0">if shard.state != nil </span><span class="cov0" title="0">{
                // Check if rebuild needed while holding read lock
                needsRebuild := shard.checkIfRebuildNeeded()
                shard.mu.RUnlock()

                if needsRebuild </span><span class="cov0" title="0">{
                        // Acquire write lock for rebuild
                        shard.mu.Lock()
                        // Double-check after acquiring write lock
                        if shard.checkIfRebuildNeeded() </span><span class="cov0" title="0">{
                                shard.lazyRebuildIndexIfNeeded(c.config, filepath.Join(c.dataDir, fmt.Sprintf("shard-%04d", shard.shardID)))
                        }</span>
                        <span class="cov0" title="0">shard.mu.Unlock()</span>
                }

                // Re-acquire read lock
                <span class="cov0" title="0">shard.mu.RLock()</span>
        }

        <span class="cov0" title="0">var total int64
        for _, file := range shard.index.Files </span><span class="cov0" title="0">{
                total += file.Entries
        }</span>

        <span class="cov0" title="0">return total, nil</span>
}

// Sync ensures all buffered data is durably written to disk
func (c *Client) Sync(ctx context.Context) error <span class="cov0" title="0">{
        c.mu.RLock()
        shards := make([]*Shard, 0, len(c.shards))
        for _, shard := range c.shards </span><span class="cov0" title="0">{
                shards = append(shards, shard)
        }</span>
        <span class="cov0" title="0">c.mu.RUnlock()

        for _, shard := range shards </span><span class="cov0" title="0">{
                shard.mu.Lock()

                // Flush and sync writer
                if shard.writer != nil </span><span class="cov0" title="0">{
                        shard.writeMu.Lock()
                        err := shard.writer.Flush()
                        if err == nil &amp;&amp; shard.dataFile != nil </span><span class="cov0" title="0">{
                                err = shard.dataFile.Sync()
                        }</span>
                        <span class="cov0" title="0">shard.writeMu.Unlock()
                        if err != nil </span><span class="cov0" title="0">{
                                shard.mu.Unlock()
                                return fmt.Errorf("failed to sync shard %d: %w", shard.shardID, err)
                        }</span>
                }

                // Force checkpoint
                // Clone index while holding lock
                <span class="cov0" title="0">indexCopy := shard.cloneIndex()
                shard.writesSinceCheckpoint = 0
                shard.lastCheckpoint = time.Now()
                shard.mu.Unlock()

                // Persist outside of lock
                // For multi-process safety, use the separate index lock if enabled
                var err error
                if c.config.Concurrency.EnableMultiProcessMode &amp;&amp; shard.indexLockFile != nil </span><span class="cov0" title="0">{
                        // Acquire exclusive lock for index writes
                        if err := syscall.Flock(int(shard.indexLockFile.Fd()), syscall.LOCK_EX); err != nil </span><span class="cov0" title="0">{
                                return fmt.Errorf("failed to acquire index lock for shard %d: %w", shard.shardID, err)
                        }</span>

                        <span class="cov0" title="0">shard.indexMu.Lock()
                        // Reload index from disk to merge with other processes' changes
                        if _, statErr := os.Stat(shard.indexPath); statErr == nil </span><span class="cov0" title="0">{
                                // Index exists, load it to get latest state
                                if diskIndex, loadErr := shard.loadBinaryIndex(); loadErr == nil </span><span class="cov0" title="0">{
                                        // Merge our changes with the disk state
                                        // Important: We need to merge carefully to avoid losing entries
                                        // The disk state represents entries written by other processes

                                        // Always use the highest entry number and write offset
                                        if diskIndex.CurrentEntryNumber &gt; indexCopy.CurrentEntryNumber </span><span class="cov0" title="0">{
                                                indexCopy.CurrentEntryNumber = diskIndex.CurrentEntryNumber
                                        }</span>
                                        <span class="cov0" title="0">if diskIndex.CurrentWriteOffset &gt; indexCopy.CurrentWriteOffset </span><span class="cov0" title="0">{
                                                indexCopy.CurrentWriteOffset = diskIndex.CurrentWriteOffset
                                        }</span>

                                        // Merge consumer offsets - keep the highest offset for each consumer
                                        <span class="cov0" title="0">for group, offset := range diskIndex.ConsumerOffsets </span><span class="cov0" title="0">{
                                                if currentOffset, exists := indexCopy.ConsumerOffsets[group]; !exists || offset &gt; currentOffset </span><span class="cov0" title="0">{
                                                        indexCopy.ConsumerOffsets[group] = offset
                                                }</span>
                                        }

                                        // Merge file info - this is critical for multi-process coordination
                                        // The disk version should have the most complete file information
                                        <span class="cov0" title="0">if len(diskIndex.Files) &gt; 0 </span><span class="cov0" title="0">{
                                                // Always use the disk version as it represents the actual files
                                                indexCopy.Files = diskIndex.Files

                                                // Update the current file info if we have one
                                                if len(indexCopy.Files) &gt; 0 &amp;&amp; indexCopy.CurrentFile != "" </span><span class="cov0" title="0">{
                                                        lastFile := &amp;indexCopy.Files[len(indexCopy.Files)-1]
                                                        // Update the last file's end offset and entry count based on our writes
                                                        if lastFile.Path == indexCopy.CurrentFile </span><span class="cov0" title="0">{
                                                                lastFile.EndOffset = indexCopy.CurrentWriteOffset
                                                                // Calculate actual entries in the file
                                                                if diskIndex.CurrentEntryNumber &gt; 0 </span><span class="cov0" title="0">{
                                                                        entriesInOtherFiles := int64(0)
                                                                        for i := 0; i &lt; len(indexCopy.Files)-1; i++ </span><span class="cov0" title="0">{
                                                                                // Skip files with corrupted entry counts
                                                                                if indexCopy.Files[i].Entries &gt;= 0 </span><span class="cov0" title="0">{
                                                                                        entriesInOtherFiles += indexCopy.Files[i].Entries
                                                                                }</span>
                                                                        }
                                                                        <span class="cov0" title="0">calculatedEntries := indexCopy.CurrentEntryNumber - entriesInOtherFiles

                                                                        // Prevent negative entry counts which cause corruption
                                                                        if calculatedEntries &gt;= 0 </span><span class="cov0" title="0">{
                                                                                lastFile.Entries = calculatedEntries
                                                                        }</span>
                                                                        // If calculation would be negative, keep the existing value
                                                                        // This prevents the uint64 overflow issue during serialization
                                                                }
                                                        }
                                                }
                                        }

                                        // Merge binary index nodes if needed
                                        <span class="cov0" title="0">if diskIndex.BinaryIndex.Nodes != nil &amp;&amp; len(diskIndex.BinaryIndex.Nodes) &gt; len(indexCopy.BinaryIndex.Nodes) </span><span class="cov0" title="0">{
                                                indexCopy.BinaryIndex = diskIndex.BinaryIndex
                                        }</span>
                                }
                        }

                        <span class="cov0" title="0">err = shard.saveBinaryIndex(indexCopy)
                        shard.indexMu.Unlock()

                        // Update mmap state BEFORE releasing lock
                        if err == nil </span><span class="cov0" title="0">{
                                shard.updateMmapState()
                        }</span>

                        // Release lock immediately after index save
                        <span class="cov0" title="0">syscall.Flock(int(shard.indexLockFile.Fd()), syscall.LOCK_UN)</span>
                } else<span class="cov0" title="0"> {
                        shard.indexMu.Lock()
                        err = shard.saveBinaryIndex(indexCopy)
                        shard.indexMu.Unlock()

                        if err == nil </span><span class="cov0" title="0">{
                                shard.updateMmapState()
                        }</span>
                }

                <span class="cov0" title="0">if err != nil </span><span class="cov0" title="0">{
                        return fmt.Errorf("failed to persist index for shard %d: %w", shard.shardID, err)
                }</span>
        }

        <span class="cov0" title="0">return nil</span>
}

// getOrCreateShard returns an existing shard or creates a new one
func (c *Client) getOrCreateShard(shardID uint32) (*Shard, error) <span class="cov8" title="1">{
        c.mu.RLock()
        shard, exists := c.shards[shardID]
        c.mu.RUnlock()

        if exists </span><span class="cov8" title="1">{
                return shard, nil
        }</span>

        <span class="cov8" title="1">c.mu.Lock()
        defer c.mu.Unlock()

        // Double-check after acquiring write lock
        if shard, exists = c.shards[shardID]; exists </span><span class="cov0" title="0">{
                return shard, nil
        }</span>

        // Create new shard
        <span class="cov8" title="1">shardDir := filepath.Join(c.dataDir, fmt.Sprintf("shard-%04d", shardID))
        if err := os.MkdirAll(shardDir, 0755); err != nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("failed to create shard directory: %w", err)
        }</span>

        <span class="cov8" title="1">shard = &amp;Shard{
                shardID:           shardID,
                logger:            c.logger.WithFields("shard", shardID),
                indexPath:         filepath.Join(shardDir, "index.bin"),
                indexLockPath:     filepath.Join(shardDir, "index.lock"),
                retentionLockPath: filepath.Join(shardDir, "retention.lock"),
                rotationLockPath:  filepath.Join(shardDir, "rotation.lock"),
                statePath:         filepath.Join(shardDir, "comet.state"),
                index: &amp;ShardIndex{
                        BoundaryInterval: c.config.Indexing.BoundaryInterval,
                        ConsumerOffsets:  make(map[string]int64),
                        BinaryIndex: BinarySearchableIndex{
                                IndexInterval: c.config.Indexing.BoundaryInterval,
                                MaxNodes:      c.config.Indexing.MaxIndexEntries, // Use full limit for binary index
                        },
                },
                lastCheckpoint: time.Now(),
        }

        // Initialize unified state (memory-mapped in multi-process mode, in-memory otherwise)
        if err := shard.initCometState(c.config.Concurrency.EnableMultiProcessMode); err != nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("failed to initialize unified state: %w", err)
        }</span>

        // Create lock files for multi-writer coordination if enabled
        <span class="cov8" title="1">if c.config.Concurrency.EnableMultiProcessMode </span><span class="cov0" title="0">{
                // Data write lock
                lockPath := filepath.Join(shardDir, "shard.lock")
                lockFile, err := os.OpenFile(lockPath, os.O_CREATE|os.O_RDWR, 0644)
                if err != nil </span><span class="cov0" title="0">{
                        return nil, fmt.Errorf("failed to create lock file: %w", err)
                }</span>
                <span class="cov0" title="0">shard.lockFile = lockFile

                // Separate index write lock for better concurrency
                indexLockFile, err := os.OpenFile(shard.indexLockPath, os.O_CREATE|os.O_RDWR, 0644)
                if err != nil </span><span class="cov0" title="0">{
                        lockFile.Close()
                        return nil, fmt.Errorf("failed to create index lock file: %w", err)
                }</span>
                <span class="cov0" title="0">shard.indexLockFile = indexLockFile

                // Separate retention lock for coordinating retention operations
                retentionLockFile, err := os.OpenFile(shard.retentionLockPath, os.O_CREATE|os.O_RDWR, 0644)
                if err != nil </span><span class="cov0" title="0">{
                        lockFile.Close()
                        indexLockFile.Close()
                        return nil, fmt.Errorf("failed to create retention lock file: %w", err)
                }</span>
                <span class="cov0" title="0">shard.retentionLockFile = retentionLockFile

                // Separate rotation lock for coordinating file rotation operations
                rotationLockFile, err := os.OpenFile(shard.rotationLockPath, os.O_CREATE|os.O_RDWR, 0644)
                if err != nil </span><span class="cov0" title="0">{
                        lockFile.Close()
                        indexLockFile.Close()
                        retentionLockFile.Close()
                        return nil, fmt.Errorf("failed to create rotation lock file: %w", err)
                }</span>
                <span class="cov0" title="0">shard.rotationLockFile = rotationLockFile</span>
        }

        // NOTE: Old mmapState and sequenceState initialization removed - replaced by CometState
        <span class="cov8" title="1">if c.config.Concurrency.EnableMultiProcessMode </span><span class="cov0" title="0">{
                // CometState initialization already handled above

                // Initialize memory-mapped writer for ultra-fast writes
                mmapWriter, err := NewMmapWriter(shardDir, c.config.Storage.MaxFileSize, shard.index, shard.state, shard.rotationLockFile)
                if err != nil </span><span class="cov0" title="0">{
                        return nil, fmt.Errorf("failed to initialize mmap writer: %w", err)
                }</span>
                <span class="cov0" title="0">shard.mmapWriter = mmapWriter</span>
        }

        // Load existing index if present
        <span class="cov8" title="1">if err := shard.loadIndex(); err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>

        // If we have an mmap writer, sync the index with the state
        <span class="cov8" title="1">if shard.mmapWriter != nil &amp;&amp; shard.mmapWriter.state != nil </span><span class="cov0" title="0">{
                writeOffset := shard.mmapWriter.state.GetWriteOffset()
                if writeOffset &gt; uint64(shard.index.CurrentWriteOffset) </span><span class="cov0" title="0">{
                        shard.index.CurrentWriteOffset = int64(writeOffset)
                }</span>
        }

        // Open or create current data file
        <span class="cov8" title="1">if err := shard.openDataFileWithConfig(shardDir, &amp;c.config); err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>

        // Recover from crash if needed
        // Track recovery attempt
        <span class="cov8" title="1">if shard.state != nil </span><span class="cov8" title="1">{
                atomic.AddUint64(&amp;shard.state.RecoveryAttempts, 1)
        }</span>
        <span class="cov8" title="1">if err := shard.recoverFromCrash(); err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>
        // Recovery successful if we got here
        <span class="cov8" title="1">if shard.state != nil </span><span class="cov8" title="1">{
                atomic.AddUint64(&amp;shard.state.RecoverySuccesses, 1)
        }</span>

        <span class="cov8" title="1">c.shards[shardID] = shard

        // Debug log shard creation
        if Debug &amp;&amp; c.logger != nil </span><span class="cov0" title="0">{
                c.logger.Debug("Created new shard",
                        "shardID", shardID,
                        "path", shardDir,
                        "multiProcess", c.config.Concurrency.EnableMultiProcessMode)
        }</span>

        <span class="cov8" title="1">return shard, nil</span>
}

// CompressedEntry represents a pre-compressed entry ready for writing
type CompressedEntry struct {
        Data           []byte
        OriginalSize   uint64
        CompressedSize uint64
        WasCompressed  bool
}

// preCompressEntries compresses entries outside of any locks to reduce contention
func (s *Shard) preCompressEntries(entries [][]byte, config *CometConfig) []CompressedEntry <span class="cov8" title="1">{
        compressed := make([]CompressedEntry, len(entries))

        for i, data := range entries </span><span class="cov8" title="1">{
                originalSize := uint64(len(data))

                if len(data) &gt;= config.Compression.MinCompressSize &amp;&amp; s.compressor != nil </span><span class="cov8" title="1">{
                        // Compress the data
                        compressionStart := time.Now()
                        compressedData := s.compressor.EncodeAll(data, nil)
                        compressionDuration := time.Since(compressionStart)

                        // Track compression time
                        if s.state != nil </span><span class="cov8" title="1">{
                                atomic.AddInt64(&amp;s.state.CompressionTimeNanos, compressionDuration.Nanoseconds())
                        }</span>

                        <span class="cov8" title="1">compressed[i] = CompressedEntry{
                                Data:           compressedData,
                                OriginalSize:   originalSize,
                                CompressedSize: uint64(len(compressedData)),
                                WasCompressed:  true,
                        }</span>
                } else<span class="cov8" title="1"> {
                        // Use original data directly (zero-copy)
                        compressed[i] = CompressedEntry{
                                Data:           data,
                                OriginalSize:   originalSize,
                                CompressedSize: originalSize,
                                WasCompressed:  false,
                        }
                }</span>
        }

        <span class="cov8" title="1">return compressed</span>
}

// WriteRequest represents a batch write operation
// Fields ordered for optimal memory alignment
type WriteRequest struct {
        UpdateFunc   func() error // Function to update index state after successful write (8 bytes)
        WriteBuffers [][]byte     // Buffers to write (24 bytes)
        IDs          []MessageID  // Message IDs for the batch (24 bytes)
}

// appendEntries adds raw entry bytes to the shard with I/O outside locks
func (s *Shard) appendEntries(entries [][]byte, clientMetrics *ClientMetrics, config *CometConfig) ([]MessageID, error) <span class="cov8" title="1">{
        startTime := time.Now()

        // Pre-compress entries OUTSIDE the lock to reduce contention
        compressedEntries := s.preCompressEntries(entries, config)

        // Prepare write request while holding lock
        var writeReq WriteRequest
        var totalOriginalBytes, totalCompressedBytes uint64
        var compressedCount, skippedCount uint64

        // Critical section: prepare write data and update indices
        var criticalErr error
        func() </span><span class="cov8" title="1">{
                s.mu.Lock()
                defer s.mu.Unlock()

                // Check mmap state for instant change detection (no lock needed for read)
                if config.Concurrency.EnableMultiProcessMode &amp;&amp; s.state != nil </span><span class="cov0" title="0">{
                        currentTimestamp := s.state.GetLastIndexUpdate()
                        if currentTimestamp != s.lastMmapCheck </span><span class="cov0" title="0">{
                                // Index changed - reload it with retry for EOF errors
                                if err := s.loadIndexWithRetry(); err != nil </span><span class="cov0" title="0">{
                                        // Try to handle missing directory gracefully
                                        if s.handleMissingShardDirectory(err) </span><span class="cov0" title="0">{
                                                s.lastMmapCheck = currentTimestamp
                                        }</span> else<span class="cov0" title="0"> {
                                                criticalErr = fmt.Errorf("failed to reload index after detecting mmap change: %w", err)
                                                return
                                        }</span>
                                } else<span class="cov0" title="0"> {
                                        s.lastMmapCheck = currentTimestamp
                                }</span>
                        }
                }

                <span class="cov8" title="1">writeReq.IDs = make([]MessageID, len(entries))
                now := startTime.UnixNano()

                // Build write buffers from pre-compressed data (minimal work under lock)
                writeReq.WriteBuffers = make([][]byte, 0, len(entries)*2) // headers + data

                // Allocate header buffer for this specific request (no sharing)
                requiredSize := len(entries) * headerSize
                allHeaders := make([]byte, requiredSize)

                // Store initial state for rollback if write fails
                initialEntryNumber := s.index.CurrentEntryNumber
                initialWriteOffset := s.index.CurrentWriteOffset
                initialWritesSinceCheckpoint := s.writesSinceCheckpoint

                // Single-process mode: fast path with immediate updates
                if !config.Concurrency.EnableMultiProcessMode </span><span class="cov8" title="1">{
                        for i, compressedEntry := range compressedEntries </span><span class="cov8" title="1">{
                                totalOriginalBytes += compressedEntry.OriginalSize
                                totalCompressedBytes += compressedEntry.CompressedSize

                                if compressedEntry.WasCompressed </span><span class="cov8" title="1">{
                                        compressedCount++
                                }</span> else<span class="cov8" title="1"> {
                                        skippedCount++
                                }</span>

                                // Use slice of pre-allocated header buffer
                                <span class="cov8" title="1">headerStart := i * headerSize
                                header := allHeaders[headerStart : headerStart+headerSize]
                                binary.LittleEndian.PutUint32(header[0:4], uint32(len(compressedEntry.Data)))
                                binary.LittleEndian.PutUint64(header[4:12], uint64(now))

                                // Add to vectored write batch
                                writeReq.WriteBuffers = append(writeReq.WriteBuffers, header, compressedEntry.Data)

                                // Track entry in binary index - only store at intervals for memory efficiency
                                entryNumber := s.index.CurrentEntryNumber
                                entrySize := int64(headerSize + len(compressedEntry.Data))

                                // Calculate the correct file index and offset for this entry
                                fileIndex := len(s.index.Files) - 1
                                byteOffset := s.index.CurrentWriteOffset

                                // If this entry will cause rotation and we're not at the start of a file,
                                // it will be written to the NEXT file at offset 0
                                willCauseRotation := s.index.CurrentWriteOffset+entrySize &gt; config.Storage.MaxFileSize
                                if willCauseRotation &amp;&amp; s.index.CurrentWriteOffset &gt; 0 </span><span class="cov8" title="1">{
                                        fileIndex = len(s.index.Files) // Will be the next file after rotation
                                        byteOffset = 0
                                }</span>

                                <span class="cov8" title="1">position := EntryPosition{
                                        FileIndex:  fileIndex,
                                        ByteOffset: byteOffset,
                                }

                                // Add to binary searchable index (it handles intervals and pruning internally)
                                s.index.BinaryIndex.AddIndexNode(entryNumber, position)

                                // Generate ID for this entry
                                writeReq.IDs[i] = MessageID{EntryNumber: entryNumber, ShardID: s.shardID}

                                // Update index state (will be rolled back if write fails)
                                s.index.CurrentEntryNumber++
                                s.index.CurrentWriteOffset += entrySize
                                s.writesSinceCheckpoint++</span>
                        }
                } else<span class="cov0" title="0"> {
                        // Multi-process mode: use atomic sequence for entry numbers
                        // Track binary index updates for deferred application
                        type binaryIndexUpdate struct {
                                entryNumber int64
                                position    EntryPosition
                        }
                        var binaryIndexUpdates []binaryIndexUpdate

                        // Pre-allocate entry numbers atomically
                        entryNumbers := make([]int64, len(entries))
                        if s.state != nil </span><span class="cov0" title="0">{
                                for i := range entries </span><span class="cov0" title="0">{
                                        // Use CometState for entry number allocation
                                        entryNumbers[i] = s.state.IncrementLastEntryNumber() - 1

                                        if Debug &amp;&amp; s.logger != nil </span><span class="cov0" title="0">{
                                                s.logger.Debug("Allocated entry",
                                                        "shard", s.shardID,
                                                        "entryNumber", entryNumbers[i])
                                        }</span>
                                }
                        } else<span class="cov0" title="0"> {
                                // Fallback if sequence state not initialized
                                baseEntry := s.index.CurrentEntryNumber
                                for i := range entries </span><span class="cov0" title="0">{
                                        entryNumbers[i] = baseEntry + int64(i)

                                        // Track in CometState if available
                                        if s.state != nil </span><span class="cov0" title="0">{
                                                s.state.IncrementLastEntryNumber()
                                        }</span>
                                }
                        }

                        // Track write position
                        <span class="cov0" title="0">writeOffset := s.index.CurrentWriteOffset

                        for i, compressedEntry := range compressedEntries </span><span class="cov0" title="0">{
                                totalOriginalBytes += compressedEntry.OriginalSize
                                totalCompressedBytes += compressedEntry.CompressedSize

                                if compressedEntry.WasCompressed </span><span class="cov0" title="0">{
                                        compressedCount++
                                }</span> else<span class="cov0" title="0"> {
                                        skippedCount++
                                }</span>

                                // Use slice of pre-allocated header buffer
                                <span class="cov0" title="0">headerStart := i * headerSize
                                header := allHeaders[headerStart : headerStart+headerSize]
                                binary.LittleEndian.PutUint32(header[0:4], uint32(len(compressedEntry.Data)))
                                binary.LittleEndian.PutUint64(header[4:12], uint64(now))

                                // Add to vectored write batch
                                writeReq.WriteBuffers = append(writeReq.WriteBuffers, header, compressedEntry.Data)

                                // Track entry in binary index - only store at intervals for memory efficiency
                                entrySize := int64(headerSize + len(compressedEntry.Data))
                                entryNumber := entryNumbers[i]

                                // Calculate the correct file index and offset for this entry
                                fileIndex := len(s.index.Files) - 1
                                byteOffset := writeOffset

                                // If this entry will cause rotation and we're not at the start of a file,
                                // it will be written to the NEXT file at offset 0
                                willCauseRotation := writeOffset+entrySize &gt; config.Storage.MaxFileSize
                                if willCauseRotation &amp;&amp; writeOffset &gt; 0 </span><span class="cov0" title="0">{
                                        fileIndex = len(s.index.Files) // Will be the next file after rotation
                                        byteOffset = 0
                                }</span>

                                <span class="cov0" title="0">position := EntryPosition{
                                        FileIndex:  fileIndex,
                                        ByteOffset: byteOffset,
                                }

                                // Store for later update
                                binaryIndexUpdates = append(binaryIndexUpdates, binaryIndexUpdate{
                                        entryNumber: entryNumber,
                                        position:    position,
                                })

                                // Generate ID for this entry
                                writeReq.IDs[i] = MessageID{EntryNumber: entryNumber, ShardID: s.shardID}

                                // Track write position for next entry
                                writeOffset += entrySize</span>
                        }

                        // Multi-process mode: set up deferred update function
                        // Calculate final entry number (highest allocated + 1)
                        <span class="cov0" title="0">finalEntryNumber := int64(0)
                        if len(entryNumbers) &gt; 0 </span><span class="cov0" title="0">{
                                finalEntryNumber = entryNumbers[len(entryNumbers)-1] + 1
                        }</span>
                        <span class="cov0" title="0">finalWriteOffset := writeOffset

                        // DEBUG: Log entry number calculation
                        if Debug &amp;&amp; s.logger != nil </span><span class="cov0" title="0">{
                                s.logger.Debug("Entry number calculation",
                                        "shard", s.shardID,
                                        "entryNumbers", entryNumbers,
                                        "finalEntryNumber", finalEntryNumber,
                                        "initialEntryNumber", initialEntryNumber)
                        }</span>
                        <span class="cov0" title="0">writeReq.UpdateFunc = func() error </span><span class="cov0" title="0">{
                                s.mu.Lock()
                                defer s.mu.Unlock()

                                // Apply the binary index updates
                                for _, update := range binaryIndexUpdates </span><span class="cov0" title="0">{
                                        s.index.BinaryIndex.AddIndexNode(update.entryNumber, update.position)
                                }</span>

                                // Apply the index updates
                                <span class="cov0" title="0">s.index.CurrentEntryNumber = finalEntryNumber
                                s.index.CurrentWriteOffset = finalWriteOffset
                                s.writesSinceCheckpoint = initialWritesSinceCheckpoint + len(entries)

                                // Update the current file's entry count and end offset
                                if len(s.index.Files) &gt; 0 </span><span class="cov0" title="0">{
                                        s.index.Files[len(s.index.Files)-1].Entries += int64(len(entries))
                                        s.index.Files[len(s.index.Files)-1].EndOffset = finalWriteOffset
                                }</span>

                                // Update mmap state to notify readers
                                <span class="cov0" title="0">if s.state != nil </span><span class="cov0" title="0">{
                                        s.updateMmapState()
                                }</span>

                                // Schedule async checkpoint instead of synchronous
                                <span class="cov0" title="0">if s.writesSinceCheckpoint &gt; 0 &amp;&amp; time.Since(s.lastCheckpoint) &gt; 10*time.Millisecond </span><span class="cov0" title="0">{
                                        s.scheduleAsyncCheckpoint(clientMetrics, config)
                                }</span>

                                <span class="cov0" title="0">return nil</span>
                        }
                }

                // Define rollback function in case write fails (only for single-process mode)
                <span class="cov8" title="1">if !config.Concurrency.EnableMultiProcessMode </span><span class="cov8" title="1">{
                        writeReq.UpdateFunc = func() error </span><span class="cov0" title="0">{
                                // On failure, rollback index state
                                s.mu.Lock()
                                defer s.mu.Unlock()
                                s.index.CurrentEntryNumber = initialEntryNumber
                                s.index.CurrentWriteOffset = initialWriteOffset
                                s.writesSinceCheckpoint = initialWritesSinceCheckpoint
                                return nil
                        }</span>
                }
        }() // End of critical section

        // Perform I/O OUTSIDE the lock
        <span class="cov8" title="1">var writeErr error

        // Use memory-mapped writer for ultra-fast multi-process writes if available
        if config.Concurrency.EnableMultiProcessMode &amp;&amp; s.mmapWriter != nil </span><span class="cov0" title="0">{
                // Extract entry numbers from IDs
                entryNumbers := make([]uint64, len(writeReq.IDs))
                for i, id := range writeReq.IDs </span><span class="cov0" title="0">{
                        entryNumbers[i] = uint64(id.EntryNumber)
                }</span>

                // Extract raw data from write buffers (skip headers, they will be recreated)
                <span class="cov0" title="0">rawEntries := make([][]byte, len(writeReq.IDs))
                for i := 0; i &lt; len(writeReq.IDs); i++ </span><span class="cov0" title="0">{
                        // WriteBuffers contains [header, data, header, data, ...]
                        // Skip the header (index i*2) and get the data (index i*2+1)
                        rawEntries[i] = writeReq.WriteBuffers[i*2+1]
                }</span>

                // Memory-mapped write (handles its own coordination)
                <span class="cov0" title="0">writeErr = s.mmapWriter.Write(rawEntries, entryNumbers)

                // Handle rotation needed error
                if writeErr != nil &amp;&amp; strings.Contains(writeErr.Error(), "rotation needed") </span><span class="cov0" title="0">{
                        // Must acquire mutex before rotating to avoid race conditions
                        s.mu.Lock()
                        rotErr := s.rotateFile(clientMetrics, config)
                        s.mu.Unlock()

                        if rotErr != nil </span><span class="cov0" title="0">{
                                return nil, fmt.Errorf("failed to rotate file: %w", rotErr)
                        }</span>

                        // Retry the write after rotation
                        <span class="cov0" title="0">writeErr = s.mmapWriter.Write(rawEntries, entryNumbers)</span>
                }

                // Handle missing directory errors in mmap mode
                <span class="cov0" title="0">if writeErr != nil &amp;&amp; s.handleMissingShardDirectory(writeErr) </span><span class="cov0" title="0">{
                        // Directory was recovered, need to reinitialize the mmap writer
                        s.mu.Lock()
                        if err := s.initializeMmapWriter(config); err != nil </span><span class="cov0" title="0">{
                                s.mu.Unlock()
                                return nil, fmt.Errorf("failed to reinitialize mmap writer after recovery: %w", err)
                        }</span>
                        <span class="cov0" title="0">s.mu.Unlock()

                        // Retry the write operation once after recovery
                        writeErr = s.mmapWriter.Write(rawEntries, entryNumbers)</span>
                }

                // Update index after successful mmap write - must be protected by mutex
                <span class="cov0" title="0">if writeErr == nil </span><span class="cov0" title="0">{
                        s.mu.Lock()
                        if s.mmapWriter.state != nil </span><span class="cov0" title="0">{
                                s.index.CurrentWriteOffset = int64(s.mmapWriter.state.GetWriteOffset())
                        }</span>
                        <span class="cov0" title="0">s.mu.Unlock()</span>
                }
        } else<span class="cov8" title="1"> {
                // Regular write path with file locking
                if config.Concurrency.EnableMultiProcessMode &amp;&amp; s.lockFile != nil </span><span class="cov0" title="0">{
                        if err := syscall.Flock(int(s.lockFile.Fd()), syscall.LOCK_EX); err != nil </span><span class="cov0" title="0">{
                                return nil, fmt.Errorf("failed to acquire shard lock for write: %w", err)
                        }</span>
                        <span class="cov0" title="0">defer syscall.Flock(int(s.lockFile.Fd()), syscall.LOCK_UN)</span>
                }

                <span class="cov8" title="1">s.writeMu.Lock()
                // Write all buffers
                for _, buf := range writeReq.WriteBuffers </span><span class="cov8" title="1">{
                        if _, err := s.writer.Write(buf); err != nil </span><span class="cov0" title="0">{
                                writeErr = err
                                break</span>
                        }
                }
                <span class="cov8" title="1">if writeErr == nil </span><span class="cov8" title="1">{
                        writeErr = s.writer.Flush()
                        // In multi-process mode, sync to ensure data hits disk before releasing lock
                        if writeErr == nil &amp;&amp; config.Concurrency.EnableMultiProcessMode &amp;&amp; s.dataFile != nil </span><span class="cov0" title="0">{
                                writeErr = s.dataFile.Sync()
                        }</span>
                }
                <span class="cov8" title="1">s.writeMu.Unlock()</span>
        }

        // Handle write error
        <span class="cov8" title="1">if writeErr != nil </span><span class="cov0" title="0">{
                // Check if this is a missing directory error that we can recover from
                if s.handleMissingShardDirectory(writeErr) </span><span class="cov0" title="0">{
                        // Directory was recovered, need to reinitialize the writer
                        s.mu.Lock()
                        if err := s.ensureWriter(config); err != nil </span><span class="cov0" title="0">{
                                s.mu.Unlock()
                                return nil, fmt.Errorf("failed to reinitialize writer after recovery: %w", err)
                        }</span>
                        <span class="cov0" title="0">s.mu.Unlock()

                        // Retry the write operation once after recovery
                        s.writeMu.Lock()
                        writeErr = nil // Reset error
                        for _, buf := range writeReq.WriteBuffers </span><span class="cov0" title="0">{
                                if _, err := s.writer.Write(buf); err != nil </span><span class="cov0" title="0">{
                                        writeErr = err
                                        break</span>
                                }
                        }
                        <span class="cov0" title="0">if writeErr == nil </span><span class="cov0" title="0">{
                                writeErr = s.writer.Flush()
                                // In multi-process mode, sync to ensure data hits disk before releasing lock
                                if writeErr == nil &amp;&amp; config.Concurrency.EnableMultiProcessMode &amp;&amp; s.dataFile != nil </span><span class="cov0" title="0">{
                                        writeErr = s.dataFile.Sync()
                                }</span>
                        }
                        <span class="cov0" title="0">s.writeMu.Unlock()</span>
                }

                // If write still failed after recovery attempt, return error
                <span class="cov0" title="0">if writeErr != nil </span><span class="cov0" title="0">{
                        // Track error
                        clientMetrics.ErrorCount.Add(1)
                        clientMetrics.LastErrorNano.Store(uint64(time.Now().UnixNano()))

                        // Track in CometState if available
                        if s.state != nil </span><span class="cov0" title="0">{
                                atomic.AddUint64(&amp;s.state.ErrorCount, 1)
                                atomic.StoreInt64(&amp;s.state.LastErrorNanos, time.Now().UnixNano())
                                // Track failed write batch
                                atomic.AddUint64(&amp;s.state.FailedWrites, 1)
                        }</span>

                        // Rollback index state
                        <span class="cov0" title="0">if writeReq.UpdateFunc != nil </span><span class="cov0" title="0">{
                                writeReq.UpdateFunc()
                        }</span>

                        <span class="cov0" title="0">return nil, fmt.Errorf("failed write: %w", writeErr)</span>
                }
        }

        // Apply deferred updates for multi-process mode BEFORE post-write operations
        <span class="cov8" title="1">if config.Concurrency.EnableMultiProcessMode &amp;&amp; writeReq.UpdateFunc != nil </span><span class="cov0" title="0">{
                if err := writeReq.UpdateFunc(); err != nil </span><span class="cov0" title="0">{
                        return nil, fmt.Errorf("failed to update index after write: %w", err)
                }</span>
        }

        // Track metrics
        <span class="cov8" title="1">writeLatency := uint64(time.Since(startTime).Nanoseconds())
        clientMetrics.TotalEntries.Add(uint64(len(entries)))
        clientMetrics.TotalBytes.Add(totalOriginalBytes)
        clientMetrics.TotalCompressed.Add(totalCompressedBytes)
        clientMetrics.CompressedEntries.Add(compressedCount)
        clientMetrics.SkippedCompression.Add(skippedCount)

        // Track unified state metrics (if available)
        if s.state != nil </span><span class="cov8" title="1">{
                atomic.AddInt64(&amp;s.state.TotalEntries, int64(len(entries)))
                atomic.AddUint64(&amp;s.state.TotalBytes, totalOriginalBytes)
                atomic.AddUint64(&amp;s.state.TotalWrites, 1)
                atomic.StoreInt64(&amp;s.state.LastWriteNanos, time.Now().UnixNano())
                atomic.AddUint64(&amp;s.state.TotalCompressed, totalCompressedBytes)
                atomic.AddUint64(&amp;s.state.CompressedEntries, compressedCount)
                atomic.AddUint64(&amp;s.state.SkippedCompression, skippedCount)

                // Batch metrics
                atomic.StoreUint64(&amp;s.state.CurrentBatchSize, uint64(len(entries)))
                atomic.AddUint64(&amp;s.state.TotalBatches, 1)
        }</span>

        // Update latency metrics using EMA
        <span class="cov8" title="1">if totalOriginalBytes &gt; 0 &amp;&amp; totalCompressedBytes &gt; 0 </span><span class="cov8" title="1">{
                ratio := (totalOriginalBytes * 100) / totalCompressedBytes // x100 for fixed point
                clientMetrics.CompressionRatio.Store(ratio)
                // Also update in shard state
                if s.state != nil </span><span class="cov8" title="1">{
                        atomic.StoreUint64(&amp;s.state.CompressionRatio, ratio)
                }</span>
        }

        // Track write latency with min/max
        <span class="cov8" title="1">clientMetrics.WriteLatencyNano.Store(writeLatency)

        // Update unified state latency metrics (if available)
        if s.state != nil </span><span class="cov8" title="1">{
                s.state.UpdateWriteLatency(writeLatency)
        }</span>
        <span class="cov8" title="1">for </span><span class="cov8" title="1">{
                currentMin := clientMetrics.MinWriteLatency.Load()
                if currentMin == 0 || writeLatency &lt; currentMin </span><span class="cov8" title="1">{
                        if clientMetrics.MinWriteLatency.CompareAndSwap(currentMin, writeLatency) </span><span class="cov8" title="1">{
                                break</span>
                        }
                } else<span class="cov8" title="1"> {
                        break</span>
                }
        }
        <span class="cov8" title="1">for </span><span class="cov8" title="1">{
                currentMax := clientMetrics.MaxWriteLatency.Load()
                if writeLatency &gt; currentMax </span><span class="cov8" title="1">{
                        if clientMetrics.MaxWriteLatency.CompareAndSwap(currentMax, writeLatency) </span><span class="cov8" title="1">{
                                break</span>
                        }
                } else<span class="cov8" title="1"> {
                        break</span>
                }
        }

        // Post-write operations
        <span class="cov8" title="1">func() </span><span class="cov8" title="1">{
                s.mu.Lock()
                defer s.mu.Unlock()

                // In single-process mode, update file entry count after successful write
                // (Multi-process mode updates this in the UpdateFunc)
                if !config.Concurrency.EnableMultiProcessMode &amp;&amp; len(s.index.Files) &gt; 0 </span><span class="cov8" title="1">{
                        currentFile := &amp;s.index.Files[len(s.index.Files)-1]
                        currentFile.Entries += int64(len(entries))
                }</span>

                // In multi-process mode, update unified state immediately for visibility
                <span class="cov8" title="1">if config.Concurrency.EnableMultiProcessMode &amp;&amp; s.state != nil </span><span class="cov0" title="0">{
                        // Fast path: Update coordination state immediately (~10ns total)
                        s.state.SetLastIndexUpdate(time.Now().UnixNano())
                        // LastEntryNumber should be the last allocated entry, not the next one
                        // CurrentEntryNumber is the next entry to be written
                        if s.index.CurrentEntryNumber &gt; 0 </span><span class="cov0" title="0">{
                                // Sync CometState with current index if needed
                                currentLastEntry := s.state.GetLastEntryNumber()
                                expectedLastEntry := s.index.CurrentEntryNumber - 1
                                if currentLastEntry &lt; expectedLastEntry </span><span class="cov0" title="0">{
                                        // Catch up to the expected value
                                        for s.state.GetLastEntryNumber() &lt; expectedLastEntry </span><span class="cov0" title="0">{
                                                s.state.IncrementLastEntryNumber()
                                        }</span>
                                }
                        }

                        // Note: Async checkpointing happens in UpdateFunc now
                } else<span class="cov8" title="1"> {
                        // Single-process mode: use regular checkpointing
                        s.maybeCheckpoint(clientMetrics, config)
                }</span>

                // Check if we need to rotate file
                <span class="cov8" title="1">if s.index.CurrentWriteOffset &gt; config.Storage.MaxFileSize </span><span class="cov8" title="1">{
                        if err := s.rotateFile(clientMetrics, config); err != nil </span><span class="cov0" title="0">{
                                criticalErr = fmt.Errorf("failed to rotate file: %w", err)
                                return
                        }</span>
                }
        }()

        // Check if critical section encountered an error
        <span class="cov8" title="1">if criticalErr != nil </span><span class="cov0" title="0">{
                return nil, criticalErr
        }</span>

        <span class="cov8" title="1">return writeReq.IDs, nil</span>
}

// maybeCheckpoint persists the index if needed
func (s *Shard) maybeCheckpoint(clientMetrics *ClientMetrics, config *CometConfig) <span class="cov8" title="1">{
        if time.Since(s.lastCheckpoint) &gt; time.Duration(config.Storage.CheckpointTime)*time.Millisecond </span><span class="cov0" title="0">{

                // Flush and sync writer
                if s.writer != nil </span><span class="cov0" title="0">{
                        s.writeMu.Lock()
                        s.writer.Flush()
                        if s.dataFile != nil </span><span class="cov0" title="0">{
                                s.dataFile.Sync()
                        }</span>
                        <span class="cov0" title="0">s.writeMu.Unlock()</span>
                }

                // Update current file info
                <span class="cov0" title="0">if len(s.index.Files) &gt; 0 </span><span class="cov0" title="0">{
                        current := &amp;s.index.Files[len(s.index.Files)-1]
                        current.EndOffset = s.index.CurrentWriteOffset
                        current.EndTime = time.Now()
                }</span>

                // Clone index while holding lock (caller holds the lock)
                <span class="cov0" title="0">indexCopy := s.cloneIndex()
                s.writesSinceCheckpoint = 0
                s.lastCheckpoint = time.Now()

                // Persist index in background to avoid blocking
                s.wg.Add(1)
                go func() </span><span class="cov0" title="0">{
                        defer s.wg.Done()

                        // For multi-process safety, use the separate index lock if enabled
                        var err error
                        if config.Concurrency.EnableMultiProcessMode &amp;&amp; s.indexLockFile != nil </span><span class="cov0" title="0">{
                                // Acquire exclusive lock for index writes
                                if err := syscall.Flock(int(s.indexLockFile.Fd()), syscall.LOCK_EX); err != nil </span><span class="cov0" title="0">{
                                        if clientMetrics != nil </span><span class="cov0" title="0">{
                                                clientMetrics.IndexPersistErrors.Add(1)
                                                clientMetrics.ErrorCount.Add(1)
                                                clientMetrics.LastErrorNano.Store(uint64(time.Now().UnixNano()))
                                        }</span>

                                        // Track in CometState if available
                                        <span class="cov0" title="0">if s.state != nil </span><span class="cov0" title="0">{
                                                atomic.AddUint64(&amp;s.state.IndexPersistErrors, 1)
                                                atomic.AddUint64(&amp;s.state.ErrorCount, 1)
                                                atomic.StoreInt64(&amp;s.state.LastErrorNanos, time.Now().UnixNano())
                                        }</span>
                                        <span class="cov0" title="0">return</span>
                                }

                                // Reload index from disk to merge with other processes' changes
                                <span class="cov0" title="0">s.indexMu.Lock()
                                if _, statErr := os.Stat(s.indexPath); statErr == nil </span><span class="cov0" title="0">{
                                        // Index exists, load it to get latest state
                                        if diskIndex, loadErr := s.loadBinaryIndex(); loadErr == nil </span><span class="cov0" title="0">{
                                                // Merge our changes with the disk state
                                                // Keep the highest entry number and write offset
                                                if diskIndex.CurrentEntryNumber &gt; indexCopy.CurrentEntryNumber </span><span class="cov0" title="0">{
                                                        indexCopy.CurrentEntryNumber = diskIndex.CurrentEntryNumber
                                                }</span>
                                                <span class="cov0" title="0">if diskIndex.CurrentWriteOffset &gt; indexCopy.CurrentWriteOffset </span><span class="cov0" title="0">{
                                                        indexCopy.CurrentWriteOffset = diskIndex.CurrentWriteOffset
                                                }</span>

                                                // Merge consumer offsets - keep the highest offset for each consumer
                                                <span class="cov0" title="0">for group, offset := range diskIndex.ConsumerOffsets </span><span class="cov0" title="0">{
                                                        if currentOffset, exists := indexCopy.ConsumerOffsets[group]; !exists || offset &gt; currentOffset </span><span class="cov0" title="0">{
                                                                indexCopy.ConsumerOffsets[group] = offset
                                                        }</span>
                                                }

                                                // Merge file info - use the disk version as base
                                                <span class="cov0" title="0">if len(diskIndex.Files) &gt; 0 </span><span class="cov0" title="0">{
                                                        indexCopy.Files = diskIndex.Files
                                                }</span>

                                                // Merge binary index nodes if needed
                                                <span class="cov0" title="0">if diskIndex.BinaryIndex.Nodes != nil &amp;&amp; len(diskIndex.BinaryIndex.Nodes) &gt; len(indexCopy.BinaryIndex.Nodes) </span><span class="cov0" title="0">{
                                                        indexCopy.BinaryIndex = diskIndex.BinaryIndex
                                                }</span>
                                        }
                                }

                                // Now save the merged index
                                <span class="cov0" title="0">err = s.saveBinaryIndex(indexCopy)
                                s.indexMu.Unlock()

                                // Update mmap state BEFORE releasing lock
                                if err == nil </span><span class="cov0" title="0">{
                                        s.updateMmapState()
                                }</span>

                                // Release lock immediately
                                <span class="cov0" title="0">syscall.Flock(int(s.indexLockFile.Fd()), syscall.LOCK_UN)</span>
                        } else<span class="cov0" title="0"> {
                                // No file locking - just use process-local mutex
                                s.indexMu.Lock()
                                err = s.saveBinaryIndex(indexCopy)
                                s.indexMu.Unlock()

                                if err == nil </span><span class="cov0" title="0">{
                                        s.updateMmapState()
                                }</span>
                        }

                        <span class="cov0" title="0">if err != nil </span><span class="cov0" title="0">{
                                // Track error in metrics - next checkpoint will retry
                                if clientMetrics != nil </span><span class="cov0" title="0">{
                                        clientMetrics.IndexPersistErrors.Add(1)
                                }</span>
                        }
                }()

                // Track checkpoint metrics (only if metrics are available)
                <span class="cov0" title="0">if clientMetrics != nil </span><span class="cov0" title="0">{
                        clientMetrics.CheckpointCount.Add(1)
                        clientMetrics.LastCheckpoint.Store(uint64(time.Now().UnixNano()))
                }</span>

                // Track in CometState if available
                <span class="cov0" title="0">if s.state != nil </span><span class="cov0" title="0">{
                        atomic.AddUint64(&amp;s.state.CheckpointCount, 1)
                        atomic.StoreInt64(&amp;s.state.LastCheckpointNanos, time.Now().UnixNano())
                }</span>
        }
}

// handleMissingShardDirectory handles the case where a shard directory was manually deleted
// Returns true if the error was handled, false if it should be propagated
func (s *Shard) handleMissingShardDirectory(err error) bool <span class="cov0" title="0">{
        // Check if this is a missing directory error
        if strings.Contains(err.Error(), "no such file or directory") ||
                strings.Contains(err.Error(), "failed to read shard directory") </span><span class="cov0" title="0">{
                // Shard directory was deleted - recreate it and reset state
                shardDir := filepath.Dir(s.indexPath)
                if mkdirErr := os.MkdirAll(shardDir, 0755); mkdirErr != nil </span><span class="cov0" title="0">{
                        // If we can't recreate the directory, this is a serious error
                        return false
                }</span>
                // Reset shard state to empty - it will be rebuilt as needed
                <span class="cov0" title="0">s.index.Files = nil
                s.index.CurrentFile = ""
                s.index.CurrentEntryNumber = 0
                s.index.CurrentWriteOffset = 0
                return true</span>
        }
        <span class="cov0" title="0">return false</span>
}

// loadIndexWithRecovery loads the index and handles missing directory gracefully
// Returns nil if the index was loaded or missing directory was recovered
func (s *Shard) loadIndexWithRecovery() error <span class="cov0" title="0">{
        if err := s.loadIndexWithRetry(); err != nil </span><span class="cov0" title="0">{
                if s.handleMissingShardDirectory(err) </span><span class="cov0" title="0">{
                        return nil // Successfully recovered from missing directory
                }</span>
                <span class="cov0" title="0">return err</span> // Other error, propagate it
        }
        <span class="cov0" title="0">return nil</span>
}

// ensureWriter ensures the writer is properly initialized after recovery
// This function should be called with the shard mutex held
func (s *Shard) ensureWriter(config *CometConfig) error <span class="cov0" title="0">{
        // Get the shard directory from the index path
        shardDir := filepath.Dir(s.indexPath)

        // Make sure the shard directory exists
        if err := os.MkdirAll(shardDir, 0755); err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to create shard directory during recovery: %w", err)
        }</span>

        // Close existing writer and data file if they exist
        <span class="cov0" title="0">if s.writer != nil </span><span class="cov0" title="0">{
                s.writer.Flush() // Ignore error since we're recovering
                s.writer = nil
        }</span>
        <span class="cov0" title="0">if s.dataFile != nil </span><span class="cov0" title="0">{
                s.dataFile.Close() // Ignore error since we're recovering
                s.dataFile = nil
        }</span>

        // Reinitialize the data file and writer
        <span class="cov0" title="0">return s.openDataFileWithConfig(shardDir, config)</span>
}

// initializeMmapWriter initializes the memory-mapped writer after recovery
// This function should be called with the shard mutex held
func (s *Shard) initializeMmapWriter(config *CometConfig) error <span class="cov0" title="0">{
        // Get the shard directory from the index path
        shardDir := filepath.Dir(s.indexPath)

        // Make sure the shard directory exists
        if err := os.MkdirAll(shardDir, 0755); err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to create shard directory during mmap recovery: %w", err)
        }</span>

        // Close existing mmap writer if it exists
        <span class="cov0" title="0">if s.mmapWriter != nil </span><span class="cov0" title="0">{
                s.mmapWriter.Close() // Ignore error since we're recovering
                s.mmapWriter = nil
        }</span>

        // Create new mmap writer - use existing state
        <span class="cov0" title="0">mmapWriter, err := NewMmapWriter(shardDir, config.Storage.MaxFileSize, s.index, s.state, s.rotationLockFile)
        if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to create mmap writer: %w", err)
        }</span>

        <span class="cov0" title="0">s.mmapWriter = mmapWriter
        return nil</span>
}

// loadIndexWithRetry loads the index with retry logic for EOF errors
// This handles the race condition between file writes and mmap state updates
func (s *Shard) loadIndexWithRetry() error <span class="cov0" title="0">{
        var err error
        for attempt := 0; attempt &lt; 3; attempt++ </span><span class="cov0" title="0">{
                err = s.loadIndex()
                if err == nil </span><span class="cov0" title="0">{
                        return nil
                }</span>
                // Retry on EOF and file size errors to handle race conditions
                <span class="cov0" title="0">if (strings.Contains(err.Error(), "unexpected EOF") ||
                        strings.Contains(err.Error(), "index file too small")) &amp;&amp; attempt &lt; 2 </span><span class="cov0" title="0">{
                        time.Sleep(time.Duration(attempt+1) * time.Millisecond) // 1ms, 2ms
                        continue</span>
                }
                <span class="cov0" title="0">break</span>
        }
        <span class="cov0" title="0">return err</span>
}

// lazyRebuildIndexIfNeeded checks if the index needs rebuilding based on file sizes
// This is called on read path in multi-process mode to ensure consistency
// Caller must hold the shard lock
// checkIfRebuildNeeded checks if the index needs rebuilding without acquiring locks
// Must be called while holding at least a read lock
func (s *Shard) checkIfRebuildNeeded() bool <span class="cov0" title="0">{
        if s.mmapWriter == nil || s.mmapWriter.state == nil </span><span class="cov0" title="0">{
                return false
        }</span>

        <span class="cov0" title="0">currentTotalWrites := s.mmapWriter.state.GetTotalWrites()

        // Check if total writes in state exceed what we have in index
        return currentTotalWrites &gt; uint64(s.index.CurrentEntryNumber)</span>
}

func (s *Shard) lazyRebuildIndexIfNeeded(config CometConfig, shardDir string) <span class="cov0" title="0">{
        if len(s.index.Files) == 0 </span><span class="cov0" title="0">{
                return
        }</span>

        // Check if any file has grown beyond what the index knows
        <span class="cov0" title="0">needsRebuild := false
        for _, file := range s.index.Files </span><span class="cov0" title="0">{
                if stat, err := os.Stat(file.Path); err == nil </span><span class="cov0" title="0">{
                        actualSize := stat.Size()
                        if actualSize &gt; file.EndOffset </span><span class="cov0" title="0">{
                                needsRebuild = true
                                break</span>
                        }
                }
        }

        <span class="cov0" title="0">if !needsRebuild </span><span class="cov0" title="0">{
                return
        }</span>

        // Rebuild the index by scanning ALL files
        // In multi-process mode, we can't trust entry counts from partial indexes
        <span class="cov0" title="0">totalEntries := int64(0)
        newBinaryNodes := make([]EntryIndexNode, 0)
        currentEntryNum := int64(0)

        for i := range s.index.Files </span><span class="cov0" title="0">{
                file := &amp;s.index.Files[i]

                // Get actual file size
                stat, err := os.Stat(file.Path)
                if err != nil </span><span class="cov0" title="0">{
                        continue</span>
                }
                <span class="cov0" title="0">actualSize := stat.Size()

                // In multi-process mode, read the shared coordination state to get actual data size
                scanSize := actualSize
                if config.Concurrency.EnableMultiProcessMode &amp;&amp; i == len(s.index.Files)-1 </span><span class="cov0" title="0">{
                        // For the current file, check the coordination state
                        statePath := filepath.Join(shardDir, "comet.state")
                        if stateFile, err := os.Open(statePath); err == nil </span><span class="cov0" title="0">{
                                defer stateFile.Close()

                                // Map the state temporarily to read it
                                if data, err := syscall.Mmap(int(stateFile.Fd()), 0, CometStateSize,
                                        syscall.PROT_READ, syscall.MAP_SHARED); err == nil </span><span class="cov0" title="0">{
                                        defer syscall.Munmap(data)

                                        // Read the write offset from the shared state
                                        // In multi-process scenarios, retry a few times to handle timing issues
                                        state := (*CometState)(unsafe.Pointer(&amp;data[0]))
                                        var writeOffset int64
                                        var totalWrites int64
                                        maxRetries := 3

                                        for retry := 0; retry &lt; maxRetries; retry++ </span><span class="cov0" title="0">{
                                                writeOffset = int64(atomic.LoadUint64(&amp;state.WriteOffset))
                                                lastWrite := atomic.LoadInt64(&amp;state.LastWriteNanos)
                                                totalWrites = int64(atomic.LoadUint64(&amp;state.TotalWrites))

                                                // Check if we expect more writes than we see in the offset
                                                // If TotalWrites is significantly higher than what the offset suggests, wait for coordination to stabilize
                                                averageBytesPerWrite := int64(50) // Rough estimate
                                                expectedMinOffset := totalWrites * averageBytesPerWrite

                                                if totalWrites &gt; 100 &amp;&amp; writeOffset &lt; expectedMinOffset/2 </span><span class="cov0" title="0">{
                                                        time.Sleep(100 * time.Millisecond)
                                                        continue</span>
                                                }

                                                // If we see a recent write timestamp, the offset should be stable
                                                <span class="cov0" title="0">if lastWrite &gt; 0 &amp;&amp; time.Since(time.Unix(0, lastWrite)) &lt; 100*time.Millisecond </span><span class="cov0" title="0">{
                                                        break</span>
                                                }

                                                // If this is our first try and offset seems small, wait a bit
                                                <span class="cov0" title="0">if retry == 0 &amp;&amp; writeOffset &lt; 1000 </span><span class="cov0" title="0">{ // Less than 1KB suggests incomplete writes
                                                        time.Sleep(50 * time.Millisecond)
                                                        continue</span>
                                                }

                                                <span class="cov0" title="0">break</span>
                                        }

                                        // Get the latest state values for stabilization check
                                        <span class="cov0" title="0">latestTotalWrites := totalWrites

                                        // Wait for state to stabilize if we're in a high-contention scenario
                                        if latestTotalWrites &gt; 100 </span><span class="cov0" title="0">{

                                                // Wait for a brief period to let any pending writes complete
                                                stableTotalWrites := latestTotalWrites
                                                stableWriteOffset := writeOffset

                                                for attempts := 0; attempts &lt; 10; attempts++ </span><span class="cov0" title="0">{
                                                        time.Sleep(50 * time.Millisecond)
                                                        currentWrites := int64(atomic.LoadUint64(&amp;state.TotalWrites))
                                                        currentOffset := int64(atomic.LoadUint64(&amp;state.WriteOffset))

                                                        if currentWrites == stableTotalWrites &amp;&amp; currentOffset == stableWriteOffset </span><span class="cov0" title="0">{
                                                                // Coordination state is stable
                                                                break</span>
                                                        }

                                                        <span class="cov0" title="0">stableTotalWrites = currentWrites
                                                        stableWriteOffset = currentOffset</span>
                                                }

                                                // Use the stabilized values
                                                <span class="cov0" title="0">writeOffset = stableWriteOffset</span>
                                        }

                                        // In multi-process mode, if we have a large pre-allocated file (&gt;100KB),
                                        // scan the entire file rather than trusting coordination state for scanning bounds
                                        // This handles cases where coordination state updates are delayed
                                        <span class="cov0" title="0">if actualSize &gt; 100*1024 </span><span class="cov0" title="0">{ // File is larger than 100KB - likely pre-allocated
                                                scanSize = actualSize
                                        }</span> else<span class="cov0" title="0"> if writeOffset &gt; 0 &amp;&amp; writeOffset &lt; actualSize </span><span class="cov0" title="0">{
                                                scanSize = writeOffset
                                        }</span> else<span class="cov0" title="0"> if writeOffset &gt;= actualSize </span>{<span class="cov0" title="0">
                                                // Use file scan - this handles large pre-allocated files
                                        }</span>
                                }
                        } else<span class="cov0" title="0"> if s.mmapWriter != nil </span><span class="cov0" title="0">{
                                // Fallback to local mmapWriter if available
                                if s.mmapWriter.state != nil </span><span class="cov0" title="0">{
                                        writeOffset := s.mmapWriter.state.GetWriteOffset()
                                        if writeOffset &gt; 0 &amp;&amp; writeOffset &lt; uint64(actualSize) </span><span class="cov0" title="0">{
                                                scanSize = int64(writeOffset)
                                        }</span>
                                }
                        }
                }

                // Always scan the entire file in multi-process mode
                // Don't trust the entry count from the partial index
                <span class="cov0" title="0">scanResult := s.scanFileForEntries(file.Path, scanSize, i, currentEntryNum)

                if scanResult.entryCount &gt; 0 </span><span class="cov0" title="0">{
                        file.Entries = scanResult.entryCount
                        file.EndOffset = actualSize
                        newBinaryNodes = append(newBinaryNodes, scanResult.indexNodes...)
                        totalEntries += scanResult.entryCount
                        currentEntryNum += scanResult.entryCount
                }</span>
        }

        // Update index state
        <span class="cov0" title="0">if totalEntries &gt; s.index.CurrentEntryNumber </span><span class="cov0" title="0">{
                s.index.CurrentEntryNumber = totalEntries
        }</span>

        // Update binary index if we found new nodes
        <span class="cov0" title="0">if len(newBinaryNodes) &gt; 0 </span><span class="cov0" title="0">{
                // Merge with existing nodes
                nodeMap := make(map[int64]EntryIndexNode)
                for _, node := range s.index.BinaryIndex.Nodes </span><span class="cov0" title="0">{
                        nodeMap[node.EntryNumber] = node
                }</span>
                <span class="cov0" title="0">for _, node := range newBinaryNodes </span><span class="cov0" title="0">{
                        nodeMap[node.EntryNumber] = node
                }</span>

                // Convert back to sorted slice
                <span class="cov0" title="0">s.index.BinaryIndex.Nodes = make([]EntryIndexNode, 0, len(nodeMap))
                for _, node := range nodeMap </span><span class="cov0" title="0">{
                        s.index.BinaryIndex.Nodes = append(s.index.BinaryIndex.Nodes, node)
                }</span>
                <span class="cov0" title="0">sort.Slice(s.index.BinaryIndex.Nodes, func(i, j int) bool </span><span class="cov0" title="0">{
                        return s.index.BinaryIndex.Nodes[i].EntryNumber &lt; s.index.BinaryIndex.Nodes[j].EntryNumber
                }</span>)
        }

        // Update write offset to match last file
        <span class="cov0" title="0">if len(s.index.Files) &gt; 0 </span><span class="cov0" title="0">{
                lastFile := &amp;s.index.Files[len(s.index.Files)-1]
                s.index.CurrentWriteOffset = lastFile.EndOffset
        }</span>
}

// scanFileResult holds the results of scanning a data file
type scanFileResult struct {
        entryCount int64
        indexNodes []EntryIndexNode
}

// scanFileForEntries scans a data file to count entries and rebuild index
// This is used in multi-process mode to ensure we capture all entries
func (s *Shard) scanFileForEntries(filePath string, fileSize int64, fileIndex int, startEntryNum int64) scanFileResult <span class="cov0" title="0">{
        f, err := os.Open(filePath)
        if err != nil </span><span class="cov0" title="0">{
                return scanFileResult{}
        }</span>
        <span class="cov0" title="0">defer f.Close()

        result := scanFileResult{
                indexNodes: make([]EntryIndexNode, 0),
        }

        var offset int64
        var entryNum int64 = startEntryNum
        interval := s.index.BinaryIndex.IndexInterval
        if interval &lt;= 0 </span><span class="cov0" title="0">{
                interval = 100 // Default interval
        }</span>

        <span class="cov0" title="0">for offset &lt; fileSize </span><span class="cov0" title="0">{
                // Read header
                headerBuf := make([]byte, headerSize)
                n, err := f.ReadAt(headerBuf, offset)
                if err != nil || n != headerSize </span><span class="cov0" title="0">{
                        break</span>
                }

                // Parse header
                <span class="cov0" title="0">length := binary.LittleEndian.Uint32(headerBuf[0:4])
                timestamp := binary.LittleEndian.Uint64(headerBuf[4:12])

                // Check for uninitialized memory (zeros) - AGGRESSIVE GAP SKIPPING
                if length == 0 &amp;&amp; timestamp == 0 </span><span class="cov0" title="0">{

                        found := false

                        // NUCLEAR APPROACH: Search every 4 bytes until we find a valid header
                        // This ensures we NEVER miss entries due to gaps
                        for searchOffset := offset + 4; searchOffset &lt;= fileSize-headerSize; searchOffset += 4 </span><span class="cov0" title="0">{
                                searchBuf := make([]byte, headerSize)
                                if n, err := f.ReadAt(searchBuf, searchOffset); err == nil &amp;&amp; n == headerSize </span><span class="cov0" title="0">{
                                        searchLength := binary.LittleEndian.Uint32(searchBuf[0:4])
                                        searchTimestamp := binary.LittleEndian.Uint64(searchBuf[4:12])

                                        // More lenient validation for gap recovery
                                        if searchLength &gt; 0 &amp;&amp; searchLength &lt;= 10*1024*1024 &amp;&amp; searchTimestamp &gt; 0 </span><span class="cov0" title="0">{
                                                // Found a valid entry!
                                                offset = searchOffset
                                                found = true
                                                break</span>
                                        }
                                }
                        }

                        <span class="cov0" title="0">if !found </span><span class="cov0" title="0">{
                                break</span>
                        }

                        // Continue scanning from the recovered position
                        <span class="cov0" title="0">continue</span>
                }

                // Validate entry bounds
                <span class="cov0" title="0">if length == 0 || length &gt; 100*1024*1024 </span><span class="cov0" title="0">{ // Sanity check: max 100MB per entry
                        break</span>
                }

                // Check if full entry fits in file
                <span class="cov0" title="0">nextOffset := offset + headerSize + int64(length)
                if nextOffset &gt; fileSize </span><span class="cov0" title="0">{
                        break</span>
                }

                // Add to binary index at intervals
                <span class="cov0" title="0">if entryNum%int64(interval) == 0 </span><span class="cov0" title="0">{
                        result.indexNodes = append(result.indexNodes, EntryIndexNode{
                                EntryNumber: entryNum,
                                Position: EntryPosition{
                                        FileIndex:  fileIndex,
                                        ByteOffset: offset,
                                },
                        })
                }</span>

                <span class="cov0" title="0">result.entryCount++
                entryNum++
                offset = nextOffset</span>
        }

        <span class="cov0" title="0">return result</span>
}

// scheduleAsyncCheckpoint schedules an asynchronous checkpoint for multi-process mode
// This allows writes to return immediately while index persistence happens in background
func (s *Shard) scheduleAsyncCheckpoint(clientMetrics *ClientMetrics, config *CometConfig) <span class="cov0" title="0">{
        // Clone index while holding lock (caller holds the lock)
        indexCopy := s.cloneIndex()
        s.writesSinceCheckpoint = 0
        s.lastCheckpoint = time.Now()

        // Persist index in background to avoid blocking writes
        s.wg.Add(1)
        go func() </span><span class="cov0" title="0">{
                defer s.wg.Done()

                // For multi-process safety, use the separate index lock
                if s.indexLockFile != nil </span><span class="cov0" title="0">{
                        // Acquire exclusive lock for index writes
                        if err := syscall.Flock(int(s.indexLockFile.Fd()), syscall.LOCK_EX); err != nil </span><span class="cov0" title="0">{
                                if clientMetrics != nil </span><span class="cov0" title="0">{
                                        clientMetrics.IndexPersistErrors.Add(1)
                                        clientMetrics.ErrorCount.Add(1)
                                        clientMetrics.LastErrorNano.Store(uint64(time.Now().UnixNano()))
                                }</span>
                                <span class="cov0" title="0">return</span>
                        }
                        <span class="cov0" title="0">defer syscall.Flock(int(s.indexLockFile.Fd()), syscall.LOCK_UN)</span>
                }

                // Serialize index writes to prevent file corruption
                <span class="cov0" title="0">s.indexMu.Lock()
                err := s.saveBinaryIndex(indexCopy)
                s.indexMu.Unlock()

                if err != nil </span><span class="cov0" title="0">{
                        // Track error in metrics - next checkpoint will retry
                        if clientMetrics != nil </span><span class="cov0" title="0">{
                                clientMetrics.IndexPersistErrors.Add(1)
                                clientMetrics.ErrorCount.Add(1)
                                clientMetrics.LastErrorNano.Store(uint64(time.Now().UnixNano()))
                        }</span>

                        // Track in CometState if available
                        <span class="cov0" title="0">if s.state != nil </span><span class="cov0" title="0">{
                                atomic.AddUint64(&amp;s.state.IndexPersistErrors, 1)
                                atomic.AddUint64(&amp;s.state.ErrorCount, 1)
                                atomic.StoreInt64(&amp;s.state.LastErrorNanos, time.Now().UnixNano())
                        }</span>
                } else<span class="cov0" title="0"> {
                        // Track checkpoint metrics
                        if clientMetrics != nil </span><span class="cov0" title="0">{
                                clientMetrics.CheckpointCount.Add(1)
                                clientMetrics.LastCheckpoint.Store(uint64(time.Now().UnixNano()))
                        }</span>

                        // Track in CometState if available
                        <span class="cov0" title="0">if s.state != nil </span><span class="cov0" title="0">{
                                atomic.AddUint64(&amp;s.state.CheckpointCount, 1)
                                atomic.StoreInt64(&amp;s.state.LastCheckpointNanos, time.Now().UnixNano())
                        }</span>
                }
        }()
}

// cloneIndex creates a deep copy of the index for safe serialization
// IMPORTANT: Caller must hold the shard mutex (either read or write lock)
func (s *Shard) cloneIndex() *ShardIndex <span class="cov0" title="0">{
        // Create a deep copy of the index
        clone := &amp;ShardIndex{
                CurrentEntryNumber: s.index.CurrentEntryNumber,
                CurrentWriteOffset: s.index.CurrentWriteOffset,
                BoundaryInterval:   s.index.BoundaryInterval,
                CurrentFile:        s.index.CurrentFile,
        }

        // Deep copy maps

        clone.ConsumerOffsets = make(map[string]int64, len(s.index.ConsumerOffsets))
        for k, v := range s.index.ConsumerOffsets </span><span class="cov0" title="0">{
                clone.ConsumerOffsets[k] = v
        }</span>

        // Deep copy binary index
        <span class="cov0" title="0">clone.BinaryIndex.IndexInterval = s.index.BinaryIndex.IndexInterval
        clone.BinaryIndex.MaxNodes = s.index.BinaryIndex.MaxNodes
        clone.BinaryIndex.Nodes = make([]EntryIndexNode, len(s.index.BinaryIndex.Nodes))
        copy(clone.BinaryIndex.Nodes, s.index.BinaryIndex.Nodes)

        // Deep copy files
        clone.Files = make([]FileInfo, len(s.index.Files))
        copy(clone.Files, s.index.Files)

        return clone</span>
}

// persistIndex atomically writes the index to disk
// IMPORTANT: Caller must hold the shard mutex (either read or write lock)
func (s *Shard) persistIndex() error <span class="cov0" title="0">{
        // Clone the index - caller already holds lock
        indexCopy := s.cloneIndex()

        // Calculate total file bytes while we have the index
        if s.state != nil </span><span class="cov0" title="0">{
                var totalBytes uint64
                for _, file := range s.index.Files </span><span class="cov0" title="0">{
                        totalBytes += uint64(file.EndOffset - file.StartOffset)
                }</span>
                <span class="cov0" title="0">atomic.StoreUint64(&amp;s.state.TotalFileBytes, totalBytes)
                // Also update current files count
                atomic.StoreUint64(&amp;s.state.CurrentFiles, uint64(len(s.index.Files)))</span>
        }

        // Use binary format for efficiency
        <span class="cov0" title="0">err := s.saveBinaryIndex(indexCopy)
        if err == nil </span><span class="cov0" title="0">{
                // Only update mmap state after successful persistence
                s.updateMmapState()

                // Track successful index persistence
                if s.state != nil </span><span class="cov0" title="0">{
                        atomic.AddUint64(&amp;s.state.IndexPersistCount, 1)
                        atomic.StoreInt64(&amp;s.state.LastIndexUpdate, time.Now().UnixNano())
                        // Update binary index node count
                        atomic.StoreUint64(&amp;s.state.BinaryIndexNodes, uint64(len(s.index.BinaryIndex.Nodes)))
                }</span>
        } else<span class="cov0" title="0"> if s.state != nil </span><span class="cov0" title="0">{
                // Track failed index persistence
                atomic.AddUint64(&amp;s.state.IndexPersistErrors, 1)
        }</span>
        <span class="cov0" title="0">return err</span>
}

// discoverDataFiles scans the shard directory for all data files
// This is critical for multi-process mode where other processes may have created files
func (s *Shard) discoverDataFiles() error <span class="cov0" title="0">{
        shardDir := filepath.Dir(s.indexPath)
        entries, err := os.ReadDir(shardDir)
        if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to read shard directory: %w", err)
        }</span>

        // Collect all data files
        <span class="cov0" title="0">var dataFiles []string
        for _, entry := range entries </span><span class="cov0" title="0">{
                if entry.IsDir() </span><span class="cov0" title="0">{
                        continue</span>
                }
                <span class="cov0" title="0">name := entry.Name()
                if strings.HasPrefix(name, "log-") &amp;&amp; strings.HasSuffix(name, ".comet") </span><span class="cov0" title="0">{
                        dataFiles = append(dataFiles, filepath.Join(shardDir, name))
                }</span>
        }

        <span class="cov0" title="0">if len(dataFiles) == 0 </span><span class="cov0" title="0">{
                return nil // No files yet
        }</span>

        // Sort files by name (which includes sequence number)
        <span class="cov0" title="0">sort.Strings(dataFiles)

        // Build file info for any files not already in our index
        existingFiles := make(map[string]bool)
        for _, f := range s.index.Files </span><span class="cov0" title="0">{
                existingFiles[f.Path] = true
        }</span>

        // Add any newly discovered files
        <span class="cov0" title="0">for _, filePath := range dataFiles </span><span class="cov0" title="0">{
                if !existingFiles[filePath] </span><span class="cov0" title="0">{
                        // Get file info
                        info, err := os.Stat(filePath)
                        if err != nil </span><span class="cov0" title="0">{
                                continue</span> // Skip files we can't stat
                        }

                        // Add to index with basic info
                        // The actual entry count and offsets will be updated during scanning
                        <span class="cov0" title="0">s.index.Files = append(s.index.Files, FileInfo{
                                Path:        filePath,
                                StartOffset: 0,
                                EndOffset:   info.Size(),
                                StartTime:   info.ModTime(),
                                EndTime:     info.ModTime(),
                                Entries:     0, // Will be updated during scan
                        })</span>
                }
        }

        // Update current file to the latest one
        <span class="cov0" title="0">if len(dataFiles) &gt; 0 </span><span class="cov0" title="0">{
                s.index.CurrentFile = dataFiles[len(dataFiles)-1]
        }</span>

        <span class="cov0" title="0">return nil</span>
}

// loadIndex loads the index from disk
func (s *Shard) loadIndex() error <span class="cov8" title="1">{
        // In multi-process mode, always scan for new files first
        if s.lockFile != nil </span><span class="cov0" title="0">{ // Multi-process mode indicator
                if err := s.discoverDataFiles(); err != nil </span><span class="cov0" title="0">{
                        return fmt.Errorf("failed to discover data files: %w", err)
                }</span>
        }

        // Check if index file exists
        <span class="cov8" title="1">if _, err := os.Stat(s.indexPath); os.IsNotExist(err) </span><span class="cov8" title="1">{
                // No index file - attempt to rebuild from data files
                if s.index == nil </span><span class="cov0" title="0">{
                        // Create default index structure
                        s.index = &amp;ShardIndex{
                                BoundaryInterval: defaultBoundaryInterval,
                                ConsumerOffsets:  make(map[string]int64),
                        }
                }</span>

                // Try to rebuild index from existing data files
                <span class="cov8" title="1">shardDir := filepath.Dir(s.indexPath)
                // Track recovery attempt
                if s.state != nil </span><span class="cov8" title="1">{
                        atomic.AddUint64(&amp;s.state.RecoveryAttempts, 1)
                }</span>
                <span class="cov8" title="1">if err := s.rebuildIndexFromDataFiles(shardDir); err != nil </span><span class="cov0" title="0">{
                        // Log the error but don't fail - empty index is better than nothing
                        // This allows new shards to start fresh
                        if len(s.index.Files) &gt; 0 </span><span class="cov0" title="0">{
                                // Only return error if we found files but couldn't read them
                                return fmt.Errorf("failed to rebuild index from data files: %w", err)
                        }</span>
                }

                // Persist the rebuilt index
                <span class="cov8" title="1">if len(s.index.Files) &gt; 0 </span><span class="cov0" title="0">{
                        if err := s.persistIndex(); err != nil </span><span class="cov0" title="0">{
                                return fmt.Errorf("failed to persist rebuilt index: %w", err)
                        }</span>
                        // Track recovery success if we successfully rebuilt from files
                        <span class="cov0" title="0">if s.state != nil </span><span class="cov0" title="0">{
                                atomic.AddUint64(&amp;s.state.RecoverySuccesses, 1)
                        }</span>
                }

                <span class="cov8" title="1">return nil</span>
        }

        // Save current config values before loading
        <span class="cov0" title="0">boundaryInterval := s.index.BoundaryInterval
        maxIndexEntries := s.index.BinaryIndex.MaxNodes

        // Load binary index
        index, err := s.loadBinaryIndex()
        if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to load binary index: %w", err)
        }</span>

        <span class="cov0" title="0">s.index = index
        // Restore config values
        s.index.BoundaryInterval = boundaryInterval
        s.index.BinaryIndex.IndexInterval = boundaryInterval
        s.index.BinaryIndex.MaxNodes = maxIndexEntries

        // In multi-process mode, verify and update file sizes
        if s.lockFile != nil </span><span class="cov0" title="0">{
                for i := range s.index.Files </span><span class="cov0" title="0">{
                        file := &amp;s.index.Files[i]
                        if info, err := os.Stat(file.Path); err == nil </span><span class="cov0" title="0">{
                                actualSize := info.Size()
                                if actualSize &gt; file.EndOffset </span><span class="cov0" title="0">{
                                        file.EndOffset = actualSize
                                }</span>
                        }
                }

                // Don't update current write offset based on file size
                // The index already has the correct write offset from when it was saved
                // For mmap files, the file size doesn't reflect the actual data written
        }

        <span class="cov0" title="0">return nil</span>
}

// NOTE: initMmapState and initSequenceState removed - replaced by initCometState

// Global sequence counter for single-process mode
var globalSequenceCounter int64

// getNextSequence atomically increments and returns the next sequence number for file naming
func (s *Shard) getNextSequence() int64 <span class="cov8" title="1">{
        if s.state != nil </span><span class="cov8" title="1">{
                // Multi-process mode: use unified state's file sequence
                return int64(s.state.AddLastFileSequence(1))
        }</span>
        // Single-process mode: use global atomic counter
        <span class="cov0" title="0">return atomic.AddInt64(&amp;globalSequenceCounter, 1)</span>
}

// rebuildIndexFromDataFiles scans all data files and rebuilds the index from scratch
// This is used for disaster recovery when the index file is lost or corrupted
func (s *Shard) rebuildIndexFromDataFiles(shardDir string) error <span class="cov8" title="1">{
        // Find all data files
        entries, err := os.ReadDir(shardDir)
        if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to read shard directory: %w", err)
        }</span>

        // Collect and sort data files by sequence number
        <span class="cov8" title="1">var dataFiles []string
        for _, entry := range entries </span><span class="cov0" title="0">{
                if entry.IsDir() </span><span class="cov0" title="0">{
                        continue</span>
                }
                <span class="cov0" title="0">name := entry.Name()
                if strings.HasPrefix(name, "log-") &amp;&amp; strings.HasSuffix(name, ".comet") </span><span class="cov0" title="0">{
                        dataFiles = append(dataFiles, filepath.Join(shardDir, name))
                }</span>
        }

        <span class="cov8" title="1">if len(dataFiles) == 0 </span><span class="cov8" title="1">{
                return nil // No files to rebuild from
        }</span>

        // Sort files by name (which includes sequence number)
        <span class="cov0" title="0">sort.Strings(dataFiles)

        // Reset index state
        s.index.Files = make([]FileInfo, 0, len(dataFiles))
        s.index.BinaryIndex.Nodes = make([]EntryIndexNode, 0)
        s.index.CurrentEntryNumber = 0
        s.index.CurrentWriteOffset = 0

        // Scan each file to rebuild the index
        for _, filePath := range dataFiles </span><span class="cov0" title="0">{
                fileInfo, err := s.scanDataFile(filePath)
                if err != nil </span><span class="cov0" title="0">{
                        // Skip corrupted files but continue with others
                        continue</span>
                }

                // Add to index
                <span class="cov0" title="0">s.index.Files = append(s.index.Files, *fileInfo)

                // Update current file if this is the last one
                if filePath == dataFiles[len(dataFiles)-1] </span><span class="cov0" title="0">{
                        s.index.CurrentFile = filePath
                        s.index.CurrentWriteOffset = fileInfo.EndOffset
                }</span>
        }

        // Update total entry count
        <span class="cov0" title="0">if len(s.index.Files) &gt; 0 </span><span class="cov0" title="0">{
                lastFile := s.index.Files[len(s.index.Files)-1]
                s.index.CurrentEntryNumber = lastFile.StartEntry + lastFile.Entries
        }</span>

        // Track recovery success
        <span class="cov0" title="0">if s.state != nil </span><span class="cov0" title="0">{
                atomic.AddUint64(&amp;s.state.RecoverySuccesses, 1)
        }</span>

        <span class="cov0" title="0">return nil</span>
}

// scanDataFile reads a data file and extracts metadata for index rebuilding
func (s *Shard) scanDataFile(filePath string) (*FileInfo, error) <span class="cov0" title="0">{
        file, err := os.Open(filePath)
        if err != nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("failed to open data file: %w", err)
        }</span>
        <span class="cov0" title="0">defer file.Close()

        stat, err := file.Stat()
        if err != nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("failed to stat file: %w", err)
        }</span>

        <span class="cov0" title="0">fileInfo := &amp;FileInfo{
                Path:        filePath,
                StartOffset: 0,
                EndOffset:   stat.Size(),
                StartTime:   stat.ModTime(), // Use modification time as approximation
                EndTime:     stat.ModTime(),
        }

        // Determine starting entry number
        if len(s.index.Files) &gt; 0 </span><span class="cov0" title="0">{
                prevFile := s.index.Files[len(s.index.Files)-1]
                fileInfo.StartEntry = prevFile.StartEntry + prevFile.Entries
        }</span> else<span class="cov0" title="0"> {
                fileInfo.StartEntry = 0
        }</span>

        // Scan through the file to count entries and build index nodes
        <span class="cov0" title="0">offset := int64(0)
        entryCount := int64(0)
        buffer := make([]byte, 12) // Header size

        for offset &lt; stat.Size() </span><span class="cov0" title="0">{
                // Read header
                n, err := file.ReadAt(buffer, offset)
                if err != nil || n &lt; 12 </span><span class="cov0" title="0">{
                        break</span> // End of file or corrupted
                }

                // Parse header
                <span class="cov0" title="0">length := binary.LittleEndian.Uint32(buffer[0:4])
                timestamp := binary.LittleEndian.Uint64(buffer[4:12])

                // Validate entry
                if length &gt; 100*1024*1024 </span><span class="cov0" title="0">{ // 100MB max
                        // Track corruption detection
                        if s.state != nil </span><span class="cov0" title="0">{
                                atomic.AddUint64(&amp;s.state.CorruptionDetected, 1)
                        }</span>
                        <span class="cov0" title="0">break</span> // Corrupted entry
                }

                // Update timestamps
                <span class="cov0" title="0">entryTime := time.Unix(0, int64(timestamp))
                if entryCount == 0 </span><span class="cov0" title="0">{
                        fileInfo.StartTime = entryTime
                }</span>
                <span class="cov0" title="0">fileInfo.EndTime = entryTime

                // Add to binary index at intervals
                if entryCount%int64(s.index.BoundaryInterval) == 0 </span><span class="cov0" title="0">{
                        s.index.BinaryIndex.AddIndexNode(fileInfo.StartEntry+entryCount, EntryPosition{
                                FileIndex:  len(s.index.Files), // Current file index
                                ByteOffset: offset,
                        })
                }</span>

                // Move to next entry
                <span class="cov0" title="0">entrySize := int64(12 + length)
                offset += entrySize
                entryCount++</span>
        }

        <span class="cov0" title="0">fileInfo.Entries = entryCount
        fileInfo.EndOffset = offset

        return fileInfo, nil</span>
}

// updateMmapState atomically updates the shared timestamp to notify other processes of index changes
func (s *Shard) updateMmapState() <span class="cov0" title="0">{
        // Check if state is still valid
        if s.state != nil </span><span class="cov0" title="0">{
                s.state.SetLastIndexUpdate(time.Now().UnixNano())
        }</span>
}

// openDataFileWithConfig opens the current data file for appending with optional config for multi-process safety
func (s *Shard) openDataFileWithConfig(shardDir string, config *CometConfig) error <span class="cov8" title="1">{
        // For multi-process safety, acquire exclusive lock when creating files
        if config != nil &amp;&amp; config.Concurrency.EnableMultiProcessMode &amp;&amp; s.lockFile != nil </span><span class="cov0" title="0">{
                if err := syscall.Flock(int(s.lockFile.Fd()), syscall.LOCK_EX); err != nil </span><span class="cov0" title="0">{
                        return fmt.Errorf("failed to acquire lock for file creation: %w", err)
                }</span>
                <span class="cov0" title="0">defer syscall.Flock(int(s.lockFile.Fd()), syscall.LOCK_UN)</span>
        }

        <span class="cov8" title="1">if s.index.CurrentFile == "" </span><span class="cov8" title="1">{
                // Create first file with sequential number
                seqNum := s.getNextSequence()
                s.index.CurrentFile = filepath.Join(shardDir, fmt.Sprintf("log-%016d.comet", seqNum))
                s.index.Files = append(s.index.Files, FileInfo{
                        Path:        s.index.CurrentFile,
                        StartOffset: 0,
                        StartEntry:  0,
                        StartTime:   time.Now(),
                        Entries:     0,
                })
        }</span>

        // Open file for appending
        <span class="cov8" title="1">file, err := os.OpenFile(s.index.CurrentFile, os.O_APPEND|os.O_CREATE|os.O_WRONLY, 0644)
        if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to open data file: %w", err)
        }</span>

        <span class="cov8" title="1">s.dataFile = file

        // Track file creation if this is a new file
        if s.state != nil </span><span class="cov8" title="1">{
                // Check if file was newly created
                fileInfo, err := file.Stat()
                if err == nil &amp;&amp; fileInfo.Size() == 0 </span><span class="cov8" title="1">{
                        // New file created
                        atomic.AddUint64(&amp;s.state.FilesCreated, 1)
                }</span>
                // Update current file count
                <span class="cov8" title="1">atomic.StoreUint64(&amp;s.state.CurrentFiles, uint64(len(s.index.Files)))</span>
        }

        // Create buffered writer
        // In multi-process mode, use smaller buffer to reduce conflicts
        <span class="cov8" title="1">bufSize := defaultBufSize
        if s.lockFile != nil </span><span class="cov0" title="0">{ // Multi-process mode
                bufSize = 8192 // 8KB buffer for more frequent flushes
        }</span>
        <span class="cov8" title="1">s.writer = bufio.NewWriterSize(s.dataFile, bufSize)

        // Set up compressor with fastest level for better throughput
        enc, err := zstd.NewWriter(nil, zstd.WithEncoderLevel(zstd.SpeedFastest))
        if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to create compressor: %w", err)
        }</span>
        <span class="cov8" title="1">s.compressor = enc

        return nil</span>
}

// recoverFromCrash scans from the last checkpoint to find actual EOF
func (s *Shard) recoverFromCrash() error <span class="cov8" title="1">{
        info, err := s.dataFile.Stat()
        if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to stat data file: %w", err)
        }</span>

        <span class="cov8" title="1">actualSize := info.Size()
        if actualSize &lt;= s.index.CurrentWriteOffset </span><span class="cov8" title="1">{
                s.index.CurrentWriteOffset = actualSize
                return nil
        }</span>

        // Scan forward from index offset to validate entries
        <span class="cov0" title="0">offset := s.index.CurrentWriteOffset
        validEntries := int64(0)
        var lastEntryOffset int64
        partialWriteDetected := false

        // Open file for reading
        f, err := os.Open(s.index.CurrentFile)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov0" title="0">defer f.Close()

        // Scan entries until EOF or corruption
        for offset &lt; actualSize </span><span class="cov0" title="0">{
                if offset+headerSize &gt; actualSize </span><span class="cov0" title="0">{
                        partialWriteDetected = true
                        break</span> // Incomplete header
                }

                <span class="cov0" title="0">header := make([]byte, headerSize)
                if _, err := f.ReadAt(header, offset); err != nil </span><span class="cov0" title="0">{
                        break</span>
                }

                <span class="cov0" title="0">length := binary.LittleEndian.Uint32(header[0:4])
                if offset+headerSize+int64(length) &gt; actualSize </span><span class="cov0" title="0">{
                        partialWriteDetected = true
                        break</span> // Incomplete entry
                }

                // Skip zero-length entries (likely uninitialized file regions)
                <span class="cov0" title="0">if length == 0 </span><span class="cov0" title="0">{
                        // Check if this is just zeros (uninitialized data)
                        timestamp := binary.LittleEndian.Uint64(header[4:12])
                        if timestamp == 0 </span><span class="cov0" title="0">{
                                // This is uninitialized data, not a real entry
                                break</span>
                        }
                }

                // Entry looks valid
                <span class="cov0" title="0">lastEntryOffset = offset
                offset += headerSize + int64(length)
                validEntries++</span>
        }

        // Update state
        <span class="cov0" title="0">s.index.CurrentWriteOffset = offset
        s.index.CurrentEntryNumber += validEntries

        // Update the current file's entry count
        if len(s.index.Files) &gt; 0 &amp;&amp; validEntries &gt; 0 </span><span class="cov0" title="0">{
                s.index.Files[len(s.index.Files)-1].Entries += validEntries
        }</span>

        // Store the last valid entry position in binary index if needed
        <span class="cov0" title="0">if validEntries &gt; 0 </span><span class="cov0" title="0">{
                s.index.BinaryIndex.AddIndexNode(s.index.CurrentEntryNumber-1, EntryPosition{
                        FileIndex:  len(s.index.Files) - 1,
                        ByteOffset: lastEntryOffset,
                })
        }</span>

        // Track partial write if detected
        <span class="cov0" title="0">if partialWriteDetected &amp;&amp; s.state != nil </span><span class="cov0" title="0">{
                atomic.AddUint64(&amp;s.state.PartialWrites, 1)
        }</span>

        <span class="cov0" title="0">return nil</span>
}

// rotateFile closes current file and starts a new one
// NOTE: This method assumes the caller holds s.mu (main shard mutex)
func (s *Shard) rotateFile(clientMetrics *ClientMetrics, config *CometConfig) error <span class="cov8" title="1">{
        // Handle mmap writer rotation
        if s.mmapWriter != nil </span><span class="cov0" title="0">{
                // Update final stats for current file BEFORE rotation
                if len(s.index.Files) &gt; 0 </span><span class="cov0" title="0">{
                        current := &amp;s.index.Files[len(s.index.Files)-1]
                        current.EndOffset = s.index.CurrentWriteOffset
                        current.EndTime = time.Now()
                }</span>

                // Ensure the shard directory exists before rotation
                <span class="cov0" title="0">shardDir := filepath.Dir(s.index.CurrentFile)
                if err := os.MkdirAll(shardDir, 0755); err != nil </span><span class="cov0" title="0">{
                        return fmt.Errorf("failed to create shard directory during mmap rotation: %w", err)
                }</span>

                // Let mmap writer handle its own file rotation
                <span class="cov0" title="0">if err := s.mmapWriter.rotateFile(); err != nil </span><span class="cov0" title="0">{
                        return fmt.Errorf("failed to rotate mmap file: %w", err)
                }</span>

                // Update index to reflect new file from mmap writer
                // The mmap writer will have updated its internal path
                <span class="cov0" title="0">seqNum := s.getNextSequence()
                newPath := filepath.Join(shardDir, fmt.Sprintf("log-%016d.comet", seqNum))
                s.index.CurrentFile = newPath

                // Add new file to index
                s.index.Files = append(s.index.Files, FileInfo{
                        Path:        newPath,
                        StartOffset: 0, // Mmap files always start at 0
                        StartEntry:  s.index.CurrentEntryNumber,
                        StartTime:   time.Now(),
                        Entries:     0,
                })

                // Track file rotation
                clientMetrics.FileRotations.Add(1)
                clientMetrics.TotalFiles.Add(1)
                if s.state != nil </span><span class="cov0" title="0">{
                        atomic.AddUint64(&amp;s.state.FileRotations, 1)
                }</span>

                // Debug log file rotation
                <span class="cov0" title="0">if Debug &amp;&amp; s.logger != nil </span><span class="cov0" title="0">{
                        var oldFile string
                        if len(s.index.Files) &gt; 1 </span><span class="cov0" title="0">{
                                oldFile = s.index.Files[len(s.index.Files)-2].Path
                        }</span>
                        <span class="cov0" title="0">s.logger.Debug("File rotated",
                                "oldFile", filepath.Base(oldFile),
                                "newFile", filepath.Base(newPath),
                                "totalFiles", len(s.index.Files))</span>
                }

                // Don't persist index on every rotation - it will be persisted during checkpoints
                // This avoids performance issues with frequent rotations

                // But update the metrics that would have been updated by persistIndex
                <span class="cov0" title="0">if s.state != nil </span><span class="cov0" title="0">{
                        // Update file count metric
                        atomic.StoreUint64(&amp;s.state.CurrentFiles, uint64(len(s.index.Files)))

                        // Calculate and update total file size
                        var totalBytes uint64
                        for _, file := range s.index.Files </span><span class="cov0" title="0">{
                                totalBytes += uint64(file.EndOffset - file.StartOffset)
                        }</span>
                        <span class="cov0" title="0">atomic.StoreUint64(&amp;s.state.TotalFileBytes, totalBytes)</span>
                }

                <span class="cov0" title="0">return nil</span>
        }

        // Regular file writer rotation - acquire write lock to ensure no writes are in progress
        <span class="cov8" title="1">s.writeMu.Lock()
        defer s.writeMu.Unlock()

        // Regular file writer rotation
        // Close direct writer
        if s.writer != nil </span><span class="cov8" title="1">{
                s.writer.Flush()
        }</span>

        // Close compressor
        <span class="cov8" title="1">if s.compressor != nil </span><span class="cov8" title="1">{
                s.compressor.Close()
        }</span>

        // Close current file (safe now that we hold writeMu)
        <span class="cov8" title="1">if err := s.dataFile.Close(); err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to close data file: %w", err)
        }</span>

        // Update final stats for current file
        <span class="cov8" title="1">if len(s.index.Files) &gt; 0 </span><span class="cov8" title="1">{
                current := &amp;s.index.Files[len(s.index.Files)-1]
                current.EndOffset = s.index.CurrentWriteOffset
                current.EndTime = time.Now()
        }</span>

        // For multi-process safety, acquire exclusive lock when creating new file
        <span class="cov8" title="1">if config.Concurrency.EnableMultiProcessMode &amp;&amp; s.lockFile != nil </span><span class="cov0" title="0">{
                if err := syscall.Flock(int(s.lockFile.Fd()), syscall.LOCK_EX); err != nil </span><span class="cov0" title="0">{
                        return fmt.Errorf("failed to acquire lock for file rotation: %w", err)
                }</span>
                <span class="cov0" title="0">defer syscall.Flock(int(s.lockFile.Fd()), syscall.LOCK_UN)</span>
        }

        // Create new file with sequential number
        <span class="cov8" title="1">shardDir := filepath.Dir(s.index.CurrentFile)
        seqNum := s.getNextSequence()
        s.index.CurrentFile = filepath.Join(shardDir, fmt.Sprintf("log-%016d.comet", seqNum))

        // Add new file to index
        s.index.Files = append(s.index.Files, FileInfo{
                Path:        s.index.CurrentFile,
                StartOffset: s.index.CurrentWriteOffset,
                StartEntry:  s.index.CurrentEntryNumber,
                StartTime:   time.Now(),
                Entries:     0,
        })

        // Track file rotation
        clientMetrics.FileRotations.Add(1)
        clientMetrics.TotalFiles.Add(1)
        if s.state != nil </span><span class="cov8" title="1">{
                atomic.AddUint64(&amp;s.state.FileRotations, 1)
        }</span>

        // Ensure the shard directory exists before trying to create the new file
        <span class="cov8" title="1">if err := os.MkdirAll(shardDir, 0755); err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to create shard directory during rotation: %w", err)
        }</span>

        // Open with preallocation and setup
        <span class="cov8" title="1">if err := s.openDataFileWithConfig(shardDir, config); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        // Don't persist index on every rotation - it will be persisted during checkpoints
        // This avoids performance issues with frequent rotations

        // But update the metrics that would have been updated by persistIndex
        <span class="cov8" title="1">if s.state != nil </span><span class="cov8" title="1">{
                // Update file count metric
                atomic.StoreUint64(&amp;s.state.CurrentFiles, uint64(len(s.index.Files)))

                // Calculate and update total file size
                var totalBytes uint64
                for _, file := range s.index.Files </span><span class="cov8" title="1">{
                        totalBytes += uint64(file.EndOffset - file.StartOffset)
                }</span>
                <span class="cov8" title="1">atomic.StoreUint64(&amp;s.state.TotalFileBytes, totalBytes)</span>
        }

        <span class="cov8" title="1">return nil</span>
}

// parseShardFromStream extracts shard ID from stream name
func parseShardFromStream(stream string) (uint32, error) <span class="cov8" title="1">{
        // Expected format: "events:v1:shard:0042"
        // Use strings.LastIndex to find the last colon, then parse what follows
        lastColonIdx := -1
        for i := len(stream) - 1; i &gt;= 0; i-- </span><span class="cov8" title="1">{
                if stream[i] == ':' </span><span class="cov8" title="1">{
                        lastColonIdx = i
                        break</span>
                }
        }

        <span class="cov8" title="1">if lastColonIdx == -1 || lastColonIdx == len(stream)-1 </span><span class="cov0" title="0">{
                return 0, fmt.Errorf("invalid stream format: missing shard number")
        }</span>

        // Parse the number after the last colon
        <span class="cov8" title="1">numStr := stream[lastColonIdx+1:]
        var shardID uint32

        // Manual parsing to avoid fmt.Sscanf overhead
        for _, char := range numStr </span><span class="cov8" title="1">{
                if char &lt; '0' || char &gt; '9' </span><span class="cov0" title="0">{
                        return 0, fmt.Errorf("invalid shard number: contains non-digit")
                }</span>
                <span class="cov8" title="1">digit := uint32(char - '0')
                if shardID &gt; (^uint32(0)-digit)/10 </span><span class="cov0" title="0">{
                        return 0, fmt.Errorf("invalid shard number: overflow")
                }</span>
                <span class="cov8" title="1">shardID = shardID*10 + digit</span>
        }

        <span class="cov8" title="1">return shardID, nil</span>
}

// Close gracefully shuts down the client
func (c *Client) Close() error <span class="cov8" title="1">{
        c.mu.Lock()
        if c.closed </span><span class="cov0" title="0">{
                c.mu.Unlock()
                return nil
        }</span>
        <span class="cov8" title="1">c.closed = true

        // Stop retention manager - close the channel while holding lock to prevent races
        var shouldWait bool
        if c.stopCh != nil </span><span class="cov8" title="1">{
                close(c.stopCh)
                shouldWait = true
        }</span>
        <span class="cov8" title="1">c.mu.Unlock()

        // Wait for retention manager to finish AFTER releasing the lock
        // This prevents deadlock where retention goroutine needs RLock while we hold Lock
        if shouldWait </span><span class="cov8" title="1">{
                c.retentionWg.Wait()
        }</span>

        // Re-acquire lock for shard cleanup
        <span class="cov8" title="1">c.mu.Lock()
        defer c.mu.Unlock()

        // Close all shards
        for _, shard := range c.shards </span><span class="cov8" title="1">{
                shard.mu.Lock()

                // Final checkpoint - pass nil for metrics since we're shutting down
                // Use the actual config for final checkpoint
                shard.maybeCheckpoint(nil, &amp;c.config)

                // Acquire write lock to ensure no writes are in progress
                shard.writeMu.Lock()

                // Close direct writer
                if shard.writer != nil </span><span class="cov8" title="1">{
                        shard.writer.Flush()
                }</span>

                // Close compressor
                <span class="cov8" title="1">if shard.compressor != nil </span><span class="cov8" title="1">{
                        shard.compressor.Close()
                }</span>

                // Close file (safe now that we hold writeMu)
                <span class="cov8" title="1">if shard.dataFile != nil </span><span class="cov8" title="1">{
                        shard.dataFile.Close()
                }</span>

                // Close memory-mapped writer
                <span class="cov8" title="1">if shard.mmapWriter != nil </span><span class="cov0" title="0">{
                        shard.mmapWriter.Close()
                }</span>

                <span class="cov8" title="1">shard.writeMu.Unlock()
                shard.mu.Unlock()

                // Wait for background operations to complete BEFORE closing files
                shard.wg.Wait()

                // Now safe to close lock files (no more background goroutines using them)
                shard.mu.Lock()
                if shard.lockFile != nil </span><span class="cov0" title="0">{
                        shard.lockFile.Close()
                }</span>
                <span class="cov8" title="1">if shard.indexLockFile != nil </span><span class="cov0" title="0">{
                        shard.indexLockFile.Close()
                }</span>
                <span class="cov8" title="1">if shard.retentionLockFile != nil </span><span class="cov0" title="0">{
                        shard.retentionLockFile.Close()
                }</span>
                <span class="cov8" title="1">if shard.rotationLockFile != nil </span><span class="cov0" title="0">{
                        shard.rotationLockFile.Close()
                }</span>
                // NOTE: sequenceFile removed - file sequence now tracked in CometState
                <span class="cov8" title="1">shard.mu.Unlock()

                // Now safe to unmap unified state
                if shard.stateData != nil </span><span class="cov0" title="0">{
                        syscall.Munmap(shard.stateData)
                        shard.stateData = nil
                        shard.state = nil
                }</span>
        }

        <span class="cov8" title="1">return nil</span>
}

// getAllShards returns all shards for testing purposes
func (c *Client) getAllShards() map[uint32]*Shard <span class="cov0" title="0">{
        c.mu.RLock()
        defer c.mu.RUnlock()

        result := make(map[uint32]*Shard, len(c.shards))
        for k, v := range c.shards </span><span class="cov0" title="0">{
                result[k] = v
        }</span>
        <span class="cov0" title="0">return result</span>
}

// GetStats returns current metrics for monitoring
func (c *Client) GetStats() CometStats <span class="cov0" title="0">{
        var totalReaders uint64
        var maxLag uint64
        var totalFiles uint64

        // Aggregate stats from all shards
        c.mu.RLock()
        for _, shard := range c.shards </span><span class="cov0" title="0">{
                readerCount := atomic.LoadInt64(&amp;shard.readerCount)
                totalReaders += uint64(readerCount)

                shard.mu.RLock()
                totalFiles += uint64(len(shard.index.Files))

                // Calculate max consumer lag in ENTRIES (not bytes!)
                for _, consumerEntry := range shard.index.ConsumerOffsets </span><span class="cov0" title="0">{
                        lag := uint64(shard.index.CurrentEntryNumber - consumerEntry)
                        if lag &gt; maxLag </span><span class="cov0" title="0">{
                                maxLag = lag
                        }</span>
                }
                <span class="cov0" title="0">shard.mu.RUnlock()</span>
        }
        <span class="cov0" title="0">c.mu.RUnlock()

        // Update aggregated metrics
        c.metrics.ActiveReaders.Store(totalReaders)
        c.metrics.ConsumerLag.Store(maxLag) // This is now entry lag, not byte lag
        c.metrics.TotalFiles.Store(totalFiles)

        return CometStats{
                TotalEntries:        c.metrics.TotalEntries.Load(),
                TotalBytes:          c.metrics.TotalBytes.Load(),
                TotalCompressed:     c.metrics.TotalCompressed.Load(),
                WriteLatencyNano:    c.metrics.WriteLatencyNano.Load(),
                MinWriteLatency:     c.metrics.MinWriteLatency.Load(),
                MaxWriteLatency:     c.metrics.MaxWriteLatency.Load(),
                CompressionRatio:    c.metrics.CompressionRatio.Load(),
                CompressedEntries:   c.metrics.CompressedEntries.Load(),
                SkippedCompression:  c.metrics.SkippedCompression.Load(),
                TotalFiles:          totalFiles,
                FileRotations:       c.metrics.FileRotations.Load(),
                CheckpointCount:     c.metrics.CheckpointCount.Load(),
                LastCheckpoint:      c.metrics.LastCheckpoint.Load(),
                ActiveReaders:       totalReaders,
                ConsumerLag:         maxLag,
                ErrorCount:          c.metrics.ErrorCount.Load(),
                LastErrorNano:       c.metrics.LastErrorNano.Load(),
                IndexPersistErrors:  c.metrics.IndexPersistErrors.Load(),
                CompressionWaitNano: c.metrics.CompressionWait.Load(),
        }</span>
}

// Health returns the current health status of the Comet client
func (c *Client) Health() Health <span class="cov0" title="0">{
        c.mu.RLock()
        shardCount := len(c.shards)
        c.mu.RUnlock()

        // Get current stats
        stats := c.GetStats()

        // Calculate uptime
        uptime := time.Since(c.startTime)
        uptimeStr := uptime.Truncate(time.Second).String()

        // Determine health status
        health := Health{
                Healthy:      true,
                Status:       "healthy",
                WritesOK:     true,
                ReadsOK:      true,
                ActiveShards: shardCount,
                TotalFiles:   stats.TotalFiles,
                ErrorCount:   stats.ErrorCount,
                Uptime:       uptimeStr,
        }

        // Check last write time across all shards
        var lastWriteNano int64
        c.mu.RLock()
        for _, shard := range c.shards </span><span class="cov0" title="0">{
                if shard.state != nil </span><span class="cov0" title="0">{
                        shardLastWrite := atomic.LoadInt64(&amp;shard.state.LastWriteNanos)
                        if shardLastWrite &gt; lastWriteNano </span><span class="cov0" title="0">{
                                lastWriteNano = shardLastWrite
                        }</span>
                }
        }
        <span class="cov0" title="0">c.mu.RUnlock()

        if lastWriteNano &gt; 0 </span><span class="cov0" title="0">{
                health.LastWriteTime = time.Unix(0, lastWriteNano)
        }</span>

        // Check last error time
        <span class="cov0" title="0">if stats.LastErrorNano &gt; 0 </span><span class="cov0" title="0">{
                health.LastErrorTime = time.Unix(0, int64(stats.LastErrorNano))

                // If error was recent (within 1 minute), mark as degraded
                if time.Since(health.LastErrorTime) &lt; time.Minute </span><span class="cov0" title="0">{
                        health.Status = "degraded"
                        health.Details = "Recent errors detected"
                }</span>
        }

        // Check for high error rate
        <span class="cov0" title="0">if stats.TotalEntries &gt; 0 &amp;&amp; stats.ErrorCount &gt; 0 </span><span class="cov0" title="0">{
                errorRate := float64(stats.ErrorCount) / float64(stats.TotalEntries)
                if errorRate &gt; 0.01 </span><span class="cov0" title="0">{ // More than 1% error rate
                        health.Status = "degraded"
                        health.Healthy = false
                        health.WritesOK = false
                        if health.Details != "" </span><span class="cov0" title="0">{
                                health.Details += "; "
                        }</span>
                        <span class="cov0" title="0">health.Details += fmt.Sprintf("High error rate: %.2f%%", errorRate*100)</span>
                }
        }

        // Check if we've written recently (within 5 minutes)
        <span class="cov0" title="0">if health.LastWriteTime.IsZero() || time.Since(health.LastWriteTime) &gt; 5*time.Minute </span><span class="cov0" title="0">{
                // No recent writes, but this might be normal for low-traffic systems
                // Don't mark as unhealthy, just note it
                if stats.TotalEntries == 0 </span><span class="cov0" title="0">{
                        if health.Details != "" </span><span class="cov0" title="0">{
                                health.Details += "; "
                        }</span>
                        <span class="cov0" title="0">health.Details += "No data written yet"</span>
                }
        }

        // Check for index persist errors
        <span class="cov0" title="0">if stats.IndexPersistErrors &gt; 0 </span><span class="cov0" title="0">{
                health.Status = "degraded"
                if health.Details != "" </span><span class="cov0" title="0">{
                        health.Details += "; "
                }</span>
                <span class="cov0" title="0">health.Details += fmt.Sprintf("Index persist errors: %d", stats.IndexPersistErrors)</span>
        }

        // Simple ping test - try to access a shard
        <span class="cov0" title="0">if shardCount &gt; 0 </span><span class="cov0" title="0">{
                // Try to get any shard to verify basic functionality
                c.mu.RLock()
                for _, shard := range c.shards </span><span class="cov0" title="0">{
                        // Just accessing the shard and checking if we can read its state
                        if shard.state == nil </span><span class="cov0" title="0">{
                                health.ReadsOK = false
                                health.Status = "unhealthy"
                                health.Healthy = false
                                if health.Details != "" </span><span class="cov0" title="0">{
                                        health.Details += "; "
                                }</span>
                                <span class="cov0" title="0">health.Details += "Shard state unavailable"</span>
                        }
                        <span class="cov0" title="0">break</span> // Just check one shard
                }
                <span class="cov0" title="0">c.mu.RUnlock()</span>
        }

        // Set final health status
        <span class="cov0" title="0">if health.Status == "unhealthy" </span><span class="cov0" title="0">{
                health.Healthy = false
        }</span>

        <span class="cov0" title="0">return health</span>
}

// initCometState initializes the unified state for metrics and coordination
// In multi-process mode, it's memory-mapped to a file for sharing between processes
// In single-process mode, it's allocated in regular memory
func (s *Shard) initCometState(multiProcessMode bool) error <span class="cov8" title="1">{
        if multiProcessMode </span><span class="cov0" title="0">{
                return s.initCometStateMmap()
        }</span> else<span class="cov8" title="1"> {
                return s.initCometStateMemory()
        }</span>
}

// initCometStateMemory initializes unified state in regular memory (single-process mode)
func (s *Shard) initCometStateMemory() error <span class="cov8" title="1">{
        s.state = &amp;CometState{}

        // Initialize version
        atomic.StoreUint64(&amp;s.state.Version, CometStateVersion1)

        // Initialize with -1 to indicate "not yet set" for LastEntryNumber
        atomic.StoreInt64(&amp;s.state.LastEntryNumber, -1)

        return nil
}</span>

// initCometStateMmap initializes unified state via memory mapping (multi-process mode)
func (s *Shard) initCometStateMmap() error <span class="cov0" title="0">{
        // Create or open the unified state file
        file, err := os.OpenFile(s.statePath, os.O_CREATE|os.O_RDWR, 0644)
        if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to open unified state file: %w", err)
        }</span>
        <span class="cov0" title="0">defer file.Close()

        // Ensure file is the correct size
        stat, err := file.Stat()
        if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to stat unified state file: %w", err)
        }</span>

        <span class="cov0" title="0">if stat.Size() == 0 </span><span class="cov0" title="0">{
                // New file - initialize with zeros and set version
                if err := file.Truncate(CometStateSize); err != nil </span><span class="cov0" title="0">{
                        return fmt.Errorf("failed to set unified state file size: %w", err)
                }</span>
        } else<span class="cov0" title="0"> if stat.Size() != CometStateSize </span><span class="cov0" title="0">{
                return fmt.Errorf("unified state file has wrong size: got %d, expected %d", stat.Size(), CometStateSize)
        }</span>

        // Memory map the file
        <span class="cov0" title="0">data, err := syscall.Mmap(int(file.Fd()), 0, CometStateSize,
                syscall.PROT_READ|syscall.PROT_WRITE, syscall.MAP_SHARED)
        if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to mmap unified state file: %w", err)
        }</span>

        <span class="cov0" title="0">s.stateData = data
        s.state = (*CometState)(unsafe.Pointer(&amp;data[0]))

        // Initialize version if this is a new file
        if stat.Size() == 0 </span><span class="cov0" title="0">{
                atomic.StoreUint64(&amp;s.state.Version, CometStateVersion1)
                // Initialize with -1 to indicate "not yet set" for LastEntryNumber
                atomic.StoreInt64(&amp;s.state.LastEntryNumber, -1)
        }</span> else<span class="cov0" title="0"> {
                // Validate existing state file
                if err := s.validateAndRecoverState(); err != nil </span><span class="cov0" title="0">{
                        return fmt.Errorf("state validation failed: %w", err)
                }</span>
        }

        <span class="cov0" title="0">return nil</span>
}
</pre>
		
		<pre class="file" id="file1" style="display: none">package comet

import (
        "context"
        "encoding/binary"
        "fmt"
        "maps"
        "path/filepath"
        "slices"
        "strings"
        "sync"
        "sync/atomic"
        "time"
)

// MessageID represents a structured message ID
// Fields ordered for optimal memory alignment: int64 first, then uint32
type MessageID struct {
        EntryNumber int64  `json:"entry_number"`
        ShardID     uint32 `json:"shard_id"`
}

// String returns the string representation of the ID (ShardID-EntryNumber format)
func (id MessageID) String() string <span class="cov0" title="0">{
        return fmt.Sprintf("%d-%d", id.ShardID, id.EntryNumber)
}</span>

// ParseMessageID parses a string ID back to MessageID
func ParseMessageID(str string) (MessageID, error) <span class="cov0" title="0">{
        var shardID uint32
        var entryNumber int64
        if _, err := fmt.Sscanf(str, "%d-%d", &amp;shardID, &amp;entryNumber); err != nil </span><span class="cov0" title="0">{
                return MessageID{}, fmt.Errorf("invalid message ID format: %w", err)
        }</span>
        <span class="cov0" title="0">return MessageID{EntryNumber: entryNumber, ShardID: shardID}, nil</span>
}

// StreamMessage represents a message read from a stream
type StreamMessage struct {
        Stream string    // Stream name/identifier
        ID     MessageID // Unique message ID
        Data   []byte    // Raw message data
}

// Consumer reads from comet stream shards
// Fields ordered for optimal memory alignment
type Consumer struct {
        // Pointers first (8 bytes on 64-bit)
        client *Client

        // Composite types
        readers sync.Map // Cached readers per shard (optimized for read-heavy workload)

        // Strings last
        group string
}

// ConsumerOptions configures a consumer
type ConsumerOptions struct {
        Group string
}

// ProcessFunc handles a batch of messages, returning error to trigger retry
type ProcessFunc func(messages []StreamMessage) error

// ProcessOption configures the Process method
type ProcessOption func(*processConfig)

// processConfig holds internal configuration built from options
type processConfig struct {
        // Core processing
        handler ProcessFunc
        autoAck bool

        // Callbacks
        onError func(err error, retryCount int)
        onBatch func(size int, duration time.Duration)

        // Behavior
        batchSize    int
        maxRetries   int
        pollInterval time.Duration
        retryDelay   time.Duration

        // Sharding
        stream        string
        shards        []uint32
        consumerID    int
        consumerCount int
}

// WithStream specifies a stream pattern for shard discovery
func WithStream(pattern string) ProcessOption <span class="cov0" title="0">{
        return func(cfg *processConfig) </span><span class="cov0" title="0">{
                cfg.stream = pattern
        }</span>
}

// WithShards specifies explicit shards to process
func WithShards(shards ...uint32) ProcessOption <span class="cov0" title="0">{
        return func(cfg *processConfig) </span><span class="cov0" title="0">{
                cfg.shards = shards
        }</span>
}

// WithBatchSize sets the number of messages to read at once
func WithBatchSize(size int) ProcessOption <span class="cov0" title="0">{
        return func(cfg *processConfig) </span><span class="cov0" title="0">{
                cfg.batchSize = size
        }</span>
}

// WithMaxRetries sets the number of retry attempts for failed batches
func WithMaxRetries(retries int) ProcessOption <span class="cov0" title="0">{
        return func(cfg *processConfig) </span><span class="cov0" title="0">{
                cfg.maxRetries = retries
        }</span>
}

// WithPollInterval sets how long to wait when no messages are available
func WithPollInterval(interval time.Duration) ProcessOption <span class="cov0" title="0">{
        return func(cfg *processConfig) </span><span class="cov0" title="0">{
                cfg.pollInterval = interval
        }</span>
}

// WithRetryDelay sets the base delay between retries
func WithRetryDelay(delay time.Duration) ProcessOption <span class="cov0" title="0">{
        return func(cfg *processConfig) </span><span class="cov0" title="0">{
                cfg.retryDelay = delay
        }</span>
}

// WithAutoAck controls automatic acknowledgment (default: true)
func WithAutoAck(enabled bool) ProcessOption <span class="cov0" title="0">{
        return func(cfg *processConfig) </span><span class="cov0" title="0">{
                cfg.autoAck = enabled
        }</span>
}

// WithErrorHandler sets a callback for processing errors
func WithErrorHandler(handler func(err error, retryCount int)) ProcessOption <span class="cov0" title="0">{
        return func(cfg *processConfig) </span><span class="cov0" title="0">{
                cfg.onError = handler
        }</span>
}

// WithBatchCallback sets a callback after each batch completes
func WithBatchCallback(callback func(size int, duration time.Duration)) ProcessOption <span class="cov0" title="0">{
        return func(cfg *processConfig) </span><span class="cov0" title="0">{
                cfg.onBatch = callback
        }</span>
}

// WithConsumerAssignment configures distributed processing
func WithConsumerAssignment(id, total int) ProcessOption <span class="cov0" title="0">{
        return func(cfg *processConfig) </span><span class="cov0" title="0">{
                cfg.consumerID = id
                cfg.consumerCount = total
        }</span>
}

// buildProcessConfig applies options and defaults
func buildProcessConfig(handler ProcessFunc, opts []ProcessOption) *processConfig <span class="cov0" title="0">{
        cfg := &amp;processConfig{
                handler:       handler,
                autoAck:       true,
                batchSize:     100,
                maxRetries:    3,
                pollInterval:  100 * time.Millisecond,
                retryDelay:    time.Second,
                consumerCount: 1,
                consumerID:    0,
        }

        for _, opt := range opts </span><span class="cov0" title="0">{
                opt(cfg)
        }</span>

        // If no stream or shards specified, discover all shards
        <span class="cov0" title="0">if cfg.stream == "" &amp;&amp; len(cfg.shards) == 0 </span><span class="cov0" title="0">{
                cfg.stream = "*:*:*:*" // Match any stream pattern
        }</span>

        <span class="cov0" title="0">return cfg</span>
}

// NewConsumer creates a new consumer for comet streams
func NewConsumer(client *Client, opts ConsumerOptions) *Consumer <span class="cov0" title="0">{
        return &amp;Consumer{
                client: client,
                group:  opts.Group,
                // readers sync.Map is zero-initialized and ready to use
        }
}</span>

// Close closes the consumer and releases all resources
func (c *Consumer) Close() error <span class="cov0" title="0">{
        // Close all cached readers
        c.readers.Range(func(key, value any) bool </span><span class="cov0" title="0">{
                if reader, ok := value.(*Reader); ok </span><span class="cov0" title="0">{
                        reader.Close()
                        // Decrement active readers count
                        shardID := key.(uint32)
                        if shard, err := c.client.getOrCreateShard(shardID); err == nil &amp;&amp; shard.state != nil </span><span class="cov0" title="0">{
                                atomic.AddUint64(&amp;shard.state.ActiveReaders, ^uint64(0)) // Decrement by 1
                        }</span>
                }
                <span class="cov0" title="0">return true</span>
        })
        <span class="cov0" title="0">return nil</span>
}

// Read reads up to count entries from the specified shards
func (c *Consumer) Read(ctx context.Context, shards []uint32, count int) ([]StreamMessage, error) <span class="cov0" title="0">{
        var messages []StreamMessage
        remaining := count

        for _, shardID := range shards </span><span class="cov0" title="0">{
                if remaining &lt;= 0 </span><span class="cov0" title="0">{
                        break</span>
                }

                <span class="cov0" title="0">shard, err := c.client.getOrCreateShard(shardID)
                if err != nil </span><span class="cov0" title="0">{
                        return nil, fmt.Errorf("failed to get shard %d: %w", shardID, err)
                }</span>

                <span class="cov0" title="0">shardMessages, err := c.readFromShard(ctx, shard, remaining)
                if err != nil </span><span class="cov0" title="0">{
                        return nil, fmt.Errorf("failed to read from shard %d: %w", shardID, err)
                }</span>

                <span class="cov0" title="0">messages = append(messages, shardMessages...)
                remaining -= len(shardMessages)</span>
        }

        <span class="cov0" title="0">return messages, nil</span>
}

// Process continuously reads and processes messages from shards.
// The simplest usage processes all discoverable shards:
//
//        err := consumer.Process(ctx, handleMessages)
//
// With options:
//
//        err := consumer.Process(ctx, handleMessages,
//            comet.WithStream("events:v1:shard:*"),
//            comet.WithBatchSize(1000),
//            comet.WithErrorHandler(logError),
//        )
//
// For distributed processing:
//
//        err := consumer.Process(ctx, handleMessages,
//            comet.WithStream("events:v1:shard:*"),
//            comet.WithConsumerAssignment(workerID, totalWorkers),
//        )
func (c *Consumer) Process(ctx context.Context, handler ProcessFunc, opts ...ProcessOption) error <span class="cov0" title="0">{
        // Build config from options
        cfg := buildProcessConfig(handler, opts)

        // Validate required fields
        if cfg.handler == nil </span><span class="cov0" title="0">{
                return fmt.Errorf("handler is required")
        }</span>

        // Determine shards to process
        <span class="cov0" title="0">var shards []uint32
        if len(cfg.shards) &gt; 0 </span><span class="cov0" title="0">{
                shards = cfg.shards
        }</span> else<span class="cov0" title="0"> if cfg.stream != "" </span><span class="cov0" title="0">{
                // Auto-discover shards from stream pattern
                discoveredShards, err := c.discoverShards(cfg.stream, cfg.consumerID, cfg.consumerCount)
                if err != nil </span><span class="cov0" title="0">{
                        return fmt.Errorf("failed to discover shards: %w", err)
                }</span>
                <span class="cov0" title="0">shards = discoveredShards</span>
        } else<span class="cov0" title="0"> {
                return fmt.Errorf("either Shards or Stream must be specified")
        }</span>

        // Apply defaults (already set in buildProcessConfig)
        <span class="cov0" title="0">autoAck := cfg.autoAck

        // Main processing loop
        for </span><span class="cov0" title="0">{
                select </span>{
                case &lt;-ctx.Done():<span class="cov0" title="0">
                        return ctx.Err()</span>
                default:<span class="cov0" title="0"></span>
                }

                <span class="cov0" title="0">start := time.Now()
                messages, err := c.Read(ctx, shards, cfg.batchSize)
                if err != nil </span><span class="cov0" title="0">{
                        if cfg.onError != nil </span><span class="cov0" title="0">{
                                cfg.onError(err, 0)
                        }</span>
                        // For read errors, sleep and retry
                        <span class="cov0" title="0">select </span>{
                        case &lt;-ctx.Done():<span class="cov0" title="0">
                                return ctx.Err()</span>
                        case &lt;-time.After(cfg.pollInterval):<span class="cov0" title="0">
                                continue</span>
                        }
                }

                <span class="cov0" title="0">if len(messages) == 0 </span><span class="cov0" title="0">{
                        // No messages, wait before polling again
                        select </span>{
                        case &lt;-ctx.Done():<span class="cov0" title="0">
                                return ctx.Err()</span>
                        case &lt;-time.After(cfg.pollInterval):<span class="cov0" title="0">
                                continue</span>
                        }
                }

                // Process batch with retries
                <span class="cov0" title="0">var processErr error
                for retry := 0; retry &lt;= cfg.maxRetries; retry++ </span><span class="cov0" title="0">{
                        processErr = cfg.handler(messages)
                        if processErr == nil </span><span class="cov0" title="0">{
                                break</span>
                        }

                        <span class="cov0" title="0">if cfg.onError != nil </span><span class="cov0" title="0">{
                                cfg.onError(processErr, retry)
                        }</span>

                        <span class="cov0" title="0">if retry &lt; cfg.maxRetries </span><span class="cov0" title="0">{
                                select </span>{
                                case &lt;-ctx.Done():<span class="cov0" title="0">
                                        return ctx.Err()</span>
                                case &lt;-time.After(cfg.retryDelay * time.Duration(retry+1)):<span class="cov0" title="0"></span>
                                        // Exponential backoff
                                }
                        }
                }

                // Auto-ack if enabled and processing succeeded
                <span class="cov0" title="0">if autoAck &amp;&amp; processErr == nil </span><span class="cov0" title="0">{
                        for _, msg := range messages </span><span class="cov0" title="0">{
                                if err := c.Ack(ctx, msg.ID); err != nil &amp;&amp; cfg.onError != nil </span><span class="cov0" title="0">{
                                        cfg.onError(fmt.Errorf("ack failed for message %s: %w", msg.ID, err), 0)
                                }</span>
                        }
                }

                // Call batch callback if provided
                <span class="cov0" title="0">if cfg.onBatch != nil </span><span class="cov0" title="0">{
                        cfg.onBatch(len(messages), time.Since(start))
                }</span>
        }
}

// getOrCreateReader gets or creates a reader for a shard
func (c *Consumer) getOrCreateReader(shard *Shard) (*Reader, error) <span class="cov0" title="0">{
        // Fast path: check if reader exists using sync.Map
        if value, ok := c.readers.Load(shard.shardID); ok </span><span class="cov0" title="0">{
                reader := value.(*Reader)

                // Check if reader is still valid by comparing file lists
                shard.mu.RLock()
                currentFiles := shard.index.Files
                shard.mu.RUnlock()

                reader.mu.RLock()
                readerFiles := reader.files
                reader.mu.RUnlock()

                // Reader is valid only if it has the exact same files as the shard
                // This handles both additions (rotation) and deletions (retention)
                if len(currentFiles) == len(readerFiles) </span><span class="cov0" title="0">{
                        // Check if all files match
                        filesMatch := true
                        for i := 0; i &lt; len(currentFiles) &amp;&amp; filesMatch; i++ </span><span class="cov0" title="0">{
                                if currentFiles[i].Path != readerFiles[i].Path </span><span class="cov0" title="0">{
                                        filesMatch = false
                                }</span>
                        }
                        <span class="cov0" title="0">if filesMatch </span><span class="cov0" title="0">{
                                // Track reader cache hit
                                if shard.state != nil </span><span class="cov0" title="0">{
                                        atomic.AddUint64(&amp;shard.state.ReaderCacheHits, 1)
                                }</span>
                                <span class="cov0" title="0">return reader, nil</span>
                        }
                }

                // Reader is stale, need to recreate
                // Delete the old one so only one goroutine recreates
                <span class="cov0" title="0">c.readers.Delete(shard.shardID)
                reader.Close()
                // Decrement active readers count
                if shard.state != nil </span><span class="cov0" title="0">{
                        atomic.AddUint64(&amp;shard.state.ActiveReaders, ^uint64(0)) // Decrement by 1
                }</span>
        }

        // Create new reader with a snapshot of the current index
        <span class="cov0" title="0">shard.mu.RLock()
        // Make a copy of the index to avoid race conditions
        indexCopy := &amp;ShardIndex{
                Files:              make([]FileInfo, len(shard.index.Files)),
                CurrentFile:        shard.index.CurrentFile,
                CurrentWriteOffset: shard.index.CurrentWriteOffset,
                CurrentEntryNumber: shard.index.CurrentEntryNumber,
                ConsumerOffsets:    make(map[string]int64),
                BinaryIndex: BinarySearchableIndex{
                        IndexInterval: shard.index.BinaryIndex.IndexInterval,
                        MaxNodes:      shard.index.BinaryIndex.MaxNodes,
                        Nodes:         make([]EntryIndexNode, len(shard.index.BinaryIndex.Nodes)),
                },
        }
        copy(indexCopy.Files, shard.index.Files)
        copy(indexCopy.BinaryIndex.Nodes, shard.index.BinaryIndex.Nodes)
        maps.Copy(indexCopy.ConsumerOffsets, shard.index.ConsumerOffsets)
        shard.mu.RUnlock()

        newReader, err := NewReader(shard.shardID, indexCopy)
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>

        // Use LoadOrStore to handle race where multiple goroutines try to create
        <span class="cov0" title="0">if actual, loaded := c.readers.LoadOrStore(shard.shardID, newReader); loaded </span><span class="cov0" title="0">{
                // Another goroutine created a reader, close ours and use theirs
                newReader.Close()
                return actual.(*Reader), nil
        }</span>

        // Track new reader creation
        <span class="cov0" title="0">if shard.state != nil </span><span class="cov0" title="0">{
                atomic.AddUint64(&amp;shard.state.TotalReaders, 1)
                atomic.AddUint64(&amp;shard.state.ActiveReaders, 1)
        }</span>

        <span class="cov0" title="0">return newReader, nil</span>
}

// findEntryPosition finds the position of an entry using binary searchable index (O(log n))
func (c *Consumer) findEntryPosition(shard *Shard, entryNum int64) (EntryPosition, error) <span class="cov0" title="0">{
        // Check if the requested entry exists
        if entryNum &gt;= shard.index.CurrentEntryNumber </span><span class="cov0" title="0">{
                return EntryPosition{}, fmt.Errorf("entry %d does not exist (current max: %d)", entryNum, shard.index.CurrentEntryNumber-1)
        }</span>

        // Use the binary searchable index for O(log n) lookup
        <span class="cov0" title="0">if len(shard.index.BinaryIndex.Nodes) &gt; 0 </span><span class="cov0" title="0">{
                // Get the best starting position for scanning
                if startPos, startEntry, found := shard.index.BinaryIndex.GetScanStartPosition(entryNum); found </span><span class="cov0" title="0">{
                        // Safety check: if the position is invalid, fall back to linear scan
                        if startPos.FileIndex &lt; 0 || startPos.FileIndex &gt;= len(shard.index.Files) </span>{<span class="cov0" title="0">
                                // Binary index has stale data, fall back to scanning from beginning
                        }</span> else<span class="cov0" title="0"> {
                                if startEntry == entryNum </span><span class="cov0" title="0">{
                                        return startPos, nil
                                }</span>
                                // Scan forward from the closest indexed entry
                                <span class="cov0" title="0">return c.scanForwardToEntry(shard, startPos, startEntry, entryNum)</span>
                        }
                }
        }

        // If no index nodes exist, start from the beginning
        <span class="cov0" title="0">if len(shard.index.Files) == 0 </span><span class="cov0" title="0">{
                return EntryPosition{}, fmt.Errorf("no files in shard")
        }</span>

        // Start from entry 0 in the first file
        <span class="cov0" title="0">startPos := EntryPosition{
                FileIndex:  0,
                ByteOffset: 0,
        }

        // In multi-process mode, entries might not start from 0
        // Calculate the first entry number based on total entries written
        totalEntries := int64(0)
        for _, f := range shard.index.Files </span><span class="cov0" title="0">{
                totalEntries += f.Entries
        }</span>
        <span class="cov0" title="0">firstEntryNum := shard.index.CurrentEntryNumber - totalEntries

        // If looking for an entry before the first one, it doesn't exist
        if entryNum &lt; firstEntryNum </span><span class="cov0" title="0">{
                return EntryPosition{}, fmt.Errorf("entry %d does not exist (first entry is %d)", entryNum, firstEntryNum)
        }</span>

        // If looking for the first entry, return the start position
        <span class="cov0" title="0">if entryNum == firstEntryNum </span><span class="cov0" title="0">{
                return startPos, nil
        }</span>

        // Otherwise scan forward from the first entry
        <span class="cov0" title="0">return c.scanForwardToEntry(shard, startPos, firstEntryNum, entryNum)</span>
}

// scanForwardToEntry scans forward from a known position to find the target entry
func (c *Consumer) scanForwardToEntry(shard *Shard, startPos EntryPosition, startEntry, targetEntry int64) (EntryPosition, error) <span class="cov0" title="0">{
        if startEntry &gt;= targetEntry </span><span class="cov0" title="0">{
                return startPos, nil // No scanning needed
        }</span>

        // Get reader for this shard
        <span class="cov0" title="0">reader, err := c.getOrCreateReader(shard)
        if err != nil </span><span class="cov0" title="0">{
                return EntryPosition{}, fmt.Errorf("failed to get reader: %w", err)
        }</span>

        // Hold reader lock during the entire scan to prevent races with remapFile
        <span class="cov0" title="0">reader.mu.RLock()
        defer reader.mu.RUnlock()

        // First, find which file contains the target entry
        targetFileIndex := -1
        for i := startPos.FileIndex; i &lt; len(shard.index.Files); i++ </span><span class="cov0" title="0">{
                fileInfo := shard.index.Files[i]
                if fileInfo.StartEntry &lt;= targetEntry &amp;&amp; targetEntry &lt; fileInfo.StartEntry+fileInfo.Entries </span><span class="cov0" title="0">{
                        targetFileIndex = i
                        break</span>
                }
        }

        <span class="cov0" title="0">if targetFileIndex == -1 </span><span class="cov0" title="0">{
                return EntryPosition{}, fmt.Errorf("entry %d not found in any file", targetEntry)
        }</span>

        // Now scan within the target file to find the exact position
        <span class="cov0" title="0">file := reader.files[targetFileIndex]
        fileData := file.data.Load()
        if len(fileData) == 0 </span><span class="cov0" title="0">{
                return EntryPosition{}, fmt.Errorf("file %d is empty but should contain entry %d", targetFileIndex, targetEntry)
        }</span>

        <span class="cov0" title="0">fileInfo := shard.index.Files[targetFileIndex]
        currentEntry := fileInfo.StartEntry
        currentOffset := int64(0)

        // Scan through the file entry by entry
        for currentEntry &lt;= targetEntry </span><span class="cov0" title="0">{
                // Check if we've found the target
                if currentEntry == targetEntry </span><span class="cov0" title="0">{
                        return EntryPosition{
                                FileIndex:  targetFileIndex,
                                ByteOffset: currentOffset,
                        }, nil
                }</span>

                // Check if we have enough data for the header
                <span class="cov0" title="0">if currentOffset+12 &gt; int64(len(fileData)) </span><span class="cov0" title="0">{
                        return EntryPosition{}, fmt.Errorf("entry %d header extends beyond file", currentEntry)
                }</span>

                // Read entry header
                <span class="cov0" title="0">header := fileData[currentOffset : currentOffset+12]
                length := binary.LittleEndian.Uint32(header[0:4])

                // Validate entry length
                if length &gt; 100*1024*1024 </span><span class="cov0" title="0">{ // 100MB max
                        return EntryPosition{}, fmt.Errorf("entry %d has invalid length %d", currentEntry, length)
                }</span>

                // Check if the full entry fits
                <span class="cov0" title="0">entryEnd := currentOffset + 12 + int64(length)
                if entryEnd &gt; int64(len(fileData)) </span><span class="cov0" title="0">{
                        return EntryPosition{}, fmt.Errorf("entry %d extends beyond file", currentEntry)
                }</span>

                // Move to next entry
                <span class="cov0" title="0">currentEntry++
                currentOffset = entryEnd</span>
        }

        <span class="cov0" title="0">return EntryPosition{}, fmt.Errorf("entry %d not found in file %d", targetEntry, targetFileIndex)</span>
}

// readFromShard reads entries from a single shard using entry-based positioning
func (c *Consumer) readFromShard(ctx context.Context, shard *Shard, maxCount int) ([]StreamMessage, error) <span class="cov0" title="0">{
        // Lock-free reader tracking
        atomic.AddInt64(&amp;shard.readerCount, 1)
        defer atomic.AddInt64(&amp;shard.readerCount, -1)

        // Check unified state for instant change detection
        if shard.state != nil </span><span class="cov0" title="0">{
                currentTimestamp := shard.state.GetLastIndexUpdate()
                if currentTimestamp != shard.lastMmapCheck </span><span class="cov0" title="0">{
                        // Index changed - reload it under write lock
                        shard.mu.Lock()
                        // Double-check after acquiring lock
                        if currentTimestamp != shard.lastMmapCheck </span><span class="cov0" title="0">{
                                if err := shard.loadIndexWithRecovery(); err != nil </span><span class="cov0" title="0">{
                                        shard.mu.Unlock()
                                        return nil, fmt.Errorf("failed to reload index after detecting mmap change: %w", err)
                                }</span>
                                <span class="cov0" title="0">shard.lastMmapCheck = currentTimestamp

                                // In multi-process mode, check if we need to rebuild index from files
                                // We can tell we're in multi-process mode if mmapState exists
                                if shard.state != nil </span><span class="cov0" title="0">{
                                        shardDir := filepath.Join(c.client.dataDir, fmt.Sprintf("shard-%04d", shard.shardID))
                                        shard.lazyRebuildIndexIfNeeded(c.client.config, shardDir)
                                }</span>

                                // Also invalidate any cached readers since the index changed
                                <span class="cov0" title="0">c.readers.Range(func(key, value any) bool </span><span class="cov0" title="0">{
                                        if key.(uint32) == shard.shardID </span><span class="cov0" title="0">{
                                                if reader, ok := value.(*Reader); ok </span><span class="cov0" title="0">{
                                                        reader.Close()
                                                        // Decrement active readers count
                                                        if shard.state != nil </span><span class="cov0" title="0">{
                                                                atomic.AddUint64(&amp;shard.state.ActiveReaders, ^uint64(0)) // Decrement by 1
                                                        }</span>
                                                }
                                                <span class="cov0" title="0">c.readers.Delete(key)
                                                return false</span> // Stop after finding this shard's reader
                                        }
                                        <span class="cov0" title="0">return true</span>
                                })
                        }
                        <span class="cov0" title="0">shard.mu.Unlock()</span>
                }
        }

        <span class="cov0" title="0">shard.mu.RLock()
        // Get consumer entry offset (not byte offset!)
        startEntryNum, exists := shard.index.ConsumerOffsets[c.group]
        if !exists </span><span class="cov0" title="0">{
                startEntryNum = 0
        }</span>

        // After retention, the requested start entry might no longer exist
        // Adjust to the earliest available entry if needed
        <span class="cov0" title="0">if len(shard.index.Files) &gt; 0 </span><span class="cov0" title="0">{
                earliestEntry := shard.index.Files[0].StartEntry
                if startEntryNum &lt; earliestEntry </span><span class="cov0" title="0">{
                        startEntryNum = earliestEntry
                }</span>
        }

        <span class="cov0" title="0">endEntryNum := shard.index.CurrentEntryNumber
        fileCount := len(shard.index.Files)
        shard.mu.RUnlock()

        // In multi-process mode, check if index might be stale by comparing with state
        if shard.state != nil &amp;&amp; shard.mmapWriter != nil &amp;&amp; shard.mmapWriter.state != nil </span><span class="cov0" title="0">{
                totalWrites := shard.mmapWriter.state.GetTotalWrites()
                if totalWrites &gt; uint64(endEntryNum) </span><span class="cov0" title="0">{
                        // Need write lock for rebuild
                        shard.mu.Lock()
                        shardDir := filepath.Join(c.client.dataDir, fmt.Sprintf("shard-%04d", shard.shardID))
                        // Force a full rebuild by manually updating the rebuild trigger
                        // Temporarily modify the file end offset to trigger a rebuild
                        oldEndOffset := int64(0)
                        if len(shard.index.Files) &gt; 0 </span><span class="cov0" title="0">{
                                oldEndOffset = shard.index.Files[len(shard.index.Files)-1].EndOffset
                                // Set end offset to 0 to force rebuild detection
                                shard.index.Files[len(shard.index.Files)-1].EndOffset = 0
                        }</span>

                        <span class="cov0" title="0">shard.lazyRebuildIndexIfNeeded(c.client.config, shardDir)

                        // Restore if rebuild didn't happen (shouldn't occur)
                        if len(shard.index.Files) &gt; 0 &amp;&amp; shard.index.Files[len(shard.index.Files)-1].EndOffset == 0 </span><span class="cov0" title="0">{
                                shard.index.Files[len(shard.index.Files)-1].EndOffset = oldEndOffset
                        }</span>
                        // Reload the updated values
                        <span class="cov0" title="0">startEntryNum, exists = shard.index.ConsumerOffsets[c.group]
                        if !exists </span><span class="cov0" title="0">{
                                startEntryNum = 0
                        }</span>

                        // After retention, adjust to earliest available entry if needed
                        <span class="cov0" title="0">if len(shard.index.Files) &gt; 0 </span><span class="cov0" title="0">{
                                earliestEntry := shard.index.Files[0].StartEntry
                                if startEntryNum &lt; earliestEntry </span><span class="cov0" title="0">{
                                        startEntryNum = earliestEntry
                                }</span>
                        }

                        <span class="cov0" title="0">endEntryNum = shard.index.CurrentEntryNumber
                        fileCount = len(shard.index.Files)
                        shard.mu.Unlock()</span>
                }
        }

        <span class="cov0" title="0">if startEntryNum &gt;= endEntryNum </span><span class="cov0" title="0">{
                return nil, nil // No new data
        }</span>

        // Safety check: if we have entries but no files, something is wrong
        <span class="cov0" title="0">if endEntryNum &gt; 0 &amp;&amp; fileCount == 0 </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("shard %d has %d entries but no data files", shard.shardID, endEntryNum)
        }</span>

        // Preallocate slice capacity based on available entries and maxCount
        <span class="cov0" title="0">availableEntries := endEntryNum - startEntryNum
        expectedCount := availableEntries
        if maxCount &gt; 0 &amp;&amp; expectedCount &gt; int64(maxCount) </span><span class="cov0" title="0">{
                expectedCount = int64(maxCount)
        }</span>
        <span class="cov0" title="0">messages := make([]StreamMessage, 0, expectedCount)

        // Read entries by entry number, looking up positions from index
        for entryNum := startEntryNum; entryNum &lt; endEntryNum &amp;&amp; len(messages) &lt; maxCount; entryNum++ </span><span class="cov0" title="0">{
                // Check context
                if ctx.Err() != nil </span><span class="cov0" title="0">{
                        return messages, ctx.Err()
                }</span>

                // Find where this entry is stored using interval-based boundaries
                <span class="cov0" title="0">shard.mu.RLock()
                position, err := c.findEntryPosition(shard, entryNum)
                shard.mu.RUnlock()

                if err != nil </span><span class="cov0" title="0">{
                        // In multi-process mode, if index-based lookup fails, try direct file scanning
                        // This handles the case where the index is incomplete but the data exists in files
                        if shard.state != nil &amp;&amp; !strings.Contains(err.Error(), "no files in shard") </span><span class="cov0" title="0">{
                                position, err = c.scanDataFilesForEntry(shard, entryNum)
                        }</span>

                        <span class="cov0" title="0">if err != nil </span><span class="cov0" title="0">{
                                // In multi-process mode, some entries might not exist due to gaps in the sequence
                                // Skip these entries instead of failing the entire read
                                if strings.Contains(err.Error(), "does not exist") </span><span class="cov0" title="0">{
                                        continue</span>
                                }
                                <span class="cov0" title="0">return nil, fmt.Errorf("failed to find entry %d position in shard %d: %w", entryNum, shard.shardID, err)</span>
                        }
                }

                // Safety check for invalid position
                <span class="cov0" title="0">if position.FileIndex &lt; 0 </span><span class="cov0" title="0">{
                        return nil, fmt.Errorf("invalid position for entry %d in shard %d: FileIndex=%d, ByteOffset=%d", entryNum, shard.shardID, position.FileIndex, position.ByteOffset)
                }</span>

                // Get reader for this shard
                <span class="cov0" title="0">reader, err := c.getOrCreateReader(shard)
                if err != nil </span><span class="cov0" title="0">{
                        return nil, fmt.Errorf("failed to get reader for shard %d: %w", shard.shardID, err)
                }</span>

                // Read the specific entry
                <span class="cov0" title="0">data, err := reader.ReadEntryAtPosition(position)
                if err != nil </span><span class="cov0" title="0">{
                        // If we get a "file not memory mapped" error, it might mean the file is empty
                        // or hasn't been flushed yet. Try to force a flush and retry once.
                        if strings.Contains(err.Error(), "file not memory mapped") </span><span class="cov0" title="0">{
                                // Force persist the index to ensure file metadata is up to date
                                shard.persistIndex()
                                // Retry the read
                                data, err = reader.ReadEntryAtPosition(position)
                        }</span>
                        <span class="cov0" title="0">if err != nil </span><span class="cov0" title="0">{
                                // Track read error in CometState
                                if shard.state != nil </span><span class="cov0" title="0">{
                                        atomic.AddUint64(&amp;shard.state.ReadErrors, 1)
                                }</span>
                                <span class="cov0" title="0">return nil, fmt.Errorf("failed to read entry %d from shard %d: %w", entryNum, shard.shardID, err)</span>
                        }
                }

                <span class="cov0" title="0">message := StreamMessage{
                        Stream: fmt.Sprintf("shard:%04d", shard.shardID),
                        ID:     MessageID{EntryNumber: entryNum, ShardID: shard.shardID},
                        Data:   data,
                }

                messages = append(messages, message)</span>
        }

        // Track read metrics in CometState
        <span class="cov0" title="0">if shard.state != nil &amp;&amp; len(messages) &gt; 0 </span><span class="cov0" title="0">{
                atomic.AddUint64(&amp;shard.state.TotalEntriesRead, uint64(len(messages)))
        }</span>

        <span class="cov0" title="0">return messages, nil</span>
}

// scanDataFilesForEntry directly scans data files to find an entry when index is incomplete
func (c *Consumer) scanDataFilesForEntry(shard *Shard, targetEntry int64) (EntryPosition, error) <span class="cov0" title="0">{
        // This is the fallback when index-based lookup fails in multi-process mode
        // We scan the actual append-only data files directly

        shard.mu.RLock()
        files := make([]FileInfo, len(shard.index.Files))
        copy(files, shard.index.Files)
        shard.mu.RUnlock()

        if len(files) == 0 </span><span class="cov0" title="0">{
                return EntryPosition{}, fmt.Errorf("no files to scan")
        }</span>

        // Get reader for file access
        <span class="cov0" title="0">reader, err := c.getOrCreateReader(shard)
        if err != nil </span><span class="cov0" title="0">{
                return EntryPosition{}, fmt.Errorf("failed to get reader: %w", err)
        }</span>

        <span class="cov0" title="0">reader.mu.RLock()
        defer reader.mu.RUnlock()

        currentEntry := int64(0)

        // Scan each file sequentially
        for fileIdx := range files </span><span class="cov0" title="0">{
                if fileIdx &gt;= len(reader.files) </span><span class="cov0" title="0">{
                        break</span> // Reader doesn't have this file yet
                }

                <span class="cov0" title="0">file := reader.files[fileIdx]
                fileData := file.data.Load()
                if fileData == nil </span><span class="cov0" title="0">{
                        continue</span>
                }

                <span class="cov0" title="0">offset := int64(0)
                fileSize := int64(len(fileData))

                // Scan entries in this file
                for offset &lt; fileSize </span><span class="cov0" title="0">{
                        // Check if we have enough data for header
                        if offset+12 &gt; fileSize </span><span class="cov0" title="0">{
                                break</span>
                        }

                        // Read entry header
                        <span class="cov0" title="0">header := fileData[offset : offset+12]
                        length := binary.LittleEndian.Uint32(header[0:4])
                        timestamp := binary.LittleEndian.Uint64(header[4:12])

                        // Validate entry
                        if length == 0 || length &gt; 100*1024*1024 || timestamp == 0 </span><span class="cov0" title="0">{
                                // Invalid entry - try to find next valid entry
                                offset += 4
                                continue</span>
                        }

                        // Check if full entry is available
                        <span class="cov0" title="0">entryEnd := offset + 12 + int64(length)
                        if entryEnd &gt; fileSize </span><span class="cov0" title="0">{
                                break</span> // Entry extends beyond file
                        }

                        // Found a valid entry - check if it's the target
                        <span class="cov0" title="0">if currentEntry == targetEntry </span><span class="cov0" title="0">{
                                return EntryPosition{
                                        FileIndex:  fileIdx,
                                        ByteOffset: offset,
                                }, nil
                        }</span>

                        // Move to next entry
                        <span class="cov0" title="0">currentEntry++
                        offset = entryEnd</span>
                }
        }

        <span class="cov0" title="0">return EntryPosition{}, fmt.Errorf("entry %d not found in data files (scanned %d entries)", targetEntry, currentEntry)</span>
}

// Ack acknowledges one or more processed messages and updates consumer offset
func (c *Consumer) Ack(ctx context.Context, messageIDs ...MessageID) error <span class="cov0" title="0">{
        if len(messageIDs) == 0 </span><span class="cov0" title="0">{
                return nil
        }</span>

        // Single message fast path
        <span class="cov0" title="0">if len(messageIDs) == 1 </span><span class="cov0" title="0">{
                messageID := messageIDs[0]
                shard, err := c.client.getOrCreateShard(messageID.ShardID)
                if err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>

                <span class="cov0" title="0">shard.mu.Lock()
                // Check if this is a new consumer group
                _, groupExists := shard.index.ConsumerOffsets[c.group]
                if !groupExists &amp;&amp; shard.state != nil </span><span class="cov0" title="0">{
                        atomic.AddUint64(&amp;shard.state.ConsumerGroups, 1)
                }</span>
                // Update consumer offset to the next entry number
                // This ensures we won't re-read this entry
                <span class="cov0" title="0">nextEntry := messageID.EntryNumber + 1
                shard.index.ConsumerOffsets[c.group] = nextEntry
                // Mark that we need a checkpoint
                shard.writesSinceCheckpoint++
                // Track acked entries
                if shard.state != nil </span><span class="cov0" title="0">{
                        atomic.AddUint64(&amp;shard.state.AckedEntries, 1)
                }</span>
                <span class="cov0" title="0">shard.mu.Unlock()

                // Don't persist immediately - let periodic checkpoint handle it
                return nil</span>
        }

        // Multiple messages - use batch logic
        <span class="cov0" title="0">return c.ackBatch(messageIDs)</span>
}

// ackBatch is a helper for batch acknowledgments
func (c *Consumer) ackBatch(messageIDs []MessageID) error <span class="cov0" title="0">{
        // Group messages by shard
        shardGroups := make(map[uint32][]MessageID)
        for _, id := range messageIDs </span><span class="cov0" title="0">{
                shardGroups[id.ShardID] = append(shardGroups[id.ShardID], id)
        }</span>

        // Process each shard group
        <span class="cov0" title="0">for shardID, ids := range shardGroups </span><span class="cov0" title="0">{
                shard, err := c.client.getOrCreateShard(shardID)
                if err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>

                <span class="cov0" title="0">shard.mu.Lock()

                // Find the highest entry number in this shard's batch
                var maxEntry int64 = -1
                for _, id := range ids </span><span class="cov0" title="0">{
                        if id.EntryNumber &gt; maxEntry </span><span class="cov0" title="0">{
                                maxEntry = id.EntryNumber
                        }</span>
                }

                // Update to one past the highest ACK'd entry
                <span class="cov0" title="0">if maxEntry &gt;= 0 </span><span class="cov0" title="0">{
                        // Check if this is a new consumer group
                        _, groupExists := shard.index.ConsumerOffsets[c.group]
                        if !groupExists &amp;&amp; shard.state != nil </span><span class="cov0" title="0">{
                                atomic.AddUint64(&amp;shard.state.ConsumerGroups, 1)
                        }</span>
                        <span class="cov0" title="0">shard.index.ConsumerOffsets[c.group] = maxEntry + 1
                        // Mark that we need a checkpoint
                        shard.writesSinceCheckpoint++
                        // Track acked entries
                        if shard.state != nil </span><span class="cov0" title="0">{
                                atomic.AddUint64(&amp;shard.state.AckedEntries, uint64(len(ids)))
                        }</span>
                }

                <span class="cov0" title="0">shard.mu.Unlock()</span>
        }

        <span class="cov0" title="0">return nil</span>
}

// GetLag returns how many entries behind this consumer group is
func (c *Consumer) GetLag(ctx context.Context, shardID uint32) (int64, error) <span class="cov0" title="0">{
        shard, err := c.client.getOrCreateShard(shardID)
        if err != nil </span><span class="cov0" title="0">{
                return 0, err
        }</span>

        // Check unified state for instant change detection
        <span class="cov0" title="0">if shard.state != nil </span><span class="cov0" title="0">{
                currentTimestamp := shard.state.GetLastIndexUpdate()
                if currentTimestamp != shard.lastMmapCheck </span><span class="cov0" title="0">{
                        // Index changed - reload it under write lock
                        shard.mu.Lock()
                        // Double-check after acquiring lock
                        if currentTimestamp != shard.lastMmapCheck </span><span class="cov0" title="0">{
                                if err := shard.loadIndexWithRecovery(); err != nil </span><span class="cov0" title="0">{
                                        shard.mu.Unlock()
                                        return 0, fmt.Errorf("failed to reload index after detecting mmap change: %w", err)
                                }</span>
                                <span class="cov0" title="0">shard.lastMmapCheck = currentTimestamp</span>
                        }
                        <span class="cov0" title="0">shard.mu.Unlock()</span>
                }
        }

        <span class="cov0" title="0">shard.mu.RLock()
        defer shard.mu.RUnlock()

        consumerEntry, exists := shard.index.ConsumerOffsets[c.group]
        if !exists </span><span class="cov0" title="0">{
                consumerEntry = 0
        }</span>

        // Entry-based lag calculation
        <span class="cov0" title="0">lag := shard.index.CurrentEntryNumber - consumerEntry

        // Track max consumer lag
        if shard.state != nil &amp;&amp; lag &gt; 0 </span><span class="cov0" title="0">{
                // Update max lag if this is higher
                for </span><span class="cov0" title="0">{
                        currentMax := atomic.LoadUint64(&amp;shard.state.MaxConsumerLag)
                        if uint64(lag) &lt;= currentMax </span><span class="cov0" title="0">{
                                break</span>
                        }
                        <span class="cov0" title="0">if atomic.CompareAndSwapUint64(&amp;shard.state.MaxConsumerLag, currentMax, uint64(lag)) </span><span class="cov0" title="0">{
                                break</span>
                        }
                }
        }

        <span class="cov0" title="0">return lag, nil</span>
}

// ResetOffset sets the consumer offset to a specific entry number
func (c *Consumer) ResetOffset(ctx context.Context, shardID uint32, entryNumber int64) error <span class="cov0" title="0">{
        shard, err := c.client.getOrCreateShard(shardID)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov0" title="0">shard.mu.Lock()
        if entryNumber &lt; 0 </span><span class="cov0" title="0">{
                // Negative means from end
                entryNumber = shard.index.CurrentEntryNumber + entryNumber
                if entryNumber &lt; 0 </span><span class="cov0" title="0">{
                        entryNumber = 0
                }</span>
        }

        <span class="cov0" title="0">shard.index.ConsumerOffsets[c.group] = entryNumber
        // Mark that we need a checkpoint
        shard.writesSinceCheckpoint++
        shard.mu.Unlock()

        // Don't persist immediately - let periodic checkpoint handle it
        return nil</span>
}

// AckRange acknowledges all messages in a contiguous range for a shard
// This is more efficient than individual acks for bulk processing
func (c *Consumer) AckRange(ctx context.Context, shardID uint32, fromEntry, toEntry int64) error <span class="cov0" title="0">{
        if fromEntry &gt; toEntry </span><span class="cov0" title="0">{
                return fmt.Errorf("invalid range: from %d &gt; to %d", fromEntry, toEntry)
        }</span>

        <span class="cov0" title="0">shard, err := c.client.getOrCreateShard(shardID)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov0" title="0">shard.mu.Lock()
        // Update to one past the end of the range
        newOffset := toEntry + 1
        currentOffset, exists := shard.index.ConsumerOffsets[c.group]

        // Only update if this advances the offset (no going backwards)
        if !exists || newOffset &gt; currentOffset </span><span class="cov0" title="0">{
                shard.index.ConsumerOffsets[c.group] = newOffset
                // Mark that we need a checkpoint
                shard.writesSinceCheckpoint++
        }</span>
        <span class="cov0" title="0">shard.mu.Unlock()

        return nil</span>
}

// StreamStats returns statistics about a shard
// Fields ordered for optimal memory alignment
type StreamStats struct {
        // 64-bit aligned fields first
        TotalEntries int64
        TotalBytes   int64
        OldestEntry  time.Time
        NewestEntry  time.Time

        // Composite types
        ConsumerOffsets map[string]int64

        // Smaller fields last
        FileCount int    // 8 bytes
        ShardID   uint32 // 4 bytes
        // 4 bytes padding
}

// GetShardStats returns statistics for a specific shard
func (c *Consumer) GetShardStats(ctx context.Context, shardID uint32) (*StreamStats, error) <span class="cov0" title="0">{
        shard, err := c.client.getOrCreateShard(shardID)
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>

        // Check unified state for instant change detection
        <span class="cov0" title="0">if shard.state != nil </span><span class="cov0" title="0">{
                currentTimestamp := shard.state.GetLastIndexUpdate()
                if currentTimestamp != shard.lastMmapCheck </span><span class="cov0" title="0">{
                        // Index changed - reload it under write lock
                        shard.mu.Lock()
                        // Double-check after acquiring lock
                        if currentTimestamp != shard.lastMmapCheck </span><span class="cov0" title="0">{
                                if err := shard.loadIndexWithRecovery(); err != nil </span><span class="cov0" title="0">{
                                        shard.mu.Unlock()
                                        return nil, fmt.Errorf("failed to reload index after detecting mmap change: %w", err)
                                }</span>
                                <span class="cov0" title="0">shard.lastMmapCheck = currentTimestamp</span>
                        }
                        <span class="cov0" title="0">shard.mu.Unlock()</span>
                }
        }

        <span class="cov0" title="0">shard.mu.RLock()
        defer shard.mu.RUnlock()

        stats := &amp;StreamStats{
                ShardID:         shardID,
                FileCount:       len(shard.index.Files),
                ConsumerOffsets: make(map[string]int64),
        }

        // Copy consumer offsets
        for group, offset := range shard.index.ConsumerOffsets </span><span class="cov0" title="0">{
                stats.ConsumerOffsets[group] = offset
        }</span>

        // Calculate totals from files
        <span class="cov0" title="0">for _, file := range shard.index.Files </span><span class="cov0" title="0">{
                stats.TotalEntries += file.Entries
                stats.TotalBytes += file.EndOffset - file.StartOffset

                if stats.OldestEntry.IsZero() || file.StartTime.Before(stats.OldestEntry) </span><span class="cov0" title="0">{
                        stats.OldestEntry = file.StartTime
                }</span>

                <span class="cov0" title="0">if file.EndTime.After(stats.NewestEntry) </span><span class="cov0" title="0">{
                        stats.NewestEntry = file.EndTime
                }</span>
        }

        <span class="cov0" title="0">return stats, nil</span>
}

// discoverShards discovers available shards based on stream pattern and consumer assignment
func (c *Consumer) discoverShards(streamPattern string, consumerID, consumerCount int) ([]uint32, error) <span class="cov0" title="0">{
        // Apply defaults
        if consumerCount &lt;= 0 </span><span class="cov0" title="0">{
                consumerCount = 1
        }</span>
        <span class="cov0" title="0">if consumerID &lt; 0 || consumerID &gt;= consumerCount </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("invalid consumerID %d for consumerCount %d", consumerID, consumerCount)
        }</span>

        // Parse stream pattern (e.g., "events:v1:shard:*")
        <span class="cov0" title="0">if !strings.HasSuffix(streamPattern, ":*") </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("stream pattern must end with :* (e.g., events:v1:shard:*)")
        }</span>

        <span class="cov0" title="0">baseStream := strings.TrimSuffix(streamPattern, "*")

        // Discover all available shards
        // Check up to 32 shards by default (can be increased if needed)
        maxShards := uint32(32)
        var allShards []uint32

        // Try to discover shards in parallel for better performance
        type result struct {
                shardID uint32
                exists  bool
        }

        results := make(chan result, maxShards)
        var wg sync.WaitGroup

        for shardID := uint32(0); shardID &lt; maxShards; shardID++ </span><span class="cov0" title="0">{
                wg.Add(1)
                go func(id uint32) </span><span class="cov0" title="0">{
                        defer wg.Done()
                        streamName := fmt.Sprintf("%s%04d", baseStream, id)
                        _, err := c.client.Len(context.Background(), streamName)
                        results &lt;- result{shardID: id, exists: err == nil}
                }</span>(shardID)
        }

        <span class="cov0" title="0">go func() </span><span class="cov0" title="0">{
                wg.Wait()
                close(results)
        }</span>()

        <span class="cov0" title="0">for res := range results </span><span class="cov0" title="0">{
                if res.exists </span><span class="cov0" title="0">{
                        allShards = append(allShards, res.shardID)
                }</span>
        }

        <span class="cov0" title="0">if len(allShards) == 0 </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("no shards found for pattern %s", streamPattern)
        }</span>

        // Sort shards for predictable assignment
        <span class="cov0" title="0">slices.Sort(allShards)

        // Assign shards to this consumer using modulo distribution
        var assignedShards []uint32
        for _, shardID := range allShards </span><span class="cov0" title="0">{
                if int(shardID)%consumerCount == consumerID </span><span class="cov0" title="0">{
                        assignedShards = append(assignedShards, shardID)
                }</span>
        }

        <span class="cov0" title="0">return assignedShards, nil</span>
}
</pre>
		
		<pre class="file" id="file2" style="display: none">package comet

import (
        "os"
        "strings"
)

// Debug controls whether debug logging is enabled
var Debug bool

func init() <span class="cov8" title="1">{
        // Check environment variable for debug mode
        debugEnv := os.Getenv("COMET_DEBUG")
        Debug = debugEnv != "" &amp;&amp; debugEnv != "0" &amp;&amp; strings.ToLower(debugEnv) != "false"
}</span>

// SetDebug allows runtime control of debug mode
func SetDebug(enabled bool) <span class="cov0" title="0">{
        Debug = enabled
}</span>

// IsDebug returns whether debug mode is enabled
func IsDebug() bool <span class="cov0" title="0">{
        return Debug
}</span>
</pre>
		
		<pre class="file" id="file3" style="display: none">package comet

import (
        "encoding/binary"
        "fmt"
        "io"
        "os"
        "time"
)

// Binary index format:
// Header:
//   [4] Magic number (0x434F4D54 = "COMT")
//   [4] Version (1)
//   [8] Current entry number
//   [8] Current write offset
//   [4] Consumer count
//   [4] Binary index node count
// Consumers:
//   For each consumer:
//     [1] Group name length
//     [N] Group name (UTF-8)
//     [8] Offset
// Binary index nodes:
//   For each node:
//     [8] Entry number
//     [4] File index
//     [8] Byte offset

const (
        indexMagic   = 0x434F4D54 // "COMT"
        indexVersion = 1
)

// saveBinaryIndex writes the index in binary format
func (s *Shard) saveBinaryIndex(index *ShardIndex) error <span class="cov0" title="0">{
        // Create temp file
        tempPath := s.indexPath + ".tmp"
        f, err := os.Create(tempPath)
        if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to create temp index: %w", err)
        }</span>
        <span class="cov0" title="0">defer f.Close()

        // Write header
        if err := binary.Write(f, binary.LittleEndian, uint32(indexMagic)); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov0" title="0">if err := binary.Write(f, binary.LittleEndian, uint32(indexVersion)); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov0" title="0">if err := binary.Write(f, binary.LittleEndian, uint64(index.CurrentEntryNumber)); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov0" title="0">if err := binary.Write(f, binary.LittleEndian, uint64(index.CurrentWriteOffset)); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov0" title="0">if err := binary.Write(f, binary.LittleEndian, uint32(len(index.ConsumerOffsets))); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov0" title="0">if err := binary.Write(f, binary.LittleEndian, uint32(len(index.BinaryIndex.Nodes))); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov0" title="0">if err := binary.Write(f, binary.LittleEndian, uint32(len(index.Files))); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        // Write consumer offsets
        <span class="cov0" title="0">for group, offset := range index.ConsumerOffsets </span><span class="cov0" title="0">{
                groupBytes := []byte(group)
                if err := binary.Write(f, binary.LittleEndian, uint8(len(groupBytes))); err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>
                <span class="cov0" title="0">if _, err := f.Write(groupBytes); err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>
                <span class="cov0" title="0">if err := binary.Write(f, binary.LittleEndian, uint64(offset)); err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>
        }

        // Write binary index nodes in a single buffer to reduce syscalls
        <span class="cov0" title="0">nodeCount := len(index.BinaryIndex.Nodes)
        if nodeCount &gt; 0 </span><span class="cov0" title="0">{
                // Pre-allocate buffer for all nodes (20 bytes per node)
                nodeBuf := make([]byte, nodeCount*20)
                offset := 0

                for _, node := range index.BinaryIndex.Nodes </span><span class="cov0" title="0">{
                        // EntryNumber (8 bytes)
                        binary.LittleEndian.PutUint64(nodeBuf[offset:], uint64(node.EntryNumber))
                        offset += 8

                        // FileIndex (4 bytes)
                        binary.LittleEndian.PutUint32(nodeBuf[offset:], uint32(node.Position.FileIndex))
                        offset += 4

                        // ByteOffset (8 bytes)
                        binary.LittleEndian.PutUint64(nodeBuf[offset:], uint64(node.Position.ByteOffset))
                        offset += 8
                }</span>

                <span class="cov0" title="0">if _, err := f.Write(nodeBuf); err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>
        }

        // Write files info - estimate buffer size and batch writes
        <span class="cov0" title="0">fileCount := len(index.Files)
        if fileCount &gt; 0 </span><span class="cov0" title="0">{
                // Estimate buffer size: 2 (path len) + avg path length + 48 bytes per file
                estimatedSize := 0
                for _, file := range index.Files </span><span class="cov0" title="0">{
                        estimatedSize += 2 + len(file.Path) + 48
                }</span>

                <span class="cov0" title="0">buf := make([]byte, 0, estimatedSize)

                for _, file := range index.Files </span><span class="cov0" title="0">{
                        // Write path length (2 bytes)
                        pathBytes := []byte(file.Path)
                        pathLen := uint16(len(pathBytes))
                        buf = append(buf, byte(pathLen), byte(pathLen&gt;&gt;8))

                        // Write path
                        buf = append(buf, pathBytes...)

                        // Write file metadata (48 bytes total)
                        metaBuf := make([]byte, 48)
                        binary.LittleEndian.PutUint64(metaBuf[0:], uint64(file.StartOffset))
                        binary.LittleEndian.PutUint64(metaBuf[8:], uint64(file.EndOffset))
                        binary.LittleEndian.PutUint64(metaBuf[16:], uint64(file.StartEntry))
                        binary.LittleEndian.PutUint64(metaBuf[24:], uint64(file.Entries))
                        binary.LittleEndian.PutUint64(metaBuf[32:], uint64(file.StartTime.UnixNano()))
                        binary.LittleEndian.PutUint64(metaBuf[40:], uint64(file.EndTime.UnixNano()))

                        buf = append(buf, metaBuf...)
                }</span>

                <span class="cov0" title="0">if _, err := f.Write(buf); err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>
        }

        // Note: We don't sync here to avoid performance issues during frequent rotations
        // The index will be synced during periodic checkpoints

        // Atomic rename
        <span class="cov0" title="0">return os.Rename(tempPath, s.indexPath)</span>
}

// loadBinaryIndex reads the index from binary format
func (s *Shard) loadBinaryIndex() (*ShardIndex, error) <span class="cov0" title="0">{
        data, err := os.ReadFile(s.indexPath)
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>

        <span class="cov0" title="0">if len(data) &lt; 32 </span><span class="cov0" title="0">{ // Minimum header size
                return nil, fmt.Errorf("index file too small")
        }</span>

        // Read and verify header
        <span class="cov0" title="0">offset := 0
        magic := binary.LittleEndian.Uint32(data[offset:])
        offset += 4
        if magic != indexMagic </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("invalid index magic: %x", magic)
        }</span>

        <span class="cov0" title="0">version := binary.LittleEndian.Uint32(data[offset:])
        offset += 4
        if version != indexVersion </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("unsupported index version: %d", version)
        }</span>

        <span class="cov0" title="0">index := &amp;ShardIndex{
                ConsumerOffsets: make(map[string]int64),
                BinaryIndex: BinarySearchableIndex{
                        IndexInterval: s.index.BinaryIndex.IndexInterval,
                        MaxNodes:      s.index.BinaryIndex.MaxNodes,
                },
        }

        index.CurrentEntryNumber = int64(binary.LittleEndian.Uint64(data[offset:]))
        offset += 8
        index.CurrentWriteOffset = int64(binary.LittleEndian.Uint64(data[offset:]))
        offset += 8

        consumerCount := binary.LittleEndian.Uint32(data[offset:])
        offset += 4
        nodeCount := binary.LittleEndian.Uint32(data[offset:])
        offset += 4
        fileCount := uint32(0)
        if offset &lt; len(data)-4 </span><span class="cov0" title="0">{
                fileCount = binary.LittleEndian.Uint32(data[offset:])
                offset += 4
        }</span>

        // Read consumer offsets
        <span class="cov0" title="0">for i := uint32(0); i &lt; consumerCount; i++ </span><span class="cov0" title="0">{
                if offset &gt;= len(data) </span><span class="cov0" title="0">{
                        return nil, io.ErrUnexpectedEOF
                }</span>

                <span class="cov0" title="0">groupLen := int(data[offset])
                offset++

                if offset+groupLen+8 &gt; len(data) </span><span class="cov0" title="0">{
                        return nil, io.ErrUnexpectedEOF
                }</span>

                <span class="cov0" title="0">group := string(data[offset : offset+groupLen])
                offset += groupLen

                consumerOffset := int64(binary.LittleEndian.Uint64(data[offset:]))
                offset += 8

                index.ConsumerOffsets[group] = consumerOffset</span>
        }

        // Read binary index nodes
        <span class="cov0" title="0">index.BinaryIndex.Nodes = make([]EntryIndexNode, 0, nodeCount)
        for i := uint32(0); i &lt; nodeCount; i++ </span><span class="cov0" title="0">{
                if offset+20 &gt; len(data) </span><span class="cov0" title="0">{
                        return nil, io.ErrUnexpectedEOF
                }</span>

                <span class="cov0" title="0">node := EntryIndexNode{
                        EntryNumber: int64(binary.LittleEndian.Uint64(data[offset:])),
                        Position: EntryPosition{
                                FileIndex:  int(binary.LittleEndian.Uint32(data[offset+8:])),
                                ByteOffset: int64(binary.LittleEndian.Uint64(data[offset+12:])),
                        },
                }
                offset += 20

                index.BinaryIndex.Nodes = append(index.BinaryIndex.Nodes, node)</span>
        }

        // Read files info
        <span class="cov0" title="0">index.Files = make([]FileInfo, 0, fileCount)
        for i := uint32(0); i &lt; fileCount; i++ </span><span class="cov0" title="0">{
                if offset+2 &gt; len(data) </span><span class="cov0" title="0">{
                        return nil, io.ErrUnexpectedEOF
                }</span>

                <span class="cov0" title="0">pathLen := int(binary.LittleEndian.Uint16(data[offset:]))
                offset += 2

                if offset+pathLen &gt; len(data) </span><span class="cov0" title="0">{
                        return nil, io.ErrUnexpectedEOF
                }</span>

                <span class="cov0" title="0">path := string(data[offset : offset+pathLen])
                offset += pathLen

                if offset+48 &gt; len(data) </span><span class="cov0" title="0">{ // 6 uint64 fields
                        return nil, io.ErrUnexpectedEOF
                }</span>

                <span class="cov0" title="0">file := FileInfo{
                        Path:        path,
                        StartOffset: int64(binary.LittleEndian.Uint64(data[offset:])),
                        EndOffset:   int64(binary.LittleEndian.Uint64(data[offset+8:])),
                        StartEntry:  int64(binary.LittleEndian.Uint64(data[offset+16:])),
                        Entries:     int64(binary.LittleEndian.Uint64(data[offset+24:])),
                        StartTime:   time.Unix(0, int64(binary.LittleEndian.Uint64(data[offset+32:]))),
                        EndTime:     time.Unix(0, int64(binary.LittleEndian.Uint64(data[offset+40:]))),
                }
                offset += 48

                index.Files = append(index.Files, file)</span>
        }

        // Set current file from last file if available
        <span class="cov0" title="0">if len(index.Files) &gt; 0 </span><span class="cov0" title="0">{
                index.CurrentFile = index.Files[len(index.Files)-1].Path
        }</span> else<span class="cov0" title="0"> {
                index.CurrentFile = s.index.CurrentFile
        }</span>
        <span class="cov0" title="0">index.BoundaryInterval = s.index.BoundaryInterval

        return index, nil</span>
}
</pre>
		
		<pre class="file" id="file4" style="display: none">package comet

import (
        "context"
        "fmt"
        "io"
        "log/slog"
        "os"
)

// LogLevel represents the severity of a log message
type LogLevel int

const (
        LogLevelDebug LogLevel = iota
        LogLevelInfo
        LogLevelWarn
        LogLevelError
)

// Logger is the interface for Comet's logging needs.
// It's designed to be simple and easy to adapt to various logging libraries.
type Logger interface {
        // Debug logs a debug message with optional key-value pairs
        Debug(msg string, keysAndValues ...interface{})

        // Info logs an informational message with optional key-value pairs
        Info(msg string, keysAndValues ...interface{})

        // Warn logs a warning message with optional key-value pairs
        Warn(msg string, keysAndValues ...interface{})

        // Error logs an error message with optional key-value pairs
        Error(msg string, keysAndValues ...interface{})

        // WithContext returns a logger with the given context
        WithContext(ctx context.Context) Logger

        // WithFields returns a logger with the given fields attached
        WithFields(keysAndValues ...interface{}) Logger
}

// NoOpLogger is a logger that discards all log messages
type NoOpLogger struct{}

var _ Logger = NoOpLogger{}

func (NoOpLogger) Debug(msg string, keysAndValues ...interface{})   {<span class="cov0" title="0">}</span>
func (NoOpLogger) Info(msg string, keysAndValues ...interface{})    {<span class="cov0" title="0">}</span>
func (NoOpLogger) Warn(msg string, keysAndValues ...interface{})    {<span class="cov0" title="0">}</span>
func (NoOpLogger) Error(msg string, keysAndValues ...interface{})   {<span class="cov0" title="0">}</span>
func (n NoOpLogger) WithContext(ctx context.Context) Logger         <span class="cov0" title="0">{ return n }</span>
func (n NoOpLogger) WithFields(keysAndValues ...interface{}) Logger <span class="cov0" title="0">{ return n }</span>

// StdLogger is a simple logger that writes to stdout/stderr
type StdLogger struct {
        level  LogLevel
        writer io.Writer
        fields []any
}

var _ Logger = (*StdLogger)(nil)

// NewStdLogger creates a new standard logger
func NewStdLogger(level LogLevel) *StdLogger <span class="cov8" title="1">{
        return &amp;StdLogger{
                level:  level,
                writer: os.Stderr,
        }
}</span>

func (s *StdLogger) log(level LogLevel, levelStr, msg string, keysAndValues ...interface{}) <span class="cov0" title="0">{
        if level &lt; s.level </span><span class="cov0" title="0">{
                return
        }</span>

        // Combine fields with keysAndValues
        <span class="cov0" title="0">allFields := append(s.fields, keysAndValues...)

        // Format the message
        output := fmt.Sprintf("[%s] %s", levelStr, msg)

        // Add fields if any
        if len(allFields) &gt; 0 </span><span class="cov0" title="0">{
                output += " {"
                for i := 0; i &lt; len(allFields); i += 2 </span><span class="cov0" title="0">{
                        if i &gt; 0 </span><span class="cov0" title="0">{
                                output += ", "
                        }</span>
                        <span class="cov0" title="0">if i+1 &lt; len(allFields) </span><span class="cov0" title="0">{
                                output += fmt.Sprintf("%v=%v", allFields[i], allFields[i+1])
                        }</span> else<span class="cov0" title="0"> {
                                output += fmt.Sprintf("%v=&lt;missing&gt;", allFields[i])
                        }</span>
                }
                <span class="cov0" title="0">output += "}"</span>
        }

        <span class="cov0" title="0">fmt.Fprintln(s.writer, output)</span>
}

func (s *StdLogger) Debug(msg string, keysAndValues ...interface{}) <span class="cov0" title="0">{
        s.log(LogLevelDebug, "DEBUG", msg, keysAndValues...)
}</span>

func (s *StdLogger) Info(msg string, keysAndValues ...interface{}) <span class="cov0" title="0">{
        s.log(LogLevelInfo, "INFO", msg, keysAndValues...)
}</span>

func (s *StdLogger) Warn(msg string, keysAndValues ...interface{}) <span class="cov0" title="0">{
        s.log(LogLevelWarn, "WARN", msg, keysAndValues...)
}</span>

func (s *StdLogger) Error(msg string, keysAndValues ...interface{}) <span class="cov0" title="0">{
        s.log(LogLevelError, "ERROR", msg, keysAndValues...)
}</span>

func (s *StdLogger) WithContext(ctx context.Context) Logger <span class="cov0" title="0">{
        // For simplicity, we don't use context in StdLogger
        return s
}</span>

func (s *StdLogger) WithFields(keysAndValues ...interface{}) Logger <span class="cov8" title="1">{
        newLogger := &amp;StdLogger{
                level:  s.level,
                writer: s.writer,
                fields: append(s.fields, keysAndValues...),
        }
        return newLogger
}</span>

// SlogAdapter adapts slog.Logger to the Comet Logger interface
type SlogAdapter struct {
        logger *slog.Logger
}

var _ Logger = (*SlogAdapter)(nil)

// NewSlogAdapter creates a new adapter for slog.Logger
func NewSlogAdapter(logger *slog.Logger) *SlogAdapter <span class="cov0" title="0">{
        return &amp;SlogAdapter{logger: logger}
}</span>

func (s *SlogAdapter) Debug(msg string, keysAndValues ...interface{}) <span class="cov0" title="0">{
        s.logger.Debug(msg, keysAndValues...)
}</span>

func (s *SlogAdapter) Info(msg string, keysAndValues ...interface{}) <span class="cov0" title="0">{
        s.logger.Info(msg, keysAndValues...)
}</span>

func (s *SlogAdapter) Warn(msg string, keysAndValues ...interface{}) <span class="cov0" title="0">{
        s.logger.Warn(msg, keysAndValues...)
}</span>

func (s *SlogAdapter) Error(msg string, keysAndValues ...interface{}) <span class="cov0" title="0">{
        s.logger.Error(msg, keysAndValues...)
}</span>

func (s *SlogAdapter) WithContext(ctx context.Context) Logger <span class="cov0" title="0">{
        // slog doesn't have built-in context support in the same way
        // You could extract values from context and add as fields if needed
        return s
}</span>

func (s *SlogAdapter) WithFields(keysAndValues ...interface{}) Logger <span class="cov0" title="0">{
        // Create a new logger with additional fields
        args := make([]any, 0, len(keysAndValues))
        args = append(args, keysAndValues...)
        newLogger := s.logger.With(args...)
        return &amp;SlogAdapter{logger: newLogger}
}</span>

// Helper function to create appropriate logger based on config
func createLogger(config LogConfig) Logger <span class="cov8" title="1">{
        if config.Logger != nil </span><span class="cov0" title="0">{
                return config.Logger
        }</span>

        <span class="cov8" title="1">switch config.Level </span>{
        case "none", "off":<span class="cov0" title="0">
                return NoOpLogger{}</span>
        default:<span class="cov8" title="1">
                level := LogLevelInfo
                switch config.Level </span>{
                case "debug":<span class="cov0" title="0">
                        level = LogLevelDebug</span>
                case "warn", "warning":<span class="cov0" title="0">
                        level = LogLevelWarn</span>
                case "error":<span class="cov0" title="0">
                        level = LogLevelError</span>
                }
                <span class="cov8" title="1">return NewStdLogger(level)</span>
        }
}
</pre>
		
		<pre class="file" id="file5" style="display: none">package comet

import (
        "sync/atomic"
)

// MetricsProvider defines the interface for metrics tracking
// This allows us to swap implementations based on single vs multi-process mode
type MetricsProvider interface {
        // Write metrics
        IncrementEntries(count uint64)
        AddBytes(bytes uint64)
        AddCompressedBytes(bytes uint64)
        RecordWriteLatency(nanos uint64)
        RecordMinWriteLatency(nanos uint64)
        RecordMaxWriteLatency(nanos uint64)

        // Compression metrics
        SetCompressionRatio(ratio uint64)
        IncrementCompressedEntries(count uint64)
        IncrementSkippedCompression(count uint64)
        AddCompressionWait(nanos uint64)

        // File management metrics
        IncrementFilesCreated(count uint64)
        IncrementFilesDeleted(count uint64)
        IncrementFileRotations(count uint64)
        IncrementCheckpoints(count uint64)
        SetLastCheckpoint(nanos uint64)

        // Consumer metrics
        SetActiveReaders(count uint64)
        SetMaxConsumerLag(lag uint64)

        // Error tracking
        IncrementErrors(count uint64)
        SetLastError(nanos uint64)
        IncrementIndexPersistErrors(count uint64)

        // Get current values
        GetStats() MetricsSnapshot
}

// MetricsSnapshot represents a point-in-time view of metrics
type MetricsSnapshot struct {
        // Write metrics
        TotalEntries     uint64
        TotalBytes       uint64
        TotalCompressed  uint64
        WriteLatencyNano uint64
        MinWriteLatency  uint64
        MaxWriteLatency  uint64

        // Compression metrics
        CompressionRatio    uint64
        CompressedEntries   uint64
        SkippedCompression  uint64
        CompressionWaitNano uint64

        // File management - NEW: separate created/deleted counters
        FilesCreated    uint64
        FilesDeleted    uint64
        FileRotations   uint64
        CheckpointCount uint64
        LastCheckpoint  uint64

        // Consumer metrics
        ActiveReaders uint64
        ConsumerLag   uint64

        // Error tracking
        ErrorCount         uint64
        LastErrorNano      uint64
        IndexPersistErrors uint64
}

// atomicMetrics implements MetricsProvider using atomic operations
// This is the current implementation, good for single-process mode
type atomicMetrics struct {
        // Write metrics
        totalEntries     atomic.Uint64
        totalBytes       atomic.Uint64
        totalCompressed  atomic.Uint64
        writeLatencyNano atomic.Uint64
        minWriteLatency  atomic.Uint64
        maxWriteLatency  atomic.Uint64

        // Compression metrics
        compressionRatio    atomic.Uint64
        compressedEntries   atomic.Uint64
        skippedCompression  atomic.Uint64
        compressionWaitNano atomic.Uint64

        // File management
        filesCreated    atomic.Uint64
        filesDeleted    atomic.Uint64
        fileRotations   atomic.Uint64
        checkpointCount atomic.Uint64
        lastCheckpoint  atomic.Uint64

        // Consumer metrics
        activeReaders atomic.Uint64
        consumerLag   atomic.Uint64

        // Error tracking
        errorCount         atomic.Uint64
        lastErrorNano      atomic.Uint64
        indexPersistErrors atomic.Uint64
}

// Implement all MetricsProvider methods for atomicMetrics
func (m *atomicMetrics) IncrementEntries(count uint64) <span class="cov0" title="0">{
        m.totalEntries.Add(count)
}</span>

func (m *atomicMetrics) AddBytes(bytes uint64) <span class="cov0" title="0">{
        m.totalBytes.Add(bytes)
}</span>

func (m *atomicMetrics) AddCompressedBytes(bytes uint64) <span class="cov0" title="0">{
        m.totalCompressed.Add(bytes)
}</span>

func (m *atomicMetrics) RecordWriteLatency(nanos uint64) <span class="cov0" title="0">{
        // For average, we'd need to track count and sum separately
        // For now, just store the latest
        m.writeLatencyNano.Store(nanos)
}</span>

func (m *atomicMetrics) RecordMinWriteLatency(nanos uint64) <span class="cov0" title="0">{
        for </span><span class="cov0" title="0">{
                current := m.minWriteLatency.Load()
                if current != 0 &amp;&amp; current &lt;= nanos </span><span class="cov0" title="0">{
                        break</span>
                }
                <span class="cov0" title="0">if m.minWriteLatency.CompareAndSwap(current, nanos) </span><span class="cov0" title="0">{
                        break</span>
                }
        }
}

func (m *atomicMetrics) RecordMaxWriteLatency(nanos uint64) <span class="cov0" title="0">{
        for </span><span class="cov0" title="0">{
                current := m.maxWriteLatency.Load()
                if current &gt;= nanos </span><span class="cov0" title="0">{
                        break</span>
                }
                <span class="cov0" title="0">if m.maxWriteLatency.CompareAndSwap(current, nanos) </span><span class="cov0" title="0">{
                        break</span>
                }
        }
}

func (m *atomicMetrics) SetCompressionRatio(ratio uint64) <span class="cov0" title="0">{
        m.compressionRatio.Store(ratio)
}</span>

func (m *atomicMetrics) IncrementCompressedEntries(count uint64) <span class="cov0" title="0">{
        m.compressedEntries.Add(count)
}</span>

func (m *atomicMetrics) IncrementSkippedCompression(count uint64) <span class="cov0" title="0">{
        m.skippedCompression.Add(count)
}</span>

func (m *atomicMetrics) AddCompressionWait(nanos uint64) <span class="cov0" title="0">{
        m.compressionWaitNano.Add(nanos)
}</span>

func (m *atomicMetrics) IncrementFilesCreated(count uint64) <span class="cov0" title="0">{
        m.filesCreated.Add(count)
}</span>

func (m *atomicMetrics) IncrementFilesDeleted(count uint64) <span class="cov0" title="0">{
        m.filesDeleted.Add(count)
}</span>

func (m *atomicMetrics) IncrementFileRotations(count uint64) <span class="cov0" title="0">{
        m.fileRotations.Add(count)
}</span>

func (m *atomicMetrics) IncrementCheckpoints(count uint64) <span class="cov0" title="0">{
        m.checkpointCount.Add(count)
}</span>

func (m *atomicMetrics) SetLastCheckpoint(nanos uint64) <span class="cov0" title="0">{
        m.lastCheckpoint.Store(nanos)
}</span>

func (m *atomicMetrics) SetActiveReaders(count uint64) <span class="cov0" title="0">{
        m.activeReaders.Store(count)
}</span>

func (m *atomicMetrics) SetMaxConsumerLag(lag uint64) <span class="cov0" title="0">{
        m.consumerLag.Store(lag)
}</span>

func (m *atomicMetrics) IncrementErrors(count uint64) <span class="cov0" title="0">{
        m.errorCount.Add(count)
}</span>

func (m *atomicMetrics) SetLastError(nanos uint64) <span class="cov0" title="0">{
        m.lastErrorNano.Store(nanos)
}</span>

func (m *atomicMetrics) IncrementIndexPersistErrors(count uint64) <span class="cov0" title="0">{
        m.indexPersistErrors.Add(count)
}</span>

func (m *atomicMetrics) GetStats() MetricsSnapshot <span class="cov0" title="0">{
        return MetricsSnapshot{
                TotalEntries:        m.totalEntries.Load(),
                TotalBytes:          m.totalBytes.Load(),
                TotalCompressed:     m.totalCompressed.Load(),
                WriteLatencyNano:    m.writeLatencyNano.Load(),
                MinWriteLatency:     m.minWriteLatency.Load(),
                MaxWriteLatency:     m.maxWriteLatency.Load(),
                CompressionRatio:    m.compressionRatio.Load(),
                CompressedEntries:   m.compressedEntries.Load(),
                SkippedCompression:  m.skippedCompression.Load(),
                CompressionWaitNano: m.compressionWaitNano.Load(),
                FilesCreated:        m.filesCreated.Load(),
                FilesDeleted:        m.filesDeleted.Load(),
                FileRotations:       m.fileRotations.Load(),
                CheckpointCount:     m.checkpointCount.Load(),
                LastCheckpoint:      m.lastCheckpoint.Load(),
                ActiveReaders:       m.activeReaders.Load(),
                ConsumerLag:         m.consumerLag.Load(),
                ErrorCount:          m.errorCount.Load(),
                LastErrorNano:       m.lastErrorNano.Load(),
                IndexPersistErrors:  m.indexPersistErrors.Load(),
        }
}</span>

// newAtomicMetrics creates a metrics provider for single-process mode
func newAtomicMetrics() MetricsProvider <span class="cov0" title="0">{
        m := &amp;atomicMetrics{}
        // Initialize min latency to max value so first write updates it
        m.minWriteLatency.Store(^uint64(0))
        return m
}</span>
</pre>
		
		<pre class="file" id="file6" style="display: none">package comet

import (
        "fmt"
        "os"
        "path/filepath"
        "sync/atomic"
        "syscall"
        "unsafe"
)

// mmapMetricsState represents the memory-mapped metrics structure
// CRITICAL: This must be a fixed-size struct with only primitive types
// All fields must be 8-byte aligned for atomic operations
type mmapMetricsState struct {
        // Write metrics
        TotalEntries     uint64
        TotalBytes       uint64
        TotalCompressed  uint64
        WriteLatencyNano uint64
        MinWriteLatency  uint64
        MaxWriteLatency  uint64

        // Compression metrics
        CompressionRatio    uint64
        CompressedEntries   uint64
        SkippedCompression  uint64
        CompressionWaitNano uint64

        // File management
        FilesCreated    uint64
        FilesDeleted    uint64
        FileRotations   uint64
        CheckpointCount uint64
        LastCheckpoint  uint64

        // Consumer metrics
        ActiveReaders uint64
        ConsumerLag   uint64

        // Error tracking
        ErrorCount         uint64
        LastErrorNano      uint64
        IndexPersistErrors uint64

        // Reserved for future expansion
        _reserved [8]uint64
}

// mmapMetrics implements MetricsProvider using memory-mapped file
type mmapMetrics struct {
        path  string
        file  *os.File
        data  []byte
        state *mmapMetricsState
        isNew bool
}

// newMmapMetrics creates a memory-mapped metrics provider
func newMmapMetrics(baseDir string) (MetricsProvider, error) <span class="cov0" title="0">{
        metricsPath := filepath.Join(baseDir, "metrics.state")

        // Check if file exists
        isNew := false
        if _, err := os.Stat(metricsPath); os.IsNotExist(err) </span><span class="cov0" title="0">{
                isNew = true
        }</span>

        // Open or create file
        <span class="cov0" title="0">file, err := os.OpenFile(metricsPath, os.O_RDWR|os.O_CREATE, 0644)
        if err != nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("failed to open metrics file: %w", err)
        }</span>

        // Ensure file is correct size
        <span class="cov0" title="0">const stateSize = unsafe.Sizeof(mmapMetricsState{})
        if err := file.Truncate(int64(stateSize)); err != nil </span><span class="cov0" title="0">{
                file.Close()
                return nil, fmt.Errorf("failed to resize metrics file: %w", err)
        }</span>

        // Memory map the file
        <span class="cov0" title="0">data, err := syscall.Mmap(
                int(file.Fd()),
                0,
                int(stateSize),
                syscall.PROT_READ|syscall.PROT_WRITE,
                syscall.MAP_SHARED,
        )
        if err != nil </span><span class="cov0" title="0">{
                file.Close()
                return nil, fmt.Errorf("failed to mmap metrics file: %w", err)
        }</span>

        // Cast to our state struct
        <span class="cov0" title="0">state := (*mmapMetricsState)(unsafe.Pointer(&amp;data[0]))

        // Initialize if new
        if isNew </span><span class="cov0" title="0">{
                state.MinWriteLatency = ^uint64(0) // Max uint64
        }</span>

        <span class="cov0" title="0">return &amp;mmapMetrics{
                path:  metricsPath,
                file:  file,
                data:  data,
                state: state,
                isNew: isNew,
        }, nil</span>
}

// Close unmaps and closes the metrics file
func (m *mmapMetrics) Close() error <span class="cov0" title="0">{
        if err := syscall.Munmap(m.data); err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to unmap metrics: %w", err)
        }</span>
        <span class="cov0" title="0">return m.file.Close()</span>
}

// Implement all MetricsProvider methods using atomic operations on mmap

func (m *mmapMetrics) IncrementEntries(count uint64) <span class="cov0" title="0">{
        atomic.AddUint64(&amp;m.state.TotalEntries, count)
}</span>

func (m *mmapMetrics) AddBytes(bytes uint64) <span class="cov0" title="0">{
        atomic.AddUint64(&amp;m.state.TotalBytes, bytes)
}</span>

func (m *mmapMetrics) AddCompressedBytes(bytes uint64) <span class="cov0" title="0">{
        atomic.AddUint64(&amp;m.state.TotalCompressed, bytes)
}</span>

func (m *mmapMetrics) RecordWriteLatency(nanos uint64) <span class="cov0" title="0">{
        atomic.StoreUint64(&amp;m.state.WriteLatencyNano, nanos)
}</span>

func (m *mmapMetrics) RecordMinWriteLatency(nanos uint64) <span class="cov0" title="0">{
        for </span><span class="cov0" title="0">{
                current := atomic.LoadUint64(&amp;m.state.MinWriteLatency)
                if current != 0 &amp;&amp; current &lt;= nanos </span><span class="cov0" title="0">{
                        break</span>
                }
                <span class="cov0" title="0">if atomic.CompareAndSwapUint64(&amp;m.state.MinWriteLatency, current, nanos) </span><span class="cov0" title="0">{
                        break</span>
                }
        }
}

func (m *mmapMetrics) RecordMaxWriteLatency(nanos uint64) <span class="cov0" title="0">{
        for </span><span class="cov0" title="0">{
                current := atomic.LoadUint64(&amp;m.state.MaxWriteLatency)
                if current &gt;= nanos </span><span class="cov0" title="0">{
                        break</span>
                }
                <span class="cov0" title="0">if atomic.CompareAndSwapUint64(&amp;m.state.MaxWriteLatency, current, nanos) </span><span class="cov0" title="0">{
                        break</span>
                }
        }
}

func (m *mmapMetrics) SetCompressionRatio(ratio uint64) <span class="cov0" title="0">{
        atomic.StoreUint64(&amp;m.state.CompressionRatio, ratio)
}</span>

func (m *mmapMetrics) IncrementCompressedEntries(count uint64) <span class="cov0" title="0">{
        atomic.AddUint64(&amp;m.state.CompressedEntries, count)
}</span>

func (m *mmapMetrics) IncrementSkippedCompression(count uint64) <span class="cov0" title="0">{
        atomic.AddUint64(&amp;m.state.SkippedCompression, count)
}</span>

func (m *mmapMetrics) AddCompressionWait(nanos uint64) <span class="cov0" title="0">{
        atomic.AddUint64(&amp;m.state.CompressionWaitNano, nanos)
}</span>

func (m *mmapMetrics) IncrementFilesCreated(count uint64) <span class="cov0" title="0">{
        atomic.AddUint64(&amp;m.state.FilesCreated, count)
}</span>

func (m *mmapMetrics) IncrementFilesDeleted(count uint64) <span class="cov0" title="0">{
        atomic.AddUint64(&amp;m.state.FilesDeleted, count)
}</span>

func (m *mmapMetrics) IncrementFileRotations(count uint64) <span class="cov0" title="0">{
        atomic.AddUint64(&amp;m.state.FileRotations, count)
}</span>

func (m *mmapMetrics) IncrementCheckpoints(count uint64) <span class="cov0" title="0">{
        atomic.AddUint64(&amp;m.state.CheckpointCount, count)
}</span>

func (m *mmapMetrics) SetLastCheckpoint(nanos uint64) <span class="cov0" title="0">{
        atomic.StoreUint64(&amp;m.state.LastCheckpoint, nanos)
}</span>

func (m *mmapMetrics) SetActiveReaders(count uint64) <span class="cov0" title="0">{
        atomic.StoreUint64(&amp;m.state.ActiveReaders, count)
}</span>

func (m *mmapMetrics) SetMaxConsumerLag(lag uint64) <span class="cov0" title="0">{
        atomic.StoreUint64(&amp;m.state.ConsumerLag, lag)
}</span>

func (m *mmapMetrics) IncrementErrors(count uint64) <span class="cov0" title="0">{
        atomic.AddUint64(&amp;m.state.ErrorCount, count)
}</span>

func (m *mmapMetrics) SetLastError(nanos uint64) <span class="cov0" title="0">{
        atomic.StoreUint64(&amp;m.state.LastErrorNano, nanos)
}</span>

func (m *mmapMetrics) IncrementIndexPersistErrors(count uint64) <span class="cov0" title="0">{
        atomic.AddUint64(&amp;m.state.IndexPersistErrors, count)
}</span>

func (m *mmapMetrics) GetStats() MetricsSnapshot <span class="cov0" title="0">{
        // Read all values atomically
        return MetricsSnapshot{
                TotalEntries:        atomic.LoadUint64(&amp;m.state.TotalEntries),
                TotalBytes:          atomic.LoadUint64(&amp;m.state.TotalBytes),
                TotalCompressed:     atomic.LoadUint64(&amp;m.state.TotalCompressed),
                WriteLatencyNano:    atomic.LoadUint64(&amp;m.state.WriteLatencyNano),
                MinWriteLatency:     atomic.LoadUint64(&amp;m.state.MinWriteLatency),
                MaxWriteLatency:     atomic.LoadUint64(&amp;m.state.MaxWriteLatency),
                CompressionRatio:    atomic.LoadUint64(&amp;m.state.CompressionRatio),
                CompressedEntries:   atomic.LoadUint64(&amp;m.state.CompressedEntries),
                SkippedCompression:  atomic.LoadUint64(&amp;m.state.SkippedCompression),
                CompressionWaitNano: atomic.LoadUint64(&amp;m.state.CompressionWaitNano),
                FilesCreated:        atomic.LoadUint64(&amp;m.state.FilesCreated),
                FilesDeleted:        atomic.LoadUint64(&amp;m.state.FilesDeleted),
                FileRotations:       atomic.LoadUint64(&amp;m.state.FileRotations),
                CheckpointCount:     atomic.LoadUint64(&amp;m.state.CheckpointCount),
                LastCheckpoint:      atomic.LoadUint64(&amp;m.state.LastCheckpoint),
                ActiveReaders:       atomic.LoadUint64(&amp;m.state.ActiveReaders),
                ConsumerLag:         atomic.LoadUint64(&amp;m.state.ConsumerLag),
                ErrorCount:          atomic.LoadUint64(&amp;m.state.ErrorCount),
                LastErrorNano:       atomic.LoadUint64(&amp;m.state.LastErrorNano),
                IndexPersistErrors:  atomic.LoadUint64(&amp;m.state.IndexPersistErrors),
        }
}</span>
</pre>
		
		<pre class="file" id="file7" style="display: none">package comet

import (
        "encoding/binary"
        "fmt"
        "os"
        "sync"
        "sync/atomic"
        "syscall"
        "time"
)

// MmapWriter implements ultra-fast memory-mapped writes for multi-process mode
type MmapWriter struct {
        mu sync.Mutex

        // Current mapped region
        dataFile     *os.File
        dataPath     string
        mappedData   []byte
        mappedOffset int64 // Where this mapping starts in the file
        mappedSize   int64 // Size of current mapping

        // Configuration
        shardDir        string
        initialSize     int64 // Initial file size (default: 128MB)
        growthIncrement int64 // How much to grow (default: 128MB)
        mappingWindow   int64 // Size of active mapping (default: 32MB)
        maxFileSize     int64 // Max file size before rotation (default: 1GB)

        // References
        index            *ShardIndex
        state            *CometState // Unified state includes metrics and coordination
        rotationLockFile *os.File    // File lock for rotation coordination

        // Local metrics
        remapCount    int64
        rotationCount int64
}

// NewMmapWriter creates a new memory-mapped writer for a shard
func NewMmapWriter(shardDir string, maxFileSize int64, index *ShardIndex, state *CometState, rotationLockFile *os.File) (*MmapWriter, error) <span class="cov0" title="0">{
        w := &amp;MmapWriter{
                shardDir:         shardDir,
                initialSize:      4 * 1024,        // 4KB initial
                growthIncrement:  1 * 1024 * 1024, // 1MB growth
                mappingWindow:    1 * 1024 * 1024, // 1MB active window
                maxFileSize:      maxFileSize,
                index:            index,
                state:            state,
                rotationLockFile: rotationLockFile,
        }

        // Open or create current data file
        if err := w.openCurrentFile(); err != nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("failed to open data file: %w", err)
        }</span>

        <span class="cov0" title="0">return w, nil</span>
}

// openCurrentFile opens the current data file and maps the active region
func (w *MmapWriter) openCurrentFile() error <span class="cov0" title="0">{
        // Get current file index
        fileIndex := w.state.GetActiveFileIndex()
        if fileIndex == 0 </span><span class="cov0" title="0">{
                fileIndex = 1
                w.state.StoreActiveFileIndex(1)
        }</span>

        // Construct file path using standard naming convention
        <span class="cov0" title="0">w.dataPath = fmt.Sprintf("%s/log-%016d.comet", w.shardDir, fileIndex)

        // Check if this is the current file in the index
        createNew := w.index.CurrentFile != w.dataPath

        // Open or create file
        file, err := os.OpenFile(w.dataPath, os.O_CREATE|os.O_RDWR, 0644)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        // Get file size
        <span class="cov0" title="0">stat, err := file.Stat()
        if err != nil </span><span class="cov0" title="0">{
                file.Close()
                return err
        }</span>

        <span class="cov0" title="0">fileSize := stat.Size()

        // Always ensure we have at least a minimal size to map
        minSize := int64(4096) // 4KB minimum
        if fileSize &lt; minSize </span><span class="cov0" title="0">{
                if err := file.Truncate(minSize); err != nil </span><span class="cov0" title="0">{
                        file.Close()
                        return err
                }</span>
                // For new files, write zeros to ensure no garbage data
                <span class="cov0" title="0">if fileSize == 0 </span><span class="cov0" title="0">{
                        zeros := make([]byte, minSize)
                        if _, err := file.Write(zeros); err != nil </span><span class="cov0" title="0">{
                                file.Close()
                                return err
                        }</span>
                }
                <span class="cov0" title="0">fileSize = minSize</span>
        }

        // Store the actual file size
        <span class="cov0" title="0">w.state.StoreFileSize(uint64(fileSize))

        // Update index if this is a new file or we're initializing
        if createNew || w.index.CurrentFile == "" </span><span class="cov0" title="0">{
                w.index.CurrentFile = w.dataPath

                // Check if we need to add this file to the index
                found := false
                for _, f := range w.index.Files </span><span class="cov0" title="0">{
                        if f.Path == w.dataPath </span><span class="cov0" title="0">{
                                found = true
                                break</span>
                        }
                }

                <span class="cov0" title="0">if !found </span><span class="cov0" title="0">{
                        w.index.Files = append(w.index.Files, FileInfo{
                                Path:        w.dataPath,
                                StartOffset: 0,
                                StartEntry:  w.index.CurrentEntryNumber,
                                StartTime:   time.Now(),
                                Entries:     0,
                        })

                        // Track file creation metric
                        if fileSize == 0 || createNew </span><span class="cov0" title="0">{
                                w.state.AddFilesCreated(1)
                        }</span>

                        // If this is the first file (no other files exist), set OldestEntryNanos
                        <span class="cov0" title="0">if len(w.index.Files) == 1 &amp;&amp; atomic.LoadInt64(&amp;w.state.OldestEntryNanos) == 0 </span><span class="cov0" title="0">{
                                atomic.StoreInt64(&amp;w.state.OldestEntryNanos, time.Now().UnixNano())
                        }</span>
                }
        }

        <span class="cov0" title="0">w.dataFile = file

        // Map the active window (last portion of file)
        return w.remapActiveWindow()</span>
}

// remapActiveWindow maps or remaps the active portion of the file
func (w *MmapWriter) remapActiveWindow() error <span class="cov0" title="0">{
        // Get current file size
        fileSize := int64(w.state.GetFileSize())

        // Calculate mapping window
        mappingStart := int64(0) // Always start from beginning for simplicity
        mappingSize := fileSize  // Map the entire file

        // Ensure we have something to map
        if mappingSize &lt;= 0 </span><span class="cov0" title="0">{
                return fmt.Errorf("file size is 0, cannot map")
        }</span>

        // Unmap previous mapping if exists
        <span class="cov0" title="0">if w.mappedData != nil </span><span class="cov0" title="0">{
                if err := syscall.Munmap(w.mappedData); err != nil </span><span class="cov0" title="0">{
                        return fmt.Errorf("failed to unmap: %w", err)
                }</span>
        }

        // Map new window
        <span class="cov0" title="0">data, err := syscall.Mmap(int(w.dataFile.Fd()), mappingStart, int(mappingSize),
                syscall.PROT_READ|syscall.PROT_WRITE, syscall.MAP_SHARED)
        if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to mmap window: %w", err)
        }</span>

        <span class="cov0" title="0">w.mappedData = data
        w.mappedOffset = mappingStart
        w.mappedSize = mappingSize
        atomic.AddInt64(&amp;w.remapCount, 1)

        return nil</span>
}

// Write appends entries using memory-mapped I/O
func (w *MmapWriter) Write(entries [][]byte, entryNumbers []uint64) error <span class="cov0" title="0">{
        if len(entries) == 0 </span><span class="cov0" title="0">{
                return nil
        }</span>

        // Calculate total size needed
        <span class="cov0" title="0">totalSize := int64(0)
        for _, entry := range entries </span><span class="cov0" title="0">{
                totalSize += 12 + int64(len(entry)) // header + data
        }</span>

        // Atomically allocate write space
        <span class="cov0" title="0">writeOffset := w.state.AddWriteOffset(uint64(totalSize)) - uint64(totalSize)

        // Check if we need rotation - return special error to let shard handle it
        if int64(writeOffset)+totalSize &gt; w.maxFileSize </span><span class="cov0" title="0">{
                // Roll back the allocation
                w.state.AddWriteOffset(uint64(-totalSize))

                // Return special error to indicate rotation is needed
                // The shard will handle rotation and index updates properly
                return fmt.Errorf("rotation needed: current file would exceed size limit")
        }</span>

        // Write entries - hold lock for entire write operation including growth check
        <span class="cov0" title="0">w.mu.Lock()
        defer w.mu.Unlock()

        // Check if we need to grow the file (inside lock to prevent races)
        currentFileSize := int64(w.state.GetFileSize())
        if int64(writeOffset)+totalSize &gt; currentFileSize </span><span class="cov0" title="0">{
                if err := w.growFile(int64(writeOffset) + totalSize); err != nil </span><span class="cov0" title="0">{
                        // Roll back the allocation on error
                        w.state.AddWriteOffset(uint64(-totalSize))
                        return err
                }</span>
        }

        // Check if we have a valid file
        <span class="cov0" title="0">if w.dataFile == nil </span><span class="cov0" title="0">{
                return fmt.Errorf("data file is nil")
        }</span>

        // Since we're mapping the entire file, just check if we have enough space
        <span class="cov0" title="0">if int64(writeOffset)+totalSize &gt; w.mappedSize </span><span class="cov0" title="0">{
                // Need to remap after growing
                if err := w.remapActiveWindow(); err != nil </span><span class="cov0" title="0">{
                        return fmt.Errorf("failed to remap for write: %w", err)
                }</span>
        }

        // Verify we have enough mapped space after potential remap
        <span class="cov0" title="0">if int64(writeOffset)+totalSize &gt; w.mappedSize </span><span class="cov0" title="0">{
                return fmt.Errorf("insufficient mapped space: need %d bytes at offset %d, but only %d bytes mapped",
                        totalSize, writeOffset, w.mappedSize)
        }</span>

        // Write directly to mapped memory
        <span class="cov0" title="0">offset := int64(writeOffset)
        now := time.Now().UnixNano()
        for i, entry := range entries </span><span class="cov0" title="0">{
                // Since we map from beginning, offset is the position
                pos := offset

                // Double-check bounds before writing
                if pos+12+int64(len(entry)) &gt; int64(len(w.mappedData)) </span><span class="cov0" title="0">{
                        return fmt.Errorf("write would exceed mapped region: pos=%d, entry=%d bytes, mapped=%d bytes",
                                pos, len(entry)+12, len(w.mappedData))
                }</span>

                // Write header (12 bytes)
                <span class="cov0" title="0">binary.LittleEndian.PutUint32(w.mappedData[pos:pos+4], uint32(len(entry)))
                binary.LittleEndian.PutUint64(w.mappedData[pos+4:pos+12], uint64(now))

                // Write data
                copy(w.mappedData[pos+12:pos+12+int64(len(entry))], entry)

                offset += 12 + int64(len(entry))
                _ = entryNumbers[i]</span> // Entry numbers already allocated by caller
        }

        // CRITICAL: Ensure file size reflects what we wrote
        // This is what makes the data visible to readers
        <span class="cov0" title="0">finalOffset := int64(writeOffset) + totalSize
        if stat, err := w.dataFile.Stat(); err == nil </span><span class="cov0" title="0">{
                if stat.Size() &lt; finalOffset </span><span class="cov0" title="0">{
                        // File is smaller than what we wrote - extend it
                        if err := w.dataFile.Truncate(finalOffset); err != nil </span><span class="cov0" title="0">{
                                return fmt.Errorf("failed to extend file to written size: %w", err)
                        }</span>
                }
        }

        // Update coordination state
        <span class="cov0" title="0">w.state.StoreLastWriteNanos(now)
        w.state.AddTotalWrites(uint64(len(entries)))

        // Update unified state metrics
        totalBytes := int64(0)
        for _, entry := range entries </span><span class="cov0" title="0">{
                totalBytes += int64(len(entry))
        }</span>

        <span class="cov0" title="0">w.state.AddTotalEntries(int64(len(entries)))
        w.state.AddTotalBytes(uint64(totalBytes))

        // If this is the first write ever (OldestEntryNanos not set), set it
        if atomic.LoadInt64(&amp;w.state.OldestEntryNanos) == 0 </span><span class="cov0" title="0">{
                atomic.StoreInt64(&amp;w.state.OldestEntryNanos, now)
        }</span>

        // Note: Index updates are handled by the caller (shard) which holds the appropriate locks
        // We only update the coordination state here

        <span class="cov0" title="0">return nil</span>
}

// growFile grows the file to accommodate more data
func (w *MmapWriter) growFile(minSize int64) error <span class="cov0" title="0">{
        // Calculate new size (round up to growth increment)
        newSize := ((minSize + w.growthIncrement - 1) / w.growthIncrement) * w.growthIncrement

        // Grow the file (already holding lock)
        if w.dataFile == nil </span><span class="cov0" title="0">{
                return fmt.Errorf("data file is nil")
        }</span>

        <span class="cov0" title="0">if err := w.dataFile.Truncate(newSize); err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to grow file: %w", err)
        }</span>

        // Update coordination state
        <span class="cov0" title="0">w.state.StoreFileSize(uint64(newSize))

        // Remap if needed
        if w.mappedOffset+w.mappedSize &lt; minSize </span><span class="cov0" title="0">{
                return w.remapActiveWindow()
        }</span>

        <span class="cov0" title="0">return nil</span>
}

// rotateFile handles file rotation when the current file is full
func (w *MmapWriter) rotateFile() error <span class="cov0" title="0">{
        // Use proper file locking for multi-process coordination
        if w.rotationLockFile != nil </span><span class="cov0" title="0">{
                // Use non-blocking try-lock to avoid hanging if another process is rotating
                if err := syscall.Flock(int(w.rotationLockFile.Fd()), syscall.LOCK_EX|syscall.LOCK_NB); err != nil </span><span class="cov0" title="0">{
                        // Another process is rotating - just return, they'll handle it
                        return nil
                }</span>
                <span class="cov0" title="0">defer syscall.Flock(int(w.rotationLockFile.Fd()), syscall.LOCK_UN)</span>
        }
        // Note: For single-process mode (rotationLockFile == nil), rotation is already
        // protected by the shard mutex in the caller, so no additional coordination needed.

        // Close current file
        <span class="cov0" title="0">w.mu.Lock()
        defer w.mu.Unlock()

        if w.mappedData != nil </span><span class="cov0" title="0">{
                syscall.Munmap(w.mappedData)
                w.mappedData = nil
        }</span>

        <span class="cov0" title="0">if w.dataFile != nil </span><span class="cov0" title="0">{
                w.dataFile.Close()
                w.dataFile = nil
        }</span>

        // Increment file index
        <span class="cov0" title="0">newIndex := w.state.AddActiveFileIndex(1)

        // Reset write offset
        w.state.StoreWriteOffset(0)
        w.state.StoreFileSize(0)

        // Open new file using standard naming convention
        w.dataPath = fmt.Sprintf("%s/log-%016d.comet", w.shardDir, newIndex)
        file, err := os.OpenFile(w.dataPath, os.O_CREATE|os.O_RDWR|os.O_EXCL, 0644)
        if err != nil </span><span class="cov0" title="0">{
                if os.IsExist(err) </span><span class="cov0" title="0">{
                        // Another process created it, just open it
                        file, err = os.OpenFile(w.dataPath, os.O_RDWR, 0644)
                        if err != nil </span><span class="cov0" title="0">{
                                return fmt.Errorf("failed to open existing file: %w", err)
                        }</span>
                } else<span class="cov0" title="0"> {
                        return fmt.Errorf("failed to create new file: %w", err)
                }</span>
        }

        // Initial size for new file - start small
        <span class="cov0" title="0">initialSize := int64(4096) // 4KB
        if err := file.Truncate(initialSize); err != nil </span><span class="cov0" title="0">{
                file.Close()
                return err
        }</span>

        <span class="cov0" title="0">w.dataFile = file
        w.state.StoreFileSize(uint64(initialSize))
        atomic.AddInt64(&amp;w.rotationCount, 1)

        // Update unified state metrics
        w.state.AddFileRotations(1)
        w.state.AddFilesCreated(1)

        // Note: Index updates are handled by the caller (shard) which holds the appropriate locks
        // We don't directly modify the index here to avoid race conditions

        // Map the new file
        return w.remapActiveWindow()</span>
}

// Sync ensures data is persisted to disk
func (w *MmapWriter) Sync() error <span class="cov0" title="0">{
        w.mu.Lock()
        defer w.mu.Unlock()

        // On macOS, use file sync instead of msync
        if w.dataFile != nil </span><span class="cov0" title="0">{
                return w.dataFile.Sync()
        }</span>

        <span class="cov0" title="0">return nil</span>
}

// Close cleans up resources
func (w *MmapWriter) Close() error <span class="cov0" title="0">{
        w.mu.Lock()
        defer w.mu.Unlock()

        // Unmap data
        if w.mappedData != nil </span><span class="cov0" title="0">{
                syscall.Munmap(w.mappedData)
                w.mappedData = nil
        }</span>

        // Close file
        <span class="cov0" title="0">if w.dataFile != nil </span><span class="cov0" title="0">{
                w.dataFile.Close()
                w.dataFile = nil
        }</span>

        <span class="cov0" title="0">return nil</span>
}
</pre>
		
		<pre class="file" id="file8" style="display: none">package comet

import (
        "encoding/binary"
        "fmt"
        "os"
        "strings"
        "sync"
        "sync/atomic"
        "syscall"

        "github.com/klauspost/compress/zstd"
)

// Reader provides memory-mapped read access to a shard
// Fields ordered for optimal memory alignment
type Reader struct {
        // Pointers first (8 bytes on 64-bit)
        decompressor *zstd.Decoder
        bufferPool   *sync.Pool

        // Slices (24 bytes: ptr + len + cap)
        files []*MappedFile

        // Mutex (platform-specific size)
        mu sync.RWMutex

        // Smaller fields last
        shardID uint32 // 4 bytes
        // 4 bytes padding
}

// AtomicSlice provides atomic access to a byte slice using atomic.Value
type AtomicSlice struct {
        value atomic.Value // Stores []byte
}

// Load atomically loads the slice
func (a *AtomicSlice) Load() []byte <span class="cov0" title="0">{
        if v := a.value.Load(); v != nil </span><span class="cov0" title="0">{
                return v.([]byte)
        }</span>
        <span class="cov0" title="0">return nil</span>
}

// Store atomically stores a new slice
func (a *AtomicSlice) Store(data []byte) <span class="cov0" title="0">{
        a.value.Store(data)
}</span>

// MappedFile represents a memory-mapped data file with atomic data updates
// Fields ordered for optimal memory alignment (embedded struct first)
type MappedFile struct {
        FileInfo             // Embedded struct (already aligned)
        data     AtomicSlice // Atomic slice for lock-free updates
        file     *os.File    // Pointer (8 bytes)
        remapMu  sync.Mutex  // Mutex for remapping operations only
        lastSize int64       // Last known size for growth detection
}

// NewReader creates a new reader for a shard
func NewReader(shardID uint32, index *ShardIndex) (*Reader, error) <span class="cov0" title="0">{
        r := &amp;Reader{
                shardID: shardID,
                files:   make([]*MappedFile, 0, len(index.Files)),
                bufferPool: &amp;sync.Pool{
                        New: func() any </span><span class="cov0" title="0">{
                                // Start with 64KB buffer, will grow as needed
                                return make([]byte, 0, 64*1024)
                        }</span>,
                },
        }

        // Create decompressor
        <span class="cov0" title="0">dec, err := zstd.NewReader(nil)
        if err != nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("failed to create decompressor: %w", err)
        }</span>
        <span class="cov0" title="0">r.decompressor = dec

        // Memory map all files
        for _, fileInfo := range index.Files </span><span class="cov0" title="0">{
                mapped, err := r.mapFile(fileInfo)
                if err != nil </span><span class="cov0" title="0">{
                        // Clean up already mapped files
                        r.Close()
                        return nil, fmt.Errorf("failed to map file %s: %w", fileInfo.Path, err)
                }</span>
                <span class="cov0" title="0">r.files = append(r.files, mapped)</span>
        }

        <span class="cov0" title="0">return r, nil</span>
}

// mapFile memory maps a single data file
func (r *Reader) mapFile(info FileInfo) (*MappedFile, error) <span class="cov0" title="0">{
        file, err := os.Open(info.Path)
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>

        <span class="cov0" title="0">stat, err := file.Stat()
        if err != nil </span><span class="cov0" title="0">{
                file.Close()
                return nil, err
        }</span>

        <span class="cov0" title="0">size := stat.Size()
        mappedFile := &amp;MappedFile{
                FileInfo: info,
                file:     file,
                lastSize: size,
        }

        if size == 0 </span><span class="cov0" title="0">{
                // For empty files, store an empty byte slice
                mappedFile.data.Store([]byte{})
                return mappedFile, nil
        }</span>

        // Memory map the file
        <span class="cov0" title="0">data, err := syscall.Mmap(int(file.Fd()), 0, int(size), syscall.PROT_READ, syscall.MAP_PRIVATE)
        if err != nil </span><span class="cov0" title="0">{
                file.Close()
                return nil, fmt.Errorf("mmap failed: %w", err)
        }</span>

        // Store data atomically
        <span class="cov0" title="0">mappedFile.data.Store(data)

        return mappedFile, nil</span>
}

// remapFile remaps a file that has grown - now lock-free for readers!
func (r *Reader) remapFile(fileIndex int) error <span class="cov0" title="0">{
        // Only validate file index under read lock
        r.mu.RLock()
        if fileIndex &lt; 0 || fileIndex &gt;= len(r.files) </span><span class="cov0" title="0">{
                r.mu.RUnlock()
                return fmt.Errorf("invalid file index %d", fileIndex)
        }</span>
        <span class="cov0" title="0">mappedFile := r.files[fileIndex]
        r.mu.RUnlock()

        // Use per-file mutex to prevent concurrent remaps of the same file
        // This doesn't block readers of other files or readers using the current mapping
        mappedFile.remapMu.Lock()
        defer mappedFile.remapMu.Unlock()

        // Get current file size
        stat, err := mappedFile.file.Stat()
        if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to stat file: %w", err)
        }</span>

        <span class="cov0" title="0">newSize := stat.Size()

        // Check if we've already been remapped by another goroutine
        if newSize &lt;= mappedFile.lastSize </span><span class="cov0" title="0">{
                return nil // No growth or already remapped
        }</span>

        // Get current data atomically
        <span class="cov0" title="0">oldData := mappedFile.data.Load()
        if oldData != nil &amp;&amp; int64(len(oldData)) &gt;= newSize </span><span class="cov0" title="0">{
                return nil // Already remapped by another goroutine
        }</span>

        // Create new mapping with the current file size
        <span class="cov0" title="0">newData, err := syscall.Mmap(int(mappedFile.file.Fd()), 0, int(newSize), syscall.PROT_READ, syscall.MAP_PRIVATE)
        if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to remap file: %w", err)
        }</span>

        // Atomically update the data pointer - readers will see either old or new mapping
        <span class="cov0" title="0">mappedFile.data.Store(newData)
        mappedFile.lastSize = newSize

        // Unmap old data if it exists
        // Safe to unmap immediately because:
        // 1. We use MAP_PRIVATE (copy-on-write) which protects active readers
        // 2. The atomic.Value ensures readers see either old or new mapping
        // 3. Any active readers have their own memory pages via COW
        if oldData != nil </span><span class="cov0" title="0">{
                go func() </span><span class="cov0" title="0">{
                        // Defer unmapping to avoid blocking the current operation
                        syscall.Munmap(oldData)
                }</span>()
        }

        <span class="cov0" title="0">return nil</span>
}

// ReadEntryAtPosition reads a single entry at the given position
func (r *Reader) ReadEntryAtPosition(pos EntryPosition) ([]byte, error) <span class="cov0" title="0">{
        r.mu.RLock()

        // Validate file index
        if pos.FileIndex &lt; 0 || pos.FileIndex &gt;= len(r.files) </span><span class="cov0" title="0">{
                r.mu.RUnlock()
                return nil, fmt.Errorf("invalid file index %d", pos.FileIndex)
        }</span>

        <span class="cov0" title="0">targetFile := r.files[pos.FileIndex]
        r.mu.RUnlock() // Release read lock early - we'll use atomic operations

        // Check if we need to remap the file due to growth
        currentData := targetFile.data.Load()
        if targetFile.file != nil </span><span class="cov0" title="0">{
                stat, err := targetFile.file.Stat()
                if err == nil &amp;&amp; (currentData == nil || stat.Size() &gt; int64(len(currentData))) </span><span class="cov0" title="0">{
                        // Remap the file with the new size (lock-free!)
                        if err := r.remapFile(pos.FileIndex); err != nil </span><span class="cov0" title="0">{
                                return nil, fmt.Errorf("failed to remap grown file: %w", err)
                        }</span>
                        // Get the potentially updated mapping
                        <span class="cov0" title="0">currentData = targetFile.data.Load()</span>
                }
        }

        // Read from the current mapping (no locks needed!)
        <span class="cov0" title="0">if currentData == nil </span><span class="cov0" title="0">{
                // File was never mapped or is empty, try to remap
                if err := r.remapFile(pos.FileIndex); err != nil </span><span class="cov0" title="0">{
                        return nil, fmt.Errorf("failed to map file: %w", err)
                }</span>
                <span class="cov0" title="0">currentData = targetFile.data.Load()</span>
        }
        <span class="cov0" title="0">data, err := r.readEntryFromFileData(currentData, pos.ByteOffset)
        if err != nil &amp;&amp; strings.Contains(err.Error(), "extends beyond file") </span><span class="cov0" title="0">{
                // Handle the case where index was updated but data hasn't been flushed yet
                // This can occur during active writes - remap and retry once
                if targetFile.file != nil </span><span class="cov0" title="0">{
                        if stat, statErr := targetFile.file.Stat(); statErr == nil </span><span class="cov0" title="0">{
                                currentSize := stat.Size()
                                if currentSize &gt; int64(len(currentData)) </span><span class="cov0" title="0">{
                                        // File has grown, remap and retry
                                        if remapErr := r.remapFile(pos.FileIndex); remapErr == nil </span><span class="cov0" title="0">{
                                                currentData = targetFile.data.Load()
                                                return r.readEntryFromFileData(currentData, pos.ByteOffset)
                                        }</span>
                                }
                        }
                }
        }
        <span class="cov0" title="0">return data, err</span>
}

// Close unmaps all files and cleans up resources
func (r *Reader) Close() error <span class="cov0" title="0">{
        r.mu.Lock()
        defer r.mu.Unlock()

        var firstErr error
        for i := range r.files </span><span class="cov0" title="0">{
                file := r.files[i]

                // Unmap if mapped
                data := file.data.Load()
                if data != nil </span><span class="cov0" title="0">{
                        if err := syscall.Munmap(data); err != nil &amp;&amp; firstErr == nil </span><span class="cov0" title="0">{
                                firstErr = err
                        }</span>
                }

                // Close file
                <span class="cov0" title="0">if file.file != nil </span><span class="cov0" title="0">{
                        if err := file.file.Close(); err != nil &amp;&amp; firstErr == nil </span><span class="cov0" title="0">{
                                firstErr = err
                        }</span>
                }
        }

        <span class="cov0" title="0">return firstErr</span>
}

// readEntryFromFileData reads a single entry from memory-mapped data at a byte offset
func (r *Reader) readEntryFromFileData(data []byte, byteOffset int64) ([]byte, error) <span class="cov0" title="0">{
        if len(data) == 0 </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("file not memory mapped (data length: %d, offset: %d)", len(data), byteOffset)
        }</span>

        <span class="cov0" title="0">if byteOffset+headerSize &gt; int64(len(data)) </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("invalid offset: header extends beyond file")
        }</span>

        // Read header
        <span class="cov0" title="0">header := data[byteOffset : byteOffset+headerSize]
        length := binary.LittleEndian.Uint32(header[0:4])

        // Check bounds
        dataStart := byteOffset + headerSize
        dataEnd := dataStart + int64(length)
        if dataEnd &gt; int64(len(data)) </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("invalid entry: data extends beyond file")
        }</span>

        // Read entry data
        <span class="cov0" title="0">entryData := data[dataStart:dataEnd]

        // Check if data is compressed by looking for zstd magic number
        if len(entryData) &gt;= 4 &amp;&amp; binary.LittleEndian.Uint32(entryData[0:4]) == 0xFD2FB528 </span><span class="cov0" title="0">{
                // Data is compressed - decompress it
                decompressed, err := r.decompressor.DecodeAll(entryData, nil)
                if err != nil </span><span class="cov0" title="0">{
                        return nil, fmt.Errorf("failed to decompress: %w", err)
                }</span>
                <span class="cov0" title="0">return decompressed, nil</span>
        }

        // Data is not compressed - return as is
        <span class="cov0" title="0">return entryData, nil</span>
}
</pre>
		
		<pre class="file" id="file9" style="display: none">package comet

import (
        "container/list"
        "encoding/binary"
        "fmt"
        "os"
        "sync"
        "sync/atomic"
        "syscall"

        "github.com/klauspost/compress/zstd"
)

// ReaderConfig configures the bounded reader behavior
type ReaderConfig struct {
        MapAllFiles       bool  // If true, maps all files (original behavior). If false, uses smart mapping
        MaxMappedFiles    int   // Maximum number of files to keep mapped (default: 10)
        MaxMemoryBytes    int64 // Maximum memory to use for mapping (default: 1GB)
        CleanupIntervalMs int   // How often to run cleanup in milliseconds (default: 5000)
}

// DefaultReaderConfig returns the default configuration for ReaderV2
func DefaultReaderConfig() ReaderConfig <span class="cov8" title="1">{
        return ReaderConfig{
                MapAllFiles:       false,            // Smart mapping by default
                MaxMappedFiles:    10,               // Reasonable default
                MaxMemoryBytes:    1024 * 1024 * 1024, // 1GB default
                CleanupIntervalMs: 5000,             // 5 second cleanup
        }
}</span>

// ReaderV2 provides memory-mapped read access with bounded memory usage
// Only maps files that contain unACKed data
type ReaderV2 struct {
        // Core components
        shardID uint32
        index   *ShardIndex
        config  ReaderConfig
        
        // File management
        fileInfos    []FileInfo                // All file metadata (not mapped)
        mappedFiles  map[int]*MappedFileV2     // Currently mapped files
        mappingMu    sync.RWMutex              // Protects mappedFiles
        
        // Memory tracking
        localMemory int64 // Atomic counter for local memory usage
        
        // LRU cache for recently accessed files
        recentCache *recentFileCache
        
        // Decompression
        decompressor *zstd.Decoder
        bufferPool   *sync.Pool
}

// MappedFileV2 represents a memory-mapped data file with atomic data updates
type MappedFileV2 struct {
        FileInfo             // Embedded file info
        data     atomic.Value  // Stores []byte atomically
        file     *os.File     // File handle
        remapMu  sync.Mutex   // Mutex for remapping operations only
        lastSize int64        // Last known size
}

// recentFileCache implements a simple LRU cache for file indices
type recentFileCache struct {
        mu       sync.Mutex
        capacity int
        items    map[int]*list.Element
        order    *list.List
}

type cacheItem struct {
        fileIndex int
}

func newRecentFileCache(capacity int) *recentFileCache <span class="cov8" title="1">{
        return &amp;recentFileCache{
                capacity: capacity,
                items:    make(map[int]*list.Element),
                order:    list.New(),
        }
}</span>

func (c *recentFileCache) access(fileIndex int) <span class="cov8" title="1">{
        c.mu.Lock()
        defer c.mu.Unlock()
        
        if elem, exists := c.items[fileIndex]; exists </span><span class="cov8" title="1">{
                // Move to front
                c.order.MoveToFront(elem)
                return
        }</span>
        
        // Add new item
        <span class="cov8" title="1">item := &amp;cacheItem{fileIndex: fileIndex}
        elem := c.order.PushFront(item)
        c.items[fileIndex] = elem
        
        // Evict if needed
        if c.order.Len() &gt; c.capacity </span><span class="cov8" title="1">{
                oldest := c.order.Back()
                if oldest != nil </span><span class="cov8" title="1">{
                        oldItem := oldest.Value.(*cacheItem)
                        delete(c.items, oldItem.fileIndex)
                        c.order.Remove(oldest)
                }</span>
        }
}

func (c *recentFileCache) contains(fileIndex int) bool <span class="cov8" title="1">{
        c.mu.Lock()
        defer c.mu.Unlock()
        _, exists := c.items[fileIndex]
        return exists
}</span>

// NewReaderV2 creates a new bounded reader for a shard
func NewReaderV2(shardID uint32, index *ShardIndex, config ReaderConfig) (*ReaderV2, error) <span class="cov8" title="1">{
        r := &amp;ReaderV2{
                shardID:     shardID,
                index:       index,
                config:      config,
                fileInfos:   make([]FileInfo, len(index.Files)),
                mappedFiles: make(map[int]*MappedFileV2),
                recentCache: newRecentFileCache(config.MaxMappedFiles / 2), // Half capacity for recent files
                bufferPool: &amp;sync.Pool{
                        New: func() any </span><span class="cov0" title="0">{
                                return make([]byte, 0, 64*1024)
                        }</span>,
                },
        }
        
        // Copy file infos
        <span class="cov8" title="1">copy(r.fileInfos, index.Files)
        
        // Create decompressor
        dec, err := zstd.NewReader(nil)
        if err != nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("failed to create decompressor: %w", err)
        }</span>
        <span class="cov8" title="1">r.decompressor = dec
        
        // If MapAllFiles is true, map all files (original behavior)
        if config.MapAllFiles </span><span class="cov8" title="1">{
                for i, fileInfo := range r.fileInfos </span><span class="cov8" title="1">{
                        mapped, err := r.mapFile(i, fileInfo)
                        if err != nil </span><span class="cov0" title="0">{
                                r.Close()
                                return nil, fmt.Errorf("failed to map file %s: %w", fileInfo.Path, err)
                        }</span>
                        <span class="cov8" title="1">r.mappedFiles[i] = mapped</span>
                }
        } else<span class="cov8" title="1"> {
                // Smart mapping - only map files that might be needed
                // For now, just map the most recent file to get started
                if len(r.fileInfos) &gt; 0 </span><span class="cov8" title="1">{
                        lastIndex := len(r.fileInfos) - 1
                        mapped, err := r.mapFile(lastIndex, r.fileInfos[lastIndex])
                        if err == nil </span><span class="cov8" title="1">{
                                r.mappedFiles[lastIndex] = mapped
                        }</span>
                }
        }
        
        <span class="cov8" title="1">return r, nil</span>
}

// mapFile memory maps a single data file
func (r *ReaderV2) mapFile(fileIndex int, info FileInfo) (*MappedFileV2, error) <span class="cov8" title="1">{
        file, err := os.Open(info.Path)
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>
        
        <span class="cov8" title="1">stat, err := file.Stat()
        if err != nil </span><span class="cov0" title="0">{
                file.Close()
                return nil, err
        }</span>
        
        <span class="cov8" title="1">size := stat.Size()
        mappedFile := &amp;MappedFileV2{
                FileInfo: info,
                file:     file,
                lastSize: size,
        }
        
        if size == 0 </span><span class="cov8" title="1">{
                // For empty files, store an empty byte slice
                mappedFile.data.Store([]byte{})
                return mappedFile, nil
        }</span>
        
        // Memory map the file
        <span class="cov8" title="1">data, err := syscall.Mmap(int(file.Fd()), 0, int(size), syscall.PROT_READ, syscall.MAP_PRIVATE)
        if err != nil </span><span class="cov0" title="0">{
                file.Close()
                return nil, fmt.Errorf("mmap failed: %w", err)
        }</span>
        
        // Store data atomically
        <span class="cov8" title="1">mappedFile.data.Store(data)
        
        // Update memory tracking
        atomic.AddInt64(&amp;r.localMemory, size)
        
        return mappedFile, nil</span>
}

// ReadEntryAtPosition reads a single entry at the given position
func (r *ReaderV2) ReadEntryAtPosition(pos EntryPosition) ([]byte, error) <span class="cov8" title="1">{
        // Check if file is mapped
        r.mappingMu.RLock()
        mapped, isMapped := r.mappedFiles[pos.FileIndex]
        r.mappingMu.RUnlock()
        
        if !isMapped </span><span class="cov8" title="1">{
                // File is not mapped - need to map it temporarily or return error
                if r.config.MapAllFiles </span><span class="cov0" title="0">{
                        // In MapAllFiles mode, all files should be mapped
                        return nil, fmt.Errorf("file %d not mapped in MapAllFiles mode", pos.FileIndex)
                }</span>
                
                // In smart mapping mode, temporarily map the file if we have capacity
                <span class="cov8" title="1">if err := r.ensureFileMapped(pos.FileIndex); err != nil </span><span class="cov8" title="1">{
                        return nil, fmt.Errorf("failed to map file for read: %w", err)
                }</span>
                
                // Get the mapped file again
                <span class="cov8" title="1">r.mappingMu.RLock()
                mapped = r.mappedFiles[pos.FileIndex]
                r.mappingMu.RUnlock()</span>
        }
        
        <span class="cov8" title="1">if mapped == nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("file %d could not be mapped", pos.FileIndex)
        }</span>
        
        // Mark file as recently accessed
        <span class="cov8" title="1">r.recentCache.access(pos.FileIndex)
        
        // Read from the mapped data
        data := mapped.data.Load().([]byte)
        return r.readEntryFromFileData(data, pos.ByteOffset)</span>
}

// ensureFileMapped ensures a file is mapped, respecting memory limits
func (r *ReaderV2) ensureFileMapped(fileIndex int) error <span class="cov8" title="1">{
        r.mappingMu.Lock()
        defer r.mappingMu.Unlock()
        
        // Check if already mapped
        if _, exists := r.mappedFiles[fileIndex]; exists </span><span class="cov0" title="0">{
                return nil
        }</span>
        
        // Check if we have room for another file
        <span class="cov8" title="1">if len(r.mappedFiles) &gt;= r.config.MaxMappedFiles </span><span class="cov8" title="1">{
                // Need to evict something
                if err := r.evictOldestFile(); err != nil </span><span class="cov0" title="0">{
                        return fmt.Errorf("failed to evict file for mapping: %w", err)
                }</span>
        }
        
        // Check memory limits
        <span class="cov8" title="1">if atomic.LoadInt64(&amp;r.localMemory) &gt;= r.config.MaxMemoryBytes </span><span class="cov0" title="0">{
                // Try to free some memory
                if err := r.evictOldestFile(); err != nil </span><span class="cov0" title="0">{
                        return fmt.Errorf("at memory limit and cannot evict: %w", err)
                }</span>
        }
        
        // Map the file
        <span class="cov8" title="1">if fileIndex &gt;= len(r.fileInfos) </span><span class="cov8" title="1">{
                return fmt.Errorf("file index %d out of range", fileIndex)
        }</span>
        
        <span class="cov8" title="1">mapped, err := r.mapFile(fileIndex, r.fileInfos[fileIndex])
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        
        <span class="cov8" title="1">r.mappedFiles[fileIndex] = mapped
        return nil</span>
}

// evictOldestFile evicts the oldest non-recent file
func (r *ReaderV2) evictOldestFile() error <span class="cov8" title="1">{
        // Find a file to evict (not in recent cache)
        for fileIndex, mapped := range r.mappedFiles </span><span class="cov8" title="1">{
                if !r.recentCache.contains(fileIndex) </span><span class="cov8" title="1">{
                        // Evict this file
                        delete(r.mappedFiles, fileIndex)
                        
                        // Unmap and close
                        if data := mapped.data.Load(); data != nil </span><span class="cov8" title="1">{
                                if dataBytes, ok := data.([]byte); ok &amp;&amp; len(dataBytes) &gt; 0 </span><span class="cov8" title="1">{
                                        syscall.Munmap(dataBytes)
                                        atomic.AddInt64(&amp;r.localMemory, -int64(len(dataBytes)))
                                }</span>
                        }
                        
                        <span class="cov8" title="1">if mapped.file != nil </span><span class="cov8" title="1">{
                                mapped.file.Close()
                        }</span>
                        
                        <span class="cov8" title="1">return nil</span>
                }
        }
        
        // If all files are recent, evict the first one anyway
        <span class="cov0" title="0">for fileIndex, mapped := range r.mappedFiles </span><span class="cov0" title="0">{
                delete(r.mappedFiles, fileIndex)
                
                // Unmap and close
                if data := mapped.data.Load(); data != nil </span><span class="cov0" title="0">{
                        if dataBytes, ok := data.([]byte); ok &amp;&amp; len(dataBytes) &gt; 0 </span><span class="cov0" title="0">{
                                syscall.Munmap(dataBytes)
                                atomic.AddInt64(&amp;r.localMemory, -int64(len(dataBytes)))
                        }</span>
                }
                
                <span class="cov0" title="0">if mapped.file != nil </span><span class="cov0" title="0">{
                        mapped.file.Close()
                }</span>
                
                <span class="cov0" title="0">return nil</span>
        }
        
        <span class="cov0" title="0">return fmt.Errorf("no files to evict")</span>
}

// readEntryFromFileData reads a single entry from memory-mapped data at a byte offset
func (r *ReaderV2) readEntryFromFileData(data []byte, byteOffset int64) ([]byte, error) <span class="cov8" title="1">{
        if len(data) == 0 </span><span class="cov8" title="1">{
                return nil, fmt.Errorf("file not memory mapped")
        }</span>
        
        <span class="cov8" title="1">headerSize := int64(12)
        if byteOffset+headerSize &gt; int64(len(data)) </span><span class="cov8" title="1">{
                return nil, fmt.Errorf("invalid offset: header extends beyond file")
        }</span>
        
        // Read header
        <span class="cov8" title="1">header := data[byteOffset : byteOffset+headerSize]
        length := binary.LittleEndian.Uint32(header[0:4])
        
        // Check bounds
        dataStart := byteOffset + headerSize
        dataEnd := dataStart + int64(length)
        if dataEnd &gt; int64(len(data)) </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("invalid entry: data extends beyond file")
        }</span>
        
        // Read entry data
        <span class="cov8" title="1">entryData := data[dataStart:dataEnd]
        
        // Check if data is compressed by looking for zstd magic number
        if len(entryData) &gt;= 4 &amp;&amp; binary.LittleEndian.Uint32(entryData[0:4]) == 0xFD2FB528 </span><span class="cov8" title="1">{
                // Data is compressed - decompress it
                decompressed, err := r.decompressor.DecodeAll(entryData, nil)
                if err != nil </span><span class="cov0" title="0">{
                        return nil, fmt.Errorf("failed to decompress: %w", err)
                }</span>
                <span class="cov8" title="1">return decompressed, nil</span>
        }
        
        // Data is not compressed - return as is
        <span class="cov8" title="1">return entryData, nil</span>
}

// UpdateFiles updates the reader with new file information
func (r *ReaderV2) UpdateFiles(newFiles []FileInfo) error <span class="cov8" title="1">{
        r.mappingMu.Lock()
        defer r.mappingMu.Unlock()
        
        // Update file infos
        r.fileInfos = make([]FileInfo, len(newFiles))
        copy(r.fileInfos, newFiles)
        
        // In MapAllFiles mode, ensure all files are mapped
        if r.config.MapAllFiles </span><span class="cov0" title="0">{
                for i, fileInfo := range r.fileInfos </span><span class="cov0" title="0">{
                        if _, exists := r.mappedFiles[i]; !exists </span><span class="cov0" title="0">{
                                mapped, err := r.mapFile(i, fileInfo)
                                if err != nil </span><span class="cov0" title="0">{
                                        continue</span> // Skip files that can't be mapped
                                }
                                <span class="cov0" title="0">r.mappedFiles[i] = mapped</span>
                        }
                }
        }
        
        <span class="cov8" title="1">return nil</span>
}

// Close unmaps all files and cleans up resources
func (r *ReaderV2) Close() error <span class="cov8" title="1">{
        r.mappingMu.Lock()
        defer r.mappingMu.Unlock()
        
        var firstErr error
        for _, mapped := range r.mappedFiles </span><span class="cov8" title="1">{
                // Unmap if mapped
                if data := mapped.data.Load(); data != nil </span><span class="cov8" title="1">{
                        if dataBytes, ok := data.([]byte); ok &amp;&amp; len(dataBytes) &gt; 0 </span><span class="cov8" title="1">{
                                if err := syscall.Munmap(dataBytes); err != nil &amp;&amp; firstErr == nil </span><span class="cov0" title="0">{
                                        firstErr = err
                                }</span>
                        }
                }
                
                // Close file
                <span class="cov8" title="1">if mapped.file != nil </span><span class="cov8" title="1">{
                        if err := mapped.file.Close(); err != nil &amp;&amp; firstErr == nil </span><span class="cov0" title="0">{
                                firstErr = err
                        }</span>
                }
        }
        
        <span class="cov8" title="1">r.mappedFiles = make(map[int]*MappedFileV2)
        return firstErr</span>
}

// GetMemoryUsage returns current memory usage statistics
func (r *ReaderV2) GetMemoryUsage() (int64, int) <span class="cov8" title="1">{
        r.mappingMu.RLock()
        defer r.mappingMu.RUnlock()
        
        return atomic.LoadInt64(&amp;r.localMemory), len(r.mappedFiles)
}</pre>
		
		<pre class="file" id="file10" style="display: none">package comet

import (
        "maps"
        "os"
        "sort"
        "sync/atomic"
        "syscall"
        "time"
)

// RetentionStats provides type-safe retention statistics
// Fields ordered for optimal memory alignment
type RetentionStats struct {
        // 8-byte aligned fields first
        TotalSizeBytes int64         `json:"total_size_bytes"`
        TotalSizeGB    float64       `json:"total_size_gb"`
        MaxTotalSizeGB float64       `json:"max_total_size_gb"`
        RetentionAge   time.Duration `json:"retention_age"`
        TotalFiles     int           `json:"total_files"`

        // Pointer fields (8 bytes)
        ShardStats map[uint32]ShardRetentionStats `json:"shard_stats,omitempty"`

        // Larger composite types last (time.Time is 24 bytes)
        OldestData time.Time `json:"oldest_data"`
        NewestData time.Time `json:"newest_data"`
}

// ShardRetentionStats provides retention stats for a single shard
// Fields ordered for optimal memory alignment
type ShardRetentionStats struct {
        // 8-byte aligned fields first
        SizeBytes int64 `json:"size_bytes"`
        Files     int   `json:"files"`

        // Pointer field (8 bytes)
        ConsumerLag map[string]int64 `json:"consumer_lag,omitempty"`

        // Larger composite types last (time.Time is 24 bytes)
        OldestEntry time.Time `json:"oldest_entry"`
        NewestEntry time.Time `json:"newest_entry"`
}

// startRetentionManager starts the background retention process
func (c *Client) startRetentionManager() <span class="cov8" title="1">{
        if c.config.Retention.CleanupInterval &lt;= 0 </span><span class="cov0" title="0">{
                // Retention disabled
                return
        }</span>

        <span class="cov8" title="1">c.retentionWg.Add(1)
        go func() </span><span class="cov8" title="1">{
                defer c.retentionWg.Done()

                ticker := time.NewTicker(c.config.Retention.CleanupInterval)
                defer ticker.Stop()

                // Run initial cleanup after short delay
                // Use cleanup interval as initial delay to avoid hardcoded 10s
                initialDelay := c.config.Retention.CleanupInterval
                if initialDelay &gt; 10*time.Second </span><span class="cov8" title="1">{
                        initialDelay = 10 * time.Second
                }</span>
                <span class="cov8" title="1">initialTimer := time.NewTimer(initialDelay)
                select </span>{
                case &lt;-initialTimer.C:<span class="cov0" title="0">
                        c.runRetentionCleanup()</span>
                case &lt;-c.stopCh:<span class="cov8" title="1">
                        initialTimer.Stop()
                        return</span>
                }

                // Run periodic cleanup
                <span class="cov0" title="0">for </span><span class="cov0" title="0">{
                        select </span>{
                        case &lt;-ticker.C:<span class="cov0" title="0">
                                c.runRetentionCleanup()</span>
                        case &lt;-c.stopCh:<span class="cov0" title="0">
                                return</span>
                        }
                }
        }()
}

// runRetentionCleanup performs a single cleanup pass
func (c *Client) runRetentionCleanup() <span class="cov0" title="0">{
        start := time.Now()

        c.mu.RLock()
        shards := make([]*Shard, 0, len(c.shards))
        for _, shard := range c.shards </span><span class="cov0" title="0">{
                shards = append(shards, shard)
                // Track retention run in each shard's metrics
                if shard.state != nil </span><span class="cov0" title="0">{
                        atomic.AddUint64(&amp;shard.state.RetentionRuns, 1)
                        atomic.StoreInt64(&amp;shard.state.LastRetentionNanos, start.UnixNano())
                }</span>
        }
        <span class="cov0" title="0">c.mu.RUnlock()

        var totalSize int64
        for _, shard := range shards </span><span class="cov0" title="0">{
                size := c.cleanupShard(shard)
                totalSize += size
        }</span>

        // Check total size limit
        <span class="cov0" title="0">if c.config.Retention.MaxTotalSize &gt; 0 &amp;&amp; totalSize &gt; c.config.Retention.MaxTotalSize </span><span class="cov0" title="0">{
                // Need to delete more files to get under total limit
                c.enforceGlobalSizeLimit(shards, totalSize)
        }</span>

        // Update retention time metrics
        <span class="cov0" title="0">duration := time.Since(start)
        for _, shard := range shards </span><span class="cov0" title="0">{
                if shard.state != nil </span><span class="cov0" title="0">{
                        atomic.AddInt64(&amp;shard.state.RetentionTimeNanos, duration.Nanoseconds())
                }</span>
        }

        // Debug log retention summary
        <span class="cov0" title="0">if Debug &amp;&amp; c.logger != nil </span><span class="cov0" title="0">{
                var totalFiles, deletedFiles int
                for _, shard := range shards </span><span class="cov0" title="0">{
                        shard.mu.RLock()
                        totalFiles += len(shard.index.Files)
                        shard.mu.RUnlock()
                        if shard.state != nil </span><span class="cov0" title="0">{
                                deletedFiles += int(atomic.LoadUint64(&amp;shard.state.FilesDeleted))
                        }</span>
                }
                <span class="cov0" title="0">c.logger.Debug("Retention cleanup completed",
                        "shards", len(shards),
                        "totalFiles", totalFiles,
                        "totalSizeMB", totalSize/(1&lt;&lt;20),
                        "duration", duration,
                        "maxAge", c.config.Retention.MaxAge,
                        "maxSizeMB", c.config.Retention.MaxTotalSize/(1&lt;&lt;20))</span>
        }
}

// cleanupShard cleans up old files in a single shard
func (c *Client) cleanupShard(shard *Shard) int64 <span class="cov0" title="0">{
        // For multi-process safety, acquire retention lock first
        if c.config.Concurrency.EnableMultiProcessMode &amp;&amp; shard.retentionLockFile != nil </span><span class="cov0" title="0">{
                // Use non-blocking try-lock to avoid hanging if another process is doing retention
                if err := syscall.Flock(int(shard.retentionLockFile.Fd()), syscall.LOCK_EX|syscall.LOCK_NB); err != nil </span><span class="cov0" title="0">{
                        // Another process is doing retention - skip this cleanup
                        // Return current shard size without cleanup
                        shard.mu.RLock()
                        var currentSize int64
                        for _, file := range shard.index.Files </span><span class="cov0" title="0">{
                                currentSize += file.EndOffset - file.StartOffset
                        }</span>
                        <span class="cov0" title="0">shard.mu.RUnlock()
                        return currentSize</span>
                }
                // Ensure we release the lock when done
                <span class="cov0" title="0">defer syscall.Flock(int(shard.retentionLockFile.Fd()), syscall.LOCK_UN)</span>
        }

        <span class="cov0" title="0">shard.mu.RLock()
        files := make([]FileInfo, len(shard.index.Files))
        copy(files, shard.index.Files)
        currentFile := shard.index.CurrentFile
        consumerOffsets := make(map[string]int64)
        maps.Copy(consumerOffsets, shard.index.ConsumerOffsets)
        shard.mu.RUnlock()

        // Calculate current state
        var shardSize int64
        var oldestProtectedEntry int64 = -1
        now := time.Now()

        // Find oldest consumer position if protecting unconsumed
        if c.config.Retention.ProtectUnconsumed </span><span class="cov0" title="0">{
                for _, offset := range consumerOffsets </span><span class="cov0" title="0">{
                        if oldestProtectedEntry == -1 || offset &lt; oldestProtectedEntry </span><span class="cov0" title="0">{
                                oldestProtectedEntry = offset
                        }</span>
                }
        }

        // Analyze files for deletion
        <span class="cov0" title="0">filesToDelete := []FileInfo{}
        filesToKeep := []FileInfo{}

        // Count total non-current files first
        totalNonCurrentFiles := 0
        for _, file := range files </span><span class="cov0" title="0">{
                if file.Path != currentFile </span><span class="cov0" title="0">{
                        totalNonCurrentFiles++
                }</span>
        }

        <span class="cov0" title="0">for i, file := range files </span><span class="cov0" title="0">{
                fileSize := file.EndOffset - file.StartOffset
                shardSize += fileSize

                // Never delete the current file
                if file.Path == currentFile </span><span class="cov0" title="0">{
                        filesToKeep = append(filesToKeep, file)
                        continue</span>
                }

                // Check if we should delete this file
                <span class="cov0" title="0">shouldDelete := false

                // Time-based deletion
                if c.config.Retention.MaxAge &gt; 0 &amp;&amp; now.Sub(file.EndTime) &gt; c.config.Retention.MaxAge </span><span class="cov0" title="0">{
                        shouldDelete = true
                }</span>

                // Force delete after time
                <span class="cov0" title="0">if c.config.Retention.ForceDeleteAfter &gt; 0 &amp;&amp; now.Sub(file.EndTime) &gt; c.config.Retention.ForceDeleteAfter </span><span class="cov0" title="0">{
                        shouldDelete = true
                        oldestProtectedEntry = -1 // Ignore consumer protection
                }</span>

                // Check if file has active readers
                <span class="cov0" title="0">if shouldDelete &amp;&amp; atomic.LoadInt64(&amp;shard.readerCount) &gt; 0 </span><span class="cov0" title="0">{
                        // Skip files that might have active readers
                        // This is conservative - we could track per-file readers for more precision
                        if i == 0 || i == len(files)-1 </span><span class="cov0" title="0">{
                                shouldDelete = false
                        }</span>
                }

                // Check consumer protection
                <span class="cov0" title="0">if shouldDelete &amp;&amp; oldestProtectedEntry &gt;= 0 </span><span class="cov0" title="0">{
                        // Check if any consumer still needs this file
                        fileLastEntry := file.StartEntry + file.Entries - 1
                        if fileLastEntry &gt;= oldestProtectedEntry </span><span class="cov0" title="0">{
                                shouldDelete = false
                                // Track files protected by consumers
                                if shard.state != nil </span><span class="cov0" title="0">{
                                        atomic.AddUint64(&amp;shard.state.ProtectedByConsumers, 1)
                                }</span>
                        }
                }

                // Enforce minimum files - check against total files, not just files already processed
                <span class="cov0" title="0">remainingFiles := totalNonCurrentFiles - len(filesToDelete)
                if shouldDelete &amp;&amp; remainingFiles &lt;= c.config.Retention.MinFilesToKeep </span><span class="cov0" title="0">{
                        shouldDelete = false
                }</span>

                <span class="cov0" title="0">if shouldDelete </span><span class="cov0" title="0">{
                        filesToDelete = append(filesToDelete, file)
                }</span> else<span class="cov0" title="0"> {
                        filesToKeep = append(filesToKeep, file)
                }</span>
        }

        // Size-based deletion (if we're over the shard limit)
        <span class="cov0" title="0">if c.config.Retention.MaxShardSize &gt; 0 &amp;&amp; shardSize &gt; c.config.Retention.MaxShardSize </span><span class="cov0" title="0">{
                // Sort kept files by age (oldest first)
                sort.Slice(filesToKeep, func(i, j int) bool </span><span class="cov0" title="0">{
                        return filesToKeep[i].EndTime.Before(filesToKeep[j].EndTime)
                }</span>)

                <span class="cov0" title="0">targetSize := shardSize
                for i := 0; i &lt; len(filesToKeep) &amp;&amp; targetSize &gt; c.config.Retention.MaxShardSize; i++ </span><span class="cov0" title="0">{
                        file := filesToKeep[i]

                        // Skip current file and minimum required files
                        if file.Path == currentFile || len(filesToKeep)-i &lt;= c.config.Retention.MinFilesToKeep </span><span class="cov0" title="0">{
                                continue</span>
                        }

                        // Move to delete list
                        <span class="cov0" title="0">filesToDelete = append(filesToDelete, file)
                        targetSize -= (file.EndOffset - file.StartOffset)

                        // Remove from keep list
                        filesToKeep = append(filesToKeep[:i], filesToKeep[i+1:]...)
                        i--</span> // Adjust index after removal
                }
        }

        // Actually delete the files
        <span class="cov0" title="0">if len(filesToDelete) &gt; 0 </span><span class="cov0" title="0">{
                c.deleteFiles(shard, filesToDelete, &amp;c.metrics)
        }</span>

        // Update oldest entry timestamp
        <span class="cov0" title="0">if shard.state != nil </span><span class="cov0" title="0">{
                if len(filesToKeep) &gt; 0 </span><span class="cov0" title="0">{
                        // Find the oldest file that remains
                        oldestTime := filesToKeep[0].StartTime
                        for _, file := range filesToKeep </span><span class="cov0" title="0">{
                                if file.StartTime.Before(oldestTime) &amp;&amp; !file.StartTime.IsZero() </span><span class="cov0" title="0">{
                                        oldestTime = file.StartTime
                                }</span>
                        }
                        // Only set if we found a valid timestamp
                        <span class="cov0" title="0">if !oldestTime.IsZero() </span><span class="cov0" title="0">{
                                atomic.StoreInt64(&amp;shard.state.OldestEntryNanos, oldestTime.UnixNano())
                        }</span>
                } else<span class="cov0" title="0"> if len(shard.index.Files) &gt; 0 </span><span class="cov0" title="0">{
                        // If we're keeping all files (no deletion), still update the metric
                        oldestTime := time.Time{}
                        for _, file := range shard.index.Files </span><span class="cov0" title="0">{
                                if !file.StartTime.IsZero() &amp;&amp; (oldestTime.IsZero() || file.StartTime.Before(oldestTime)) </span><span class="cov0" title="0">{
                                        oldestTime = file.StartTime
                                }</span>
                        }
                        // Only set if we found a valid timestamp
                        <span class="cov0" title="0">if !oldestTime.IsZero() </span><span class="cov0" title="0">{
                                atomic.StoreInt64(&amp;shard.state.OldestEntryNanos, oldestTime.UnixNano())
                        }</span>
                }
        }

        // Return remaining size
        <span class="cov0" title="0">var remainingSize int64
        for _, file := range filesToKeep </span><span class="cov0" title="0">{
                remainingSize += file.EndOffset - file.StartOffset
        }</span>
        <span class="cov0" title="0">return remainingSize</span>
}

// deleteFiles removes files from disk and updates the shard index
// CRITICAL: Updates index BEFORE deleting files to prevent readers from accessing deleted files
func (c *Client) deleteFiles(shard *Shard, files []FileInfo, metrics *ClientMetrics) <span class="cov0" title="0">{
        if len(files) == 0 </span><span class="cov0" title="0">{
                return
        }</span>

        // STEP 1: Update the shard index FIRST to prevent readers from accessing files we're about to delete
        // For multi-process safety, use the index lock
        <span class="cov0" title="0">if c.config.Concurrency.EnableMultiProcessMode &amp;&amp; shard.indexLockFile != nil </span><span class="cov0" title="0">{
                // Acquire exclusive lock for index writes
                if err := syscall.Flock(int(shard.indexLockFile.Fd()), syscall.LOCK_EX); err != nil </span><span class="cov0" title="0">{
                        // Failed to acquire index lock - skip deletion to avoid corruption
                        return
                }</span>
                <span class="cov0" title="0">defer syscall.Flock(int(shard.indexLockFile.Fd()), syscall.LOCK_UN)</span>
        }

        <span class="cov0" title="0">shard.mu.Lock()

        // Create a map of files to delete for quick lookup
        deletedMap := make(map[string]bool)
        for _, file := range files </span><span class="cov0" title="0">{
                deletedMap[file.Path] = true
        }</span>

        // Filter out files to be deleted from the index
        <span class="cov0" title="0">newFiles := make([]FileInfo, 0, len(shard.index.Files))
        for _, file := range shard.index.Files </span><span class="cov0" title="0">{
                if !deletedMap[file.Path] </span><span class="cov0" title="0">{
                        newFiles = append(newFiles, file)
                }</span>
        }

        <span class="cov0" title="0">shard.index.Files = newFiles

        // Clean up entry boundaries for deleted files
        // Clean up binary index to remove entries that reference deleted files
        // Must be done while holding the lock!
        if len(files) &gt; 0 </span><span class="cov0" title="0">{
                minDeletedEntry := files[0].StartEntry

                // Filter binary index nodes
                newNodes := make([]EntryIndexNode, 0)
                for _, node := range shard.index.BinaryIndex.Nodes </span><span class="cov0" title="0">{
                        if node.EntryNumber &lt; minDeletedEntry </span><span class="cov0" title="0">{
                                newNodes = append(newNodes, node)
                        }</span>
                }
                <span class="cov0" title="0">shard.index.BinaryIndex.Nodes = newNodes</span>
        }

        // Persist the updated index while still holding the lock
        <span class="cov0" title="0">shard.persistIndex()

        // Now we can release the lock - all index modifications are complete
        shard.mu.Unlock()

        // STEP 2: Now delete the physical files - readers can no longer find them in the index
        deletedCount := 0
        var bytesReclaimed uint64
        var entriesDeleted uint64

        for _, file := range files </span><span class="cov0" title="0">{
                err := os.Remove(file.Path)
                if err != nil &amp;&amp; !os.IsNotExist(err) </span><span class="cov0" title="0">{
                        // Log error but continue - file may have been deleted by another process
                        // Track retention errors
                        if shard.state != nil </span><span class="cov0" title="0">{
                                atomic.AddUint64(&amp;shard.state.RetentionErrors, 1)
                        }</span>
                        <span class="cov0" title="0">continue</span>
                }
                <span class="cov0" title="0">deletedCount++
                bytesReclaimed += uint64(file.EndOffset - file.StartOffset)
                entriesDeleted += uint64(file.Entries)</span>
        }

        // Update retention metrics
        <span class="cov0" title="0">if shard.state != nil &amp;&amp; deletedCount &gt; 0 </span><span class="cov0" title="0">{
                atomic.AddUint64(&amp;shard.state.FilesDeleted, uint64(deletedCount))
                atomic.AddUint64(&amp;shard.state.BytesReclaimed, bytesReclaimed)
                atomic.AddUint64(&amp;shard.state.EntriesDeleted, entriesDeleted)
        }</span>
}

// enforceGlobalSizeLimit deletes files across all shards to meet total size limit
func (c *Client) enforceGlobalSizeLimit(shards []*Shard, currentTotal int64) <span class="cov0" title="0">{
        if currentTotal &lt;= c.config.Retention.MaxTotalSize </span><span class="cov0" title="0">{
                return
        }</span>

        // Collect all files from all shards with their metadata
        <span class="cov0" title="0">type FileWithShard struct {
                shard *Shard
                file  FileInfo
        }

        var allFiles []FileWithShard
        for _, shard := range shards </span><span class="cov0" title="0">{
                shard.mu.RLock()
                for _, file := range shard.index.Files </span><span class="cov0" title="0">{
                        if file.Path != shard.index.CurrentFile </span><span class="cov0" title="0">{
                                allFiles = append(allFiles, FileWithShard{shard: shard, file: file})
                        }</span>
                }
                <span class="cov0" title="0">shard.mu.RUnlock()</span>
        }

        // Sort by age (oldest first)
        <span class="cov0" title="0">sort.Slice(allFiles, func(i, j int) bool </span><span class="cov0" title="0">{
                return allFiles[i].file.EndTime.Before(allFiles[j].file.EndTime)
        }</span>)

        // Delete oldest files until we're under the limit
        <span class="cov0" title="0">bytesToDelete := currentTotal - c.config.Retention.MaxTotalSize
        deletionMap := make(map[*Shard][]FileInfo)

        for _, fw := range allFiles </span><span class="cov0" title="0">{
                if bytesToDelete &lt;= 0 </span><span class="cov0" title="0">{
                        break</span>
                }

                <span class="cov0" title="0">fileSize := fw.file.EndOffset - fw.file.StartOffset
                deletionMap[fw.shard] = append(deletionMap[fw.shard], fw.file)
                bytesToDelete -= fileSize</span>
        }

        // Perform deletions
        <span class="cov0" title="0">for shard, files := range deletionMap </span><span class="cov0" title="0">{
                c.deleteFiles(shard, files, &amp;c.metrics)
        }</span>
}

// ForceRetentionCleanup forces an immediate retention cleanup pass
// This is primarily useful for testing retention behavior
func (c *Client) ForceRetentionCleanup() <span class="cov0" title="0">{
        if c.config.Retention.CleanupInterval &lt;= 0 </span><span class="cov0" title="0">{
                // Retention is disabled
                return
        }</span>
        <span class="cov0" title="0">c.runRetentionCleanup()</span>
}

// GetRetentionStats returns current retention statistics
func (c *Client) GetRetentionStats() *RetentionStats <span class="cov0" title="0">{
        stats := &amp;RetentionStats{
                RetentionAge:   c.config.Retention.MaxAge,
                MaxTotalSizeGB: float64(c.config.Retention.MaxTotalSize) / (1 &lt;&lt; 30),
                ShardStats:     make(map[uint32]ShardRetentionStats),
        }

        c.mu.RLock()
        defer c.mu.RUnlock()

        for shardID, shard := range c.shards </span><span class="cov0" title="0">{
                shard.mu.RLock()

                shardStat := ShardRetentionStats{
                        Files:       len(shard.index.Files),
                        ConsumerLag: make(map[string]int64),
                }

                // Calculate shard size and time range
                for _, file := range shard.index.Files </span><span class="cov0" title="0">{
                        size := file.EndOffset - file.StartOffset
                        shardStat.SizeBytes += size
                        stats.TotalSizeBytes += size

                        if shardStat.OldestEntry.IsZero() || file.StartTime.Before(shardStat.OldestEntry) </span><span class="cov0" title="0">{
                                shardStat.OldestEntry = file.StartTime
                        }</span>
                        // For the current file, use current time as end time
                        <span class="cov0" title="0">endTime := file.EndTime
                        if file.Path == shard.index.CurrentFile &amp;&amp; (endTime.IsZero() || endTime.Before(file.StartTime)) </span><span class="cov0" title="0">{
                                endTime = time.Now()
                        }</span>

                        <span class="cov0" title="0">if endTime.After(shardStat.NewestEntry) </span><span class="cov0" title="0">{
                                shardStat.NewestEntry = endTime
                        }</span>

                        // Update global oldest/newest
                        <span class="cov0" title="0">if stats.OldestData.IsZero() || file.StartTime.Before(stats.OldestData) </span><span class="cov0" title="0">{
                                stats.OldestData = file.StartTime
                        }</span>
                        <span class="cov0" title="0">if endTime.After(stats.NewestData) </span><span class="cov0" title="0">{
                                stats.NewestData = endTime
                        }</span>
                }

                // Calculate consumer lag for this shard
                <span class="cov0" title="0">for group, offset := range shard.index.ConsumerOffsets </span><span class="cov0" title="0">{
                        lag := shard.index.CurrentEntryNumber - offset
                        if lag &gt; 0 </span><span class="cov0" title="0">{
                                shardStat.ConsumerLag[group] = lag
                        }</span>
                }

                <span class="cov0" title="0">stats.TotalFiles += shardStat.Files
                stats.ShardStats[shardID] = shardStat

                shard.mu.RUnlock()</span>
        }

        <span class="cov0" title="0">stats.TotalSizeGB = float64(stats.TotalSizeBytes) / (1 &lt;&lt; 30)

        return stats</span>
}
</pre>
		
		<pre class="file" id="file11" style="display: none">package comet

import (
        "fmt"
        "sync/atomic"
        "unsafe"
)

// CometState consolidates ALL mmap state and comprehensive metrics
// Stored in: comet.state (1KB file per shard)
// Total size: 1024 bytes (1KB) for plenty of room to grow
//
// Design principles:
// 1. Group related metrics in same cache line
// 2. Hot path metrics in early cache lines
// 3. Generous reserved space for future additions
// 4. All fields are atomic-safe for multi-process access
// 5. CRITICAL: Use raw int64/uint64 fields with atomic operations, NOT atomic.Int64
type CometState struct {
        // ======== Header (0-63): Version and core state ========
        Version          uint64  // 0-7:   Format version (start with 1)
        WriteOffset      uint64  // 8-15:  Current write position in active file
        LastEntryNumber  int64   // 16-23: Sequence counter for entry IDs
        LastIndexUpdate  int64   // 24-31: Timestamp of last index modification
        ActiveFileIndex  uint64  // 32-39: Current file being written to
        FileSize         uint64  // 40-47: Current size of active file
        LastFileSequence uint64  // 48-55: Sequence counter for file naming
        _pad0            [8]byte // 56-63: Padding

        // ======== Cache Line 1 (64-127): Write metrics (hot path) ========
        TotalEntries     int64   // 64-71:  Total entries written
        TotalBytes       uint64  // 72-79:  Total uncompressed bytes
        TotalWrites      uint64  // 80-87:  Total write operations
        LastWriteNanos   int64   // 88-95:  Timestamp of last write
        CurrentBatchSize uint64  // 96-103: Current batch being built
        TotalBatches     uint64  // 104-111: Total batches written
        FailedWrites     uint64  // 112-119: Write failures
        _pad1            [8]byte // 120-127: Padding

        // ======== Cache Line 2 (128-191): Compression metrics ========
        TotalCompressed      uint64  // 128-135: Total compressed bytes
        CompressedEntries    uint64  // 136-143: Number of compressed entries
        SkippedCompression   uint64  // 144-151: Entries too small to compress
        CompressionRatio     uint64  // 152-159: Average ratio * 100
        CompressionTimeNanos int64   // 160-167: Total time compressing
        BestCompression      uint64  // 168-175: Best ratio seen * 100
        WorstCompression     uint64  // 176-183: Worst ratio seen * 100
        _pad2                [8]byte // 184-191: Padding

        // ======== Cache Line 3 (192-255): Latency metrics ========
        WriteLatencySum   uint64  // 192-199: Sum for averaging
        WriteLatencyCount uint64  // 200-207: Count for averaging
        MinWriteLatency   uint64  // 208-215: Minimum seen
        MaxWriteLatency   uint64  // 216-223: Maximum seen
        P50WriteLatency   uint64  // 224-231: Median estimate
        P99WriteLatency   uint64  // 232-239: 99th percentile estimate
        SyncLatencyNanos  int64   // 240-247: Time spent in fsync
        _pad3             [8]byte // 248-255: Padding

        // ======== Cache Line 4 (256-319): File operation metrics ========
        FilesCreated      uint64  // 256-263: Total files created
        FilesDeleted      uint64  // 264-271: Files removed by retention
        FileRotations     uint64  // 272-279: Successful rotations
        RotationTimeNanos int64   // 280-287: Time spent rotating
        CurrentFiles      uint64  // 288-295: Current file count
        TotalFileBytes    uint64  // 296-303: Total size on disk
        FailedRotations   uint64  // 304-311: Rotation failures
        _pad4             [8]byte // 312-319: Padding

        // ======== Cache Line 5 (320-383): Checkpoint/Index metrics ========
        CheckpointCount     uint64  // 320-327: Total checkpoints
        LastCheckpointNanos int64   // 328-335: Last checkpoint time
        CheckpointTimeNanos int64   // 336-343: Total checkpoint time
        IndexPersistCount   uint64  // 344-351: Index saves
        IndexPersistErrors  uint64  // 352-359: Failed index saves
        IndexSizeBytes      uint64  // 360-367: Current index size
        BinaryIndexNodes    uint64  // 368-375: Nodes in binary index
        _pad5               [8]byte // 376-383: Padding

        // ======== Cache Line 6 (384-447): Consumer metrics ========
        ActiveReaders    uint64  // 384-391: Current reader count
        TotalReaders     uint64  // 392-399: Total readers created
        MaxConsumerLag   uint64  // 400-407: Max entries behind
        TotalEntriesRead uint64  // 408-415: Total read operations
        ConsumerGroups   uint64  // 416-423: Active consumer groups
        AckedEntries     uint64  // 424-431: Acknowledged entries
        ReaderCacheHits  uint64  // 432-439: Cache hit count
        _pad6            [8]byte // 440-447: Padding

        // ======== Cache Line 7 (448-511): Error/Recovery metrics ========
        ErrorCount         uint64  // 448-455: Total errors
        LastErrorNanos     int64   // 456-463: Last error timestamp
        CorruptionDetected uint64  // 464-471: Corrupted entries found
        RecoveryAttempts   uint64  // 472-479: Auto-recovery attempts
        RecoverySuccesses  uint64  // 480-487: Successful recoveries
        PartialWrites      uint64  // 488-495: Incomplete write detected
        ReadErrors         uint64  // 496-503: Read failures
        _pad7              [8]byte // 504-511: Padding

        // ======== Cache Lines 8-9 (512-639): Retention metrics ========
        RetentionRuns        uint64   // 512-519: Cleanup executions
        LastRetentionNanos   int64    // 520-527: Last cleanup time
        RetentionTimeNanos   int64    // 528-535: Total cleanup time
        EntriesDeleted       uint64   // 536-543: Entries removed
        BytesReclaimed       uint64   // 544-551: Space freed
        OldestEntryNanos     int64    // 552-559: Oldest data timestamp
        RetentionErrors      uint64   // 560-567: Cleanup failures
        ProtectedByConsumers uint64   // 568-575: Files kept for consumers
        _pad8                [64]byte // 576-639: Full line padding

        // ======== Cache Lines 10-11 (640-767): Multi-process coordination ========
        ProcessCount         uint64   // 640-647: Active processes
        LastProcessHeartbeat int64    // 648-655: Latest heartbeat
        ContentionCount      uint64   // 656-663: Lock contentions
        LockWaitNanos        int64    // 664-671: Time waiting for locks
        MMAPRemapCount       uint64   // 672-679: File remappings
        FalseShareCount      uint64   // 680-687: Detected false sharing
        _pad9                [80]byte // 688-767: Padding

        // ======== Cache Lines 12-15 (768-1023): Reserved for future ========
        _reserved [256]byte // 768-1023: Future expansion space
}

// Compile-time checks
const stateSize = unsafe.Sizeof(CometState{})

func init() <span class="cov8" title="1">{
        if stateSize != CometStateSize </span><span class="cov0" title="0">{
                panic(fmt.Sprintf("CometState must be exactly %d bytes, got %d", CometStateSize, stateSize))</span>
        }
        <span class="cov8" title="1">if stateSize%64 != 0 </span><span class="cov0" title="0">{
                panic("CometState must be 64-byte aligned")</span>
        }
}

// Helper methods for non-atomic version field
func (s *CometState) GetVersion() uint64 <span class="cov0" title="0">{
        return atomic.LoadUint64(&amp;s.Version)
}</span>

func (s *CometState) SetVersion(v uint64) <span class="cov0" title="0">{
        atomic.StoreUint64(&amp;s.Version, v)
}</span>

// Helper methods for atomic operations on uint64 fields
func (s *CometState) GetLastEntryNumber() int64 <span class="cov0" title="0">{
        val := atomic.LoadInt64(&amp;s.LastEntryNumber)
        if Debug </span><span class="cov0" title="0">{
                fmt.Printf("DEBUG GetLastEntryNumber: val=%d, ptr=%p, structPtr=%p\n", val, &amp;s.LastEntryNumber, s)
        }</span>
        <span class="cov0" title="0">return val</span>
}

func (s *CometState) IncrementLastEntryNumber() int64 <span class="cov0" title="0">{
        oldVal := atomic.LoadInt64(&amp;s.LastEntryNumber)
        newVal := atomic.AddInt64(&amp;s.LastEntryNumber, 1)
        afterVal := atomic.LoadInt64(&amp;s.LastEntryNumber)

        if Debug </span><span class="cov0" title="0">{
                fmt.Printf("DEBUG IncrementLastEntryNumber: old=%d, returned=%d, after=%d, ptr=%p, fieldPtr=%p\n",
                        oldVal, newVal, afterVal, s, &amp;s.LastEntryNumber)
        }</span>
        <span class="cov0" title="0">return newVal</span>
}

func (s *CometState) GetLastIndexUpdate() int64 <span class="cov0" title="0">{
        return atomic.LoadInt64(&amp;s.LastIndexUpdate)
}</span>

func (s *CometState) SetLastIndexUpdate(nanos int64) <span class="cov0" title="0">{
        atomic.StoreInt64(&amp;s.LastIndexUpdate, nanos)
}</span>

// WriteOffset methods
func (s *CometState) GetWriteOffset() uint64 <span class="cov0" title="0">{
        return atomic.LoadUint64(&amp;s.WriteOffset)
}</span>

func (s *CometState) AddWriteOffset(delta uint64) uint64 <span class="cov0" title="0">{
        return atomic.AddUint64(&amp;s.WriteOffset, delta)
}</span>

func (s *CometState) StoreWriteOffset(val uint64) <span class="cov0" title="0">{
        atomic.StoreUint64(&amp;s.WriteOffset, val)
}</span>

// ActiveFileIndex methods
func (s *CometState) GetActiveFileIndex() uint64 <span class="cov0" title="0">{
        return atomic.LoadUint64(&amp;s.ActiveFileIndex)
}</span>

func (s *CometState) AddActiveFileIndex(delta uint64) uint64 <span class="cov0" title="0">{
        return atomic.AddUint64(&amp;s.ActiveFileIndex, delta)
}</span>

func (s *CometState) StoreActiveFileIndex(val uint64) <span class="cov0" title="0">{
        atomic.StoreUint64(&amp;s.ActiveFileIndex, val)
}</span>

// FileSize methods
func (s *CometState) GetFileSize() uint64 <span class="cov0" title="0">{
        return atomic.LoadUint64(&amp;s.FileSize)
}</span>

func (s *CometState) StoreFileSize(val uint64) <span class="cov0" title="0">{
        atomic.StoreUint64(&amp;s.FileSize, val)
}</span>

// LastFileSequence methods
func (s *CometState) AddLastFileSequence(delta uint64) uint64 <span class="cov8" title="1">{
        return atomic.AddUint64(&amp;s.LastFileSequence, delta)
}</span>

// TotalWrites methods
func (s *CometState) GetTotalWrites() uint64 <span class="cov0" title="0">{
        return atomic.LoadUint64(&amp;s.TotalWrites)
}</span>

func (s *CometState) AddTotalWrites(delta uint64) uint64 <span class="cov0" title="0">{
        return atomic.AddUint64(&amp;s.TotalWrites, delta)
}</span>

// LastWriteNanos methods
func (s *CometState) GetLastWriteNanos() int64 <span class="cov0" title="0">{
        return atomic.LoadInt64(&amp;s.LastWriteNanos)
}</span>

func (s *CometState) StoreLastWriteNanos(val int64) <span class="cov0" title="0">{
        atomic.StoreInt64(&amp;s.LastWriteNanos, val)
}</span>

// TotalEntries methods
func (s *CometState) AddTotalEntries(delta int64) int64 <span class="cov0" title="0">{
        return atomic.AddInt64(&amp;s.TotalEntries, delta)
}</span>

// TotalBytes methods
func (s *CometState) AddTotalBytes(delta uint64) uint64 <span class="cov0" title="0">{
        return atomic.AddUint64(&amp;s.TotalBytes, delta)
}</span>

// FileRotations methods
func (s *CometState) AddFileRotations(delta uint64) uint64 <span class="cov0" title="0">{
        return atomic.AddUint64(&amp;s.FileRotations, delta)
}</span>

// FilesCreated methods
func (s *CometState) AddFilesCreated(delta uint64) uint64 <span class="cov0" title="0">{
        return atomic.AddUint64(&amp;s.FilesCreated, delta)
}</span>

// MinWriteLatency methods
func (s *CometState) GetMinWriteLatency() uint64 <span class="cov0" title="0">{
        return atomic.LoadUint64(&amp;s.MinWriteLatency)
}</span>

func (s *CometState) CompareAndSwapMinWriteLatency(old, new uint64) bool <span class="cov0" title="0">{
        return atomic.CompareAndSwapUint64(&amp;s.MinWriteLatency, old, new)
}</span>

// MaxWriteLatency methods
func (s *CometState) GetMaxWriteLatency() uint64 <span class="cov0" title="0">{
        return atomic.LoadUint64(&amp;s.MaxWriteLatency)
}</span>

func (s *CometState) CompareAndSwapMaxWriteLatency(old, new uint64) bool <span class="cov0" title="0">{
        return atomic.CompareAndSwapUint64(&amp;s.MaxWriteLatency, old, new)
}</span>

// WriteLatencySum methods
func (s *CometState) AddWriteLatencySum(delta uint64) uint64 <span class="cov0" title="0">{
        return atomic.AddUint64(&amp;s.WriteLatencySum, delta)
}</span>

// WriteLatencyCount methods
func (s *CometState) AddWriteLatencyCount(delta uint64) uint64 <span class="cov0" title="0">{
        return atomic.AddUint64(&amp;s.WriteLatencyCount, delta)
}</span>

// Computed metrics helpers
func (s *CometState) GetAverageWriteLatency() uint64 <span class="cov0" title="0">{
        count := atomic.LoadUint64(&amp;s.WriteLatencyCount)
        if count == 0 </span><span class="cov0" title="0">{
                return 0
        }</span>
        <span class="cov0" title="0">return atomic.LoadUint64(&amp;s.WriteLatencySum) / count</span>
}

func (s *CometState) GetCompressionRatioFloat() float64 <span class="cov0" title="0">{
        compressed := atomic.LoadUint64(&amp;s.TotalCompressed)
        original := atomic.LoadUint64(&amp;s.TotalBytes)
        if original == 0 </span><span class="cov0" title="0">{
                return 1.0
        }</span>
        <span class="cov0" title="0">return float64(compressed) / float64(original)</span>
}

func (s *CometState) GetErrorRate() float64 <span class="cov0" title="0">{
        errors := atomic.LoadUint64(&amp;s.ErrorCount)
        writes := atomic.LoadUint64(&amp;s.TotalWrites)
        if writes == 0 </span><span class="cov0" title="0">{
                return 0.0
        }</span>
        <span class="cov0" title="0">return float64(errors) / float64(writes)</span>
}

// UpdateWriteLatency updates latency metrics with a new sample
// Note: nanos should be uint64 since latencies are always positive
func (s *CometState) UpdateWriteLatency(nanos uint64) <span class="cov8" title="1">{
        atomic.AddUint64(&amp;s.WriteLatencySum, nanos)
        atomic.AddUint64(&amp;s.WriteLatencyCount, 1)

        // Update min
        for </span><span class="cov8" title="1">{
                min := atomic.LoadUint64(&amp;s.MinWriteLatency)
                if min &gt; 0 &amp;&amp; min &lt;= nanos </span><span class="cov8" title="1">{
                        break</span>
                }
                <span class="cov8" title="1">if atomic.CompareAndSwapUint64(&amp;s.MinWriteLatency, min, nanos) </span><span class="cov8" title="1">{
                        break</span>
                }
        }

        // Update max
        <span class="cov8" title="1">for </span><span class="cov8" title="1">{
                max := atomic.LoadUint64(&amp;s.MaxWriteLatency)
                if max &gt;= nanos </span><span class="cov8" title="1">{
                        break</span>
                }
                <span class="cov8" title="1">if atomic.CompareAndSwapUint64(&amp;s.MaxWriteLatency, max, nanos) </span><span class="cov8" title="1">{
                        break</span>
                }
        }

        // Update approximate percentiles using exponential weighted moving average
        // This is a simple approximation - for accurate percentiles, use a histogram
        // Note: This uses integer arithmetic to avoid floating point in atomic operations

        // Update P50 (median approximation)
        <span class="cov8" title="1">currentP50 := atomic.LoadUint64(&amp;s.P50WriteLatency)
        if currentP50 == 0 </span><span class="cov8" title="1">{
                // Initialize with first value
                atomic.StoreUint64(&amp;s.P50WriteLatency, nanos)
        }</span> else<span class="cov8" title="1"> {
                // EWMA with alpha=0.05: new = 0.05 * current + 0.95 * old
                // Using integer math: new = (5 * current + 95 * old) / 100
                newP50 := (5*nanos + 95*currentP50) / 100
                atomic.StoreUint64(&amp;s.P50WriteLatency, newP50)
        }</span>

        // Update P99 (99th percentile approximation)
        // For P99, we maintain a high watermark that decays slowly
        <span class="cov8" title="1">currentP99 := atomic.LoadUint64(&amp;s.P99WriteLatency)
        if currentP99 == 0 </span><span class="cov8" title="1">{
                // Initialize with first value, but higher than P50
                initialP99 := nanos * 2 // Start with 2x the first value
                if initialP99 &lt; nanos </span><span class="cov0" title="0">{
                        initialP99 = nanos // Handle overflow
                }</span>
                <span class="cov8" title="1">atomic.StoreUint64(&amp;s.P99WriteLatency, initialP99)</span>
        } else<span class="cov8" title="1"> {
                // Always ensure P99 &gt;= P50
                currentP50Again := atomic.LoadUint64(&amp;s.P50WriteLatency)

                if nanos &gt; currentP99 </span><span class="cov8" title="1">{
                        // New high value - this becomes our new P99
                        atomic.StoreUint64(&amp;s.P99WriteLatency, nanos)
                }</span> else<span class="cov8" title="1"> {
                        // Decay P99 very slowly (0.999 factor)
                        newP99 := (currentP99 * 999) / 1000

                        // But ensure P99 never goes below P50
                        if newP99 &lt; currentP50Again </span><span class="cov0" title="0">{
                                newP99 = currentP50Again * 2  // Keep P99 at least 2x P50
                                if newP99 &lt; currentP50Again </span><span class="cov0" title="0">{ // Handle overflow
                                        newP99 = currentP50Again
                                }</span>
                        }

                        <span class="cov8" title="1">atomic.StoreUint64(&amp;s.P99WriteLatency, newP99)</span>
                }
        }
}

// Constants for the unified state
const (
        CometStateVersion1 = 1
        CometStateSize     = 1024
)
</pre>
		
		<pre class="file" id="file12" style="display: none">package comet

import (
        "fmt"
        "os"
        "path/filepath"
        "sync/atomic"
        "syscall"
        "time"
)

// validateAndRecoverState validates the state file and recovers from corruption
func (s *Shard) validateAndRecoverState() error <span class="cov0" title="0">{
        if s.state == nil </span><span class="cov0" title="0">{
                return fmt.Errorf("state is nil")
        }</span>

        // Check version
        <span class="cov0" title="0">version := atomic.LoadUint64(&amp;s.state.Version)
        if version == 0 || version &gt; CometStateVersion1 </span><span class="cov0" title="0">{
                // Version 0 means uninitialized or corrupted
                // Version &gt; current means newer format we don't understand
                return s.recoverCorruptedState(fmt.Sprintf("invalid state version: %d", version))
        }</span>

        // Validate critical fields for sanity
        <span class="cov0" title="0">writeOffset := atomic.LoadUint64(&amp;s.state.WriteOffset)
        fileSize := atomic.LoadUint64(&amp;s.state.FileSize)
        lastEntry := atomic.LoadInt64(&amp;s.state.LastEntryNumber)

        // Basic sanity checks
        if writeOffset &gt; fileSize &amp;&amp; fileSize &gt; 0 </span><span class="cov0" title="0">{
                return s.recoverCorruptedState(fmt.Sprintf("write offset (%d) exceeds file size (%d)", writeOffset, fileSize))
        }</span>

        // Check for impossible values that indicate corruption
        <span class="cov0" title="0">if writeOffset &gt; 1&lt;&lt;40 </span><span class="cov0" title="0">{ // 1TB - unreasonably large
                return s.recoverCorruptedState(fmt.Sprintf("write offset unreasonably large: %d", writeOffset))
        }</span>

        // LastEntryNumber should be -1 (uninitialized) or &gt;= 0
        <span class="cov0" title="0">if lastEntry &lt; -1 </span><span class="cov0" title="0">{
                return s.recoverCorruptedState(fmt.Sprintf("invalid last entry number: %d", lastEntry))
        }</span>

        // Validate metrics are within reasonable bounds
        <span class="cov0" title="0">totalWrites := atomic.LoadUint64(&amp;s.state.TotalWrites)
        totalEntries := atomic.LoadInt64(&amp;s.state.TotalEntries)

        // TotalWrites should be &gt;= TotalEntries (can have failed writes)
        if totalEntries &gt; 0 &amp;&amp; totalWrites == 0 </span>{<span class="cov0" title="0">
                // This is suspicious but not necessarily corruption
                // Log it but don't recover
        }</span>

        <span class="cov0" title="0">return nil</span>
}

// recoverCorruptedState handles corrupted state by resetting to safe defaults
func (s *Shard) recoverCorruptedState(reason string) error <span class="cov0" title="0">{
        // Log the corruption
        if s.logger != nil </span><span class="cov0" title="0">{
                s.logger.Warn("Corrupted state detected, recovering...",
                        "shard", s.shardID,
                        "indexPath", s.indexPath,
                        "reason", reason)
        }</span>

        // Store recovery counts to restore after reinit
        <span class="cov0" title="0">var prevRecoveryAttempts uint64
        var prevCorruptionDetected uint64
        if s.state != nil </span><span class="cov0" title="0">{
                prevRecoveryAttempts = atomic.LoadUint64(&amp;s.state.RecoveryAttempts)
                prevCorruptionDetected = atomic.LoadUint64(&amp;s.state.CorruptionDetected)
        }</span>

        // Close and unmap current state
        <span class="cov0" title="0">if s.state != nil &amp;&amp; s.stateData != nil </span><span class="cov0" title="0">{
                // In mmap mode, unmap the memory
                if len(s.stateData) &gt; 0 </span><span class="cov0" title="0">{
                        syscall.Munmap(s.stateData)
                        s.stateData = nil
                }</span>
                <span class="cov0" title="0">s.state = nil</span>
        }

        // Rename corrupted file for investigation
        <span class="cov0" title="0">shardDir := filepath.Dir(s.indexPath)
        statePath := shardDir + "/comet.state"
        backupPath := fmt.Sprintf("%s.corrupted.%d", statePath, time.Now().Unix())
        os.Rename(statePath, backupPath)

        // Reinitialize state from scratch
        // Determine if we're in multi-process mode based on statePath
        multiProcessMode := s.statePath != ""
        if err := s.initCometState(multiProcessMode); err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to reinitialize state after corruption: %w", err)
        }</span>

        // Try to recover some information from the index
        <span class="cov0" title="0">s.mu.Lock()
        defer s.mu.Unlock()

        if s.index != nil &amp;&amp; s.state != nil </span><span class="cov0" title="0">{
                // Restore what we can from the index
                atomic.StoreInt64(&amp;s.state.LastEntryNumber, s.index.CurrentEntryNumber)
                atomic.StoreUint64(&amp;s.state.WriteOffset, uint64(s.index.CurrentWriteOffset))
                atomic.StoreUint64(&amp;s.state.CurrentFiles, uint64(len(s.index.Files)))

                // Set file index based on current file
                if s.index.CurrentFile != "" </span><span class="cov0" title="0">{
                        // Extract file index from filename
                        base := filepath.Base(s.index.CurrentFile)
                        var fileIndex uint64
                        if _, err := fmt.Sscanf(base, "log-%016d.comet", &amp;fileIndex); err == nil </span><span class="cov0" title="0">{
                                atomic.StoreUint64(&amp;s.state.ActiveFileIndex, fileIndex)
                        }</span>
                }

                // Restore and increment recovery metrics
                <span class="cov0" title="0">atomic.StoreUint64(&amp;s.state.RecoveryAttempts, prevRecoveryAttempts+1)
                atomic.StoreUint64(&amp;s.state.CorruptionDetected, prevCorruptionDetected+1)
                atomic.AddUint64(&amp;s.state.RecoverySuccesses, 1)</span>
        }

        <span class="cov0" title="0">return nil</span>
}

// migrateStateVersion handles upgrading state from older versions
func (s *Shard) migrateStateVersion(fromVersion, toVersion uint64) error <span class="cov0" title="0">{
        // For now, we only have version 1
        // Future versions would add migration logic here

        switch fromVersion </span>{
        case 0:<span class="cov0" title="0">
                // Version 0 -&gt; 1: Initialize all fields to defaults
                return s.recoverCorruptedState("migrating from version 0")</span>
        default:<span class="cov0" title="0">
                return fmt.Errorf("unknown state version for migration: %d", fromVersion)</span>
        }
}
</pre>
		
		<pre class="file" id="file13" style="display: none">package comet

import (
        "bytes"
        "context"
        "fmt"
        "strings"
        "testing"
)

// TestLogger captures log output for testing
type TestLogger struct {
        t      *testing.T
        buffer *bytes.Buffer
        level  LogLevel
        fields []interface{}
}

// NewTestLogger creates a logger that captures output for testing
func NewTestLogger(t *testing.T, level LogLevel) *TestLogger <span class="cov0" title="0">{
        return &amp;TestLogger{
                t:      t,
                buffer: &amp;bytes.Buffer{},
                level:  level,
        }
}</span>

func (tl *TestLogger) log(level LogLevel, levelStr, msg string, keysAndValues ...interface{}) <span class="cov0" title="0">{
        if level &lt; tl.level </span><span class="cov0" title="0">{
                return
        }</span>

        // Combine fields with keysAndValues
        <span class="cov0" title="0">allFields := append(tl.fields, keysAndValues...)

        // Log to test output
        tl.t.Logf("[%s] %s %v", levelStr, msg, allFields)

        // Also capture in buffer for assertions
        output := "[" + levelStr + "] " + msg
        if len(allFields) &gt; 0 </span><span class="cov0" title="0">{
                output += " " + formatFields(allFields...)
        }</span>
        <span class="cov0" title="0">tl.buffer.WriteString(output + "\n")</span>
}

func formatFields(keysAndValues ...interface{}) string <span class="cov0" title="0">{
        var parts []string
        for i := 0; i &lt; len(keysAndValues); i += 2 </span><span class="cov0" title="0">{
                if i+1 &lt; len(keysAndValues) </span><span class="cov0" title="0">{
                        parts = append(parts, fmt.Sprintf("%v=%v", keysAndValues[i], keysAndValues[i+1]))
                }</span> else<span class="cov0" title="0"> {
                        parts = append(parts, fmt.Sprintf("%v=&lt;missing&gt;", keysAndValues[i]))
                }</span>
        }
        <span class="cov0" title="0">return "{" + strings.Join(parts, ", ") + "}"</span>
}

func (tl *TestLogger) Debug(msg string, keysAndValues ...interface{}) <span class="cov0" title="0">{
        tl.log(LogLevelDebug, "DEBUG", msg, keysAndValues...)
}</span>

func (tl *TestLogger) Info(msg string, keysAndValues ...interface{}) <span class="cov0" title="0">{
        tl.log(LogLevelInfo, "INFO", msg, keysAndValues...)
}</span>

func (tl *TestLogger) Warn(msg string, keysAndValues ...interface{}) <span class="cov0" title="0">{
        tl.log(LogLevelWarn, "WARN", msg, keysAndValues...)
}</span>

func (tl *TestLogger) Error(msg string, keysAndValues ...interface{}) <span class="cov0" title="0">{
        tl.log(LogLevelError, "ERROR", msg, keysAndValues...)
}</span>

func (tl *TestLogger) WithContext(ctx context.Context) Logger <span class="cov0" title="0">{
        return tl
}</span>

func (tl *TestLogger) WithFields(keysAndValues ...interface{}) Logger <span class="cov0" title="0">{
        // Create a new logger with the additional fields
        newLogger := &amp;TestLogger{
                t:      tl.t,
                buffer: tl.buffer, // Share the same buffer
                level:  tl.level,
                fields: append(tl.fields, keysAndValues...),
        }
        return newLogger
}</span>

// GetOutput returns all captured log output
func (tl *TestLogger) GetOutput() string <span class="cov0" title="0">{
        return tl.buffer.String()
}</span>

// Contains checks if the log output contains a string
func (tl *TestLogger) Contains(substr string) bool <span class="cov0" title="0">{
        return strings.Contains(tl.buffer.String(), substr)
}</span>
</pre>
		
		</div>
	</body>
	<script>
	(function() {
		var files = document.getElementById('files');
		var visible;
		files.addEventListener('change', onChange, false);
		function select(part) {
			if (visible)
				visible.style.display = 'none';
			visible = document.getElementById(part);
			if (!visible)
				return;
			files.value = part;
			visible.style.display = 'block';
			location.hash = part;
		}
		function onChange() {
			select(files.value);
			window.scrollTo(0, 0);
		}
		if (location.hash != "") {
			select(location.hash.substr(1));
		}
		if (!visible) {
			select("file0");
		}
	})();
	</script>
</html>
