
<!DOCTYPE html>
<html>
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
		<title>comet: Go Coverage Report</title>
		<style>
			body {
				background: black;
				color: rgb(80, 80, 80);
			}
			body, pre, #legend span {
				font-family: Menlo, monospace;
				font-weight: bold;
			}
			#topbar {
				background: black;
				position: fixed;
				top: 0; left: 0; right: 0;
				height: 42px;
				border-bottom: 1px solid rgb(80, 80, 80);
			}
			#content {
				margin-top: 50px;
			}
			#nav, #legend {
				float: left;
				margin-left: 10px;
			}
			#legend {
				margin-top: 12px;
			}
			#nav {
				margin-top: 10px;
			}
			#legend span {
				margin: 0 5px;
			}
			.cov0 { color: rgb(192, 0, 0) }
.cov1 { color: rgb(128, 128, 128) }
.cov2 { color: rgb(116, 140, 131) }
.cov3 { color: rgb(104, 152, 134) }
.cov4 { color: rgb(92, 164, 137) }
.cov5 { color: rgb(80, 176, 140) }
.cov6 { color: rgb(68, 188, 143) }
.cov7 { color: rgb(56, 200, 146) }
.cov8 { color: rgb(44, 212, 149) }
.cov9 { color: rgb(32, 224, 152) }
.cov10 { color: rgb(20, 236, 155) }

		</style>
	</head>
	<body>
		<div id="topbar">
			<div id="nav">
				<select id="files">
				
				<option value="file0">github.com/orbiterhq/comet/client.go (67.4%)</option>
				
				<option value="file1">github.com/orbiterhq/comet/cmd/test_worker/main.go (0.0%)</option>
				
				<option value="file2">github.com/orbiterhq/comet/consumer.go (66.2%)</option>
				
				<option value="file3">github.com/orbiterhq/comet/index_binary.go (72.7%)</option>
				
				<option value="file4">github.com/orbiterhq/comet/mmap_writer.go (75.1%)</option>
				
				<option value="file5">github.com/orbiterhq/comet/reader.go (47.1%)</option>
				
				<option value="file6">github.com/orbiterhq/comet/retention.go (22.9%)</option>
				
				</select>
			</div>
			<div id="legend">
				<span>not tracked</span>
			
				<span class="cov0">not covered</span>
				<span class="cov8">covered</span>
			
			</div>
		</div>
		<div id="content">
		
		<pre class="file" id="file0" style="display: none">package comet

import (
        "bufio"
        "context"
        "encoding/binary"
        "fmt"
        "hash/fnv"
        "log"
        "os"
        "path/filepath"
        "slices"
        "sort"
        "strings"
        "sync"
        "sync/atomic"
        "syscall"
        "time"
        "unsafe"

        "github.com/klauspost/compress/zstd"
)

// Wire format for each entry:
// [uint32 length][uint64 timestamp][byte[] data]

// MmapSharedState provides instant multi-process coordination
// Just 8 bytes - a timestamp indicating when the index was last modified
type MmapSharedState struct {
        LastUpdateNanos int64 // Nanosecond timestamp of last index change
}

// SequenceState is a memory-mapped structure for atomic entry number generation
type SequenceState struct {
        LastEntryNumber int64 // Last allocated entry number
}

// DURABILITY SEMANTICS:
//
// 1. Write Acknowledgment:
//    - Add() returns successfully after data is written to OS buffers
//    - Direct I/O bypasses page cache when available (Linux)
//    - Not guaranteed durable until explicit Sync() or checkpoint
//
// 2. Checkpointing (automatic durability):
//    - Index persisted every 10,000 writes OR 5 seconds
//    - Data file sync'd during checkpoint
//    - Consumer offsets persisted immediately on ACK
//
// 3. Crash Recovery:
//    - Files scanned from last checkpoint to find actual EOF
//    - Partial writes at EOF are discarded
//    - Consumer offsets recovered from last persisted index
//
// 4. Consistency Guarantees:
//    - Entry-based addressing ensures compression compatibility
//    - Atomic index updates prevent torn reads
//    - Memory barriers ensure proper ordering
//
// 5. Performance vs Durability Trade-offs:
//    - Fast writes: Data buffered, periodic sync
//    - Crash safety: Index checkpoints provide recovery points
//    - Consumer safety: ACKs are immediately durable

const (
        headerSize              = 12      // 4 bytes length + 8 bytes timestamp
        defaultBufSize          = 1 &lt;&lt; 20 // 1MB buffer
        checkpointWrites        = 10000
        maxFileSize             = 1 &lt;&lt; 30   // 1GB per file
        preallocSize            = 128 &lt;&lt; 20 // 128MB preallocation
        minCompressSize         = 256       // Don't compress entries smaller than this
        vectorIOBatch           = 64        // Number of entries to batch for vectored I/O
        defaultBoundaryInterval = 100       // Store boundaries every N entries for memory efficiency
        compressionWorkers      = 4         // Number of parallel compression workers
        compressionQueueSize    = 256       // Buffer size for compression queue
        defaultShardCount       = 16        // Default number of shards for load distribution
)

// Smart Sharding Helpers

// ShardStreamName generates a stream name for a specific shard
func ShardStreamName(namespace string, version string, shardID uint32) string <span class="cov8" title="1">{
        return fmt.Sprintf("%s:%s:shard:%04d", namespace, version, shardID)
}</span>

// PickShard selects a shard based on a key using consistent hashing
// Returns a shard ID in the range [0, shardCount)
func PickShard(key string, shardCount uint32) uint32 <span class="cov8" title="1">{
        if shardCount == 0 </span><span class="cov8" title="1">{
                shardCount = defaultShardCount
        }</span>

        <span class="cov8" title="1">h := fnv.New32a()
        h.Write([]byte(key))
        return h.Sum32() % shardCount</span>
}

// PickShardStream combines PickShard and ShardStreamName for convenience
// Example: PickShardStream("user123", "events", "v1", 16) -&gt; "events:v1:shard:0007"
func PickShardStream(key, namespace, version string, shardCount uint32) string <span class="cov8" title="1">{
        shardID := PickShard(key, shardCount)
        return ShardStreamName(namespace, version, shardID)
}</span>

// AllShardsRange returns a slice of shard IDs from 0 to shardCount-1
// Useful for consumers that need to read from all shards
func AllShardsRange(shardCount uint32) []uint32 <span class="cov8" title="1">{
        if shardCount == 0 </span><span class="cov8" title="1">{
                shardCount = defaultShardCount
        }</span>

        <span class="cov8" title="1">shards := make([]uint32, shardCount)
        for i := uint32(0); i &lt; shardCount; i++ </span><span class="cov8" title="1">{
                shards[i] = i
        }</span>
        <span class="cov8" title="1">return shards</span>
}

// AllShardStreams returns stream names for all shards in a namespace
func AllShardStreams(namespace, version string, shardCount uint32) []string <span class="cov8" title="1">{
        if shardCount == 0 </span><span class="cov0" title="0">{
                shardCount = defaultShardCount
        }</span>

        <span class="cov8" title="1">streams := make([]string, shardCount)
        for i := uint32(0); i &lt; shardCount; i++ </span><span class="cov8" title="1">{
                streams[i] = ShardStreamName(namespace, version, i)
        }</span>
        <span class="cov8" title="1">return streams</span>
}

// CometStats tracks key metrics for monitoring comet performance
type CometStats struct {
        // Write metrics
        TotalEntries     uint64 `json:"total_entries"`
        TotalBytes       uint64 `json:"total_bytes"`
        TotalCompressed  uint64 `json:"total_compressed_bytes"`
        WriteLatencyNano uint64 `json:"avg_write_latency_nano"`
        MinWriteLatency  uint64 `json:"min_write_latency_nano"`
        MaxWriteLatency  uint64 `json:"max_write_latency_nano"`

        // Compression metrics
        CompressionRatio   uint64 `json:"compression_ratio_x100"` // x100 for fixed point
        CompressedEntries  uint64 `json:"compressed_entries"`
        SkippedCompression uint64 `json:"skipped_compression"`

        // File management
        TotalFiles      uint64 `json:"total_files"`
        FileRotations   uint64 `json:"file_rotations"`
        CheckpointCount uint64 `json:"checkpoint_count"`
        LastCheckpoint  uint64 `json:"last_checkpoint_nano"`

        // Consumer metrics
        ActiveReaders uint64 `json:"active_readers"`
        ConsumerLag   uint64 `json:"max_consumer_lag_bytes"`

        // Error tracking
        ErrorCount         uint64 `json:"error_count"`
        LastErrorNano      uint64 `json:"last_error_nano"`
        IndexPersistErrors uint64 `json:"index_persist_errors"`

        // Compression metrics
        CompressionWaitNano uint64 `json:"compression_wait_nano"`
}

// ClientMetrics holds atomic counters for thread-safe metrics tracking
type ClientMetrics struct {
        TotalEntries       atomic.Uint64
        TotalBytes         atomic.Uint64
        TotalCompressed    atomic.Uint64
        WriteLatencyNano   atomic.Uint64
        MinWriteLatency    atomic.Uint64
        MaxWriteLatency    atomic.Uint64
        CompressionRatio   atomic.Uint64
        CompressedEntries  atomic.Uint64
        SkippedCompression atomic.Uint64
        TotalFiles         atomic.Uint64
        FileRotations      atomic.Uint64
        CheckpointCount    atomic.Uint64
        LastCheckpoint     atomic.Uint64
        ActiveReaders      atomic.Uint64
        ConsumerLag        atomic.Uint64
        ErrorCount         atomic.Uint64
        LastErrorNano      atomic.Uint64
        CompressionWait    atomic.Uint64 // Time waiting for compression
        IndexPersistErrors atomic.Uint64 // Failed index persistence attempts
}

// RetentionConfig defines data retention policies
type RetentionConfig struct {
        // Time-based retention
        MaxAge time.Duration `json:"max_age"` // Delete files older than this

        // Size-based retention
        MaxTotalSize int64 `json:"max_total_size"` // Total size limit across all shards
        MaxShardSize int64 `json:"max_shard_size"` // Size limit per shard

        // Cleanup behavior
        CleanupInterval   time.Duration `json:"cleanup_interval"`   // How often to run cleanup
        MinFilesToKeep    int           `json:"min_files_to_keep"`  // Always keep at least N files
        ProtectUnconsumed bool          `json:"protect_unconsumed"` // Don't delete unread data
        ForceDeleteAfter  time.Duration `json:"force_delete_after"` // Delete even if unread after this time
}

// CompressionConfig controls compression behavior
type CompressionConfig struct {
        MinCompressSize int `json:"min_compress_size"` // Don't compress entries smaller than this
}

// IndexingConfig controls indexing behavior
type IndexingConfig struct {
        BoundaryInterval int `json:"boundary_interval"` // Store boundaries every N entries
        MaxIndexEntries  int `json:"max_index_entries"` // Max boundary entries per shard (0 = unlimited)
}

// StorageConfig controls file storage behavior
type StorageConfig struct {
        MaxFileSize    int64 `json:"max_file_size"`      // Maximum size per file before rotation
        CheckpointTime int   `json:"checkpoint_time_ms"` // Checkpoint every N milliseconds
}

// ConcurrencyConfig controls multi-process behavior
type ConcurrencyConfig struct {
        EnableMultiProcessMode bool `json:"enable_file_locking"` // Enable file-based locking for multi-process safety
}

// CometConfig holds configuration for comet client
type CometConfig struct {
        // Compression settings
        Compression CompressionConfig `json:"compression"`

        // Indexing settings
        Indexing IndexingConfig `json:"indexing"`

        // Storage settings
        Storage StorageConfig `json:"storage"`

        // Concurrency settings
        Concurrency ConcurrencyConfig `json:"concurrency"`

        // Retention policy
        Retention RetentionConfig `json:"retention"`
}

// DefaultCometConfig returns sensible defaults optimized for logging workloads
func DefaultCometConfig() CometConfig <span class="cov8" title="1">{
        return CometConfig{
                // Compression - optimized for logging workloads
                Compression: CompressionConfig{
                        MinCompressSize: 4096, // Only compress entries &gt;4KB to avoid latency hit on typical logs
                },

                // Indexing - memory efficient boundary tracking
                Indexing: IndexingConfig{
                        BoundaryInterval: 100,   // Store boundaries every 100 entries
                        MaxIndexEntries:  10000, // Limit index memory growth
                },

                // Storage - optimized for 1GB files
                Storage: StorageConfig{
                        MaxFileSize:    1 &lt;&lt; 30, // 1GB per file
                        CheckpointTime: 2000,    // Checkpoint every 2 seconds
                },

                // Concurrency - disabled by default for performance
                // Enable this for multi-process deployments (e.g., prefork mode)
                Concurrency: ConcurrencyConfig{
                        EnableMultiProcessMode: false, // Single-process mode is faster
                },

                // Retention - defaults for edge observability
                Retention: RetentionConfig{
                        MaxAge:            4 * time.Hour,    // Keep 4 hours of data
                        MaxTotalSize:      10 &lt;&lt; 30,         // 10GB total
                        MaxShardSize:      1 &lt;&lt; 30,          // 1GB per shard
                        CleanupInterval:   15 * time.Minute, // Check every 15 minutes
                        MinFilesToKeep:    2,                // Keep at least 2 files
                        ProtectUnconsumed: true,             // Don't delete unread data by default
                        ForceDeleteAfter:  24 * time.Hour,   // Force delete after 24 hours
                },
        }
}</span>

// HighCompressionConfig returns a config optimized for high compression ratios
// Suitable for text-heavy logs and structured data
func HighCompressionConfig() CometConfig <span class="cov0" title="0">{
        cfg := DefaultCometConfig()
        cfg.Compression.MinCompressSize = 256 // Compress almost everything
        return cfg
}</span>

// MultiProcessConfig returns a config suitable for multi-process deployments
// Enable this for prefork servers or when multiple processes write to the same stream
func MultiProcessConfig() CometConfig <span class="cov8" title="1">{
        cfg := DefaultCometConfig()
        cfg.Concurrency.EnableMultiProcessMode = true
        cfg.Storage.CheckpointTime = 100 // More frequent checkpoints for multi-process coordination
        return cfg
}</span>

// HighThroughputConfig returns a config optimized for high write throughput
// Trades some memory for better performance
func HighThroughputConfig() CometConfig <span class="cov0" title="0">{
        cfg := DefaultCometConfig()
        cfg.Indexing.BoundaryInterval = 1000    // Less frequent indexing
        cfg.Indexing.MaxIndexEntries = 100000   // Allow larger index
        cfg.Storage.CheckpointTime = 5000       // Less frequent checkpoints
        cfg.Compression.MinCompressSize = 10240 // Only compress very large entries
        return cfg
}</span>

// migrateConfig migrates deprecated fields to new structure
func migrateConfig(cfg *CometConfig) {<span class="cov8" title="1">
        // Currently no migrations needed since we removed all deprecated fields
}</span>

// validateConfig ensures configuration values are reasonable
func validateConfig(cfg *CometConfig) error <span class="cov8" title="1">{
        // Storage validation
        if cfg.Storage.MaxFileSize &lt; 100 </span><span class="cov0" title="0">{
                return fmt.Errorf("MaxFileSize must be at least 100 bytes, got %d", cfg.Storage.MaxFileSize)
        }</span>
        <span class="cov8" title="1">if cfg.Storage.MaxFileSize &gt; 10&lt;&lt;30 </span><span class="cov0" title="0">{ // 10GB
                return fmt.Errorf("MaxFileSize cannot exceed 10GB, got %d", cfg.Storage.MaxFileSize)
        }</span>
        <span class="cov8" title="1">if cfg.Storage.CheckpointTime &lt; 0 </span><span class="cov0" title="0">{
                return fmt.Errorf("CheckpointTime cannot be negative, got %d", cfg.Storage.CheckpointTime)
        }</span>
        <span class="cov8" title="1">if cfg.Storage.CheckpointTime &gt; 300000 </span><span class="cov0" title="0">{ // 5 minutes
                return fmt.Errorf("CheckpointTime cannot exceed 5 minutes (300000ms), got %d", cfg.Storage.CheckpointTime)
        }</span>

        // Compression validation
        <span class="cov8" title="1">if cfg.Compression.MinCompressSize &lt; 0 </span><span class="cov0" title="0">{
                return fmt.Errorf("MinCompressSize cannot be negative, got %d", cfg.Compression.MinCompressSize)
        }</span>
        // Note: MinCompressSize can be larger than MaxFileSize to effectively disable compression

        // Indexing validation
        <span class="cov8" title="1">if cfg.Indexing.BoundaryInterval &lt; 1 </span><span class="cov0" title="0">{
                return fmt.Errorf("BoundaryInterval must be at least 1, got %d", cfg.Indexing.BoundaryInterval)
        }</span>
        <span class="cov8" title="1">if cfg.Indexing.BoundaryInterval &gt; 10000 </span><span class="cov0" title="0">{
                return fmt.Errorf("BoundaryInterval cannot exceed 10000, got %d", cfg.Indexing.BoundaryInterval)
        }</span>
        <span class="cov8" title="1">if cfg.Indexing.MaxIndexEntries &lt; 0 </span><span class="cov0" title="0">{
                return fmt.Errorf("MaxIndexEntries cannot be negative, got %d", cfg.Indexing.MaxIndexEntries)
        }</span>
        <span class="cov8" title="1">if cfg.Indexing.MaxIndexEntries &gt; 0 &amp;&amp; cfg.Indexing.MaxIndexEntries &lt; 10 </span><span class="cov0" title="0">{
                return fmt.Errorf("MaxIndexEntries must be 0 (unlimited) or at least 10, got %d", cfg.Indexing.MaxIndexEntries)
        }</span>

        // Retention validation
        <span class="cov8" title="1">if cfg.Retention.MaxShardSize &lt; 0 </span><span class="cov0" title="0">{
                return fmt.Errorf("MaxShardSize cannot be negative, got %d", cfg.Retention.MaxShardSize)
        }</span>
        <span class="cov8" title="1">if cfg.Retention.MaxShardSize &gt; 0 &amp;&amp; cfg.Retention.MaxShardSize &lt; cfg.Storage.MaxFileSize </span><span class="cov0" title="0">{
                return fmt.Errorf("MaxShardSize must be 0 (unlimited) or at least MaxFileSize (%d), got %d",
                        cfg.Storage.MaxFileSize, cfg.Retention.MaxShardSize)
        }</span>
        <span class="cov8" title="1">if cfg.Retention.MaxAge &lt; 0 </span><span class="cov0" title="0">{
                return fmt.Errorf("MaxAge cannot be negative, got %v", cfg.Retention.MaxAge)
        }</span>

        <span class="cov8" title="1">return nil</span>
}

// Client implements a local file-based stream client with append-only storage
type Client struct {
        dataDir     string
        config      CometConfig
        shards      map[uint32]*Shard
        metrics     ClientMetrics
        mu          sync.RWMutex
        closed      bool
        retentionWg sync.WaitGroup
        stopCh      chan struct{}
}

// Shard represents a single stream shard with its own files
// Fields ordered for optimal memory alignment (64-bit words first)
type Shard struct {
        // 64-bit aligned fields first (8 bytes each)
        readerCount     int64     // Lock-free reader tracking
        lastCheckpoint  time.Time // 64-bit on most systems
        lastMmapCheck   int64     // Last mmap timestamp we saw (for change detection)
        sequenceCounter *int64    // Memory-mapped sequence counter for file naming

        // Pointers (8 bytes each on 64-bit)
        dataFile      *os.File         // Data file handle
        writer        *bufio.Writer    // Buffered writer
        compressor    *zstd.Encoder    // Compression engine
        index         *ShardIndex      // Shard metadata
        lockFile      *os.File         // File lock for multi-writer safety
        indexLockFile *os.File         // Separate lock for index writes
        mmapState     *MmapSharedState // Memory-mapped coordination state
        sequenceState *SequenceState   // Memory-mapped sequence state for entry numbers
        sequenceFile  *os.File         // File handle for sequence counter
        mmapWriter    *MmapWriter      // Memory-mapped writer for ultra-fast multi-process writes

        // Strings (24 bytes: ptr + len + cap)
        indexPath         string // Path to index file
        indexStatePath    string // Path to index state file
        indexLockPath     string // Path to index lock file
        sequenceStatePath string // Path to sequence counter file
        indexStateData    []byte // Memory-mapped index state data (slice header: 24 bytes)
        sequenceStateData []byte // Memory-mapped sequence state data (slice header: 24 bytes)

        // Mutex (platform-specific, often 24 bytes)
        mu      sync.RWMutex
        writeMu sync.Mutex // Protects DirectWriter from concurrent writes
        indexMu sync.Mutex // Protects index file writes

        // Synchronization for background operations
        wg sync.WaitGroup // Tracks background goroutines

        // Smaller fields last
        writesSinceCheckpoint int    // 8 bytes
        shardID               uint32 // 4 bytes
        // 4 bytes padding will be added automatically for 8-byte alignment
}

// EntryIndexNode represents a node in the binary searchable index
type EntryIndexNode struct {
        EntryNumber int64         `json:"entry_number"` // Entry number this node covers
        Position    EntryPosition `json:"position"`     // Position in files
}

// BinarySearchableIndex provides O(log n) entry lookups
type BinarySearchableIndex struct {
        // Sorted slice of index nodes for binary search
        Nodes []EntryIndexNode `json:"nodes"`
        // Interval between indexed entries (default: 1000)
        IndexInterval int `json:"index_interval"`
        // Maximum number of nodes to keep (0 = unlimited)
        MaxNodes int `json:"max_nodes"`
}

// ShardIndex tracks files and consumer offsets
// Fixed to use entry-based addressing instead of byte offsets
// Fields ordered for optimal memory alignment
type ShardIndex struct {
        // 64-bit aligned fields first (8 bytes each)
        CurrentEntryNumber int64 `json:"current_entry_number"` // Entry-based tracking (not byte offsets!)
        CurrentWriteOffset int64 `json:"current_write_offset"` // Still track for file management

        // Maps (8 bytes pointer each)
        ConsumerOffsets map[string]int64 `json:"consumer_entry_offsets"` // Consumer tracking by entry number (not bytes!)

        // Composite types
        BinaryIndex BinarySearchableIndex `json:"binary_index"` // Binary searchable index for O(log n) lookups

        // Slices (24 bytes: ptr + len + cap)
        Files []FileInfo `json:"files"` // File management

        // Strings (24 bytes: ptr + len + cap)
        CurrentFile string `json:"current_file"`

        // Smaller fields last
        BoundaryInterval int `json:"boundary_interval"` // 8 bytes
}

// AddIndexNode adds a new entry to the binary searchable index
func (bi *BinarySearchableIndex) AddIndexNode(entryNumber int64, position EntryPosition) <span class="cov8" title="1">{
        // Only index entries at the specified interval to limit memory usage
        if bi.IndexInterval == 0 </span><span class="cov0" title="0">{
                bi.IndexInterval = 1000 // Default interval
        }</span>

        <span class="cov8" title="1">if entryNumber%int64(bi.IndexInterval) == 0 || entryNumber == 0 </span><span class="cov8" title="1">{
                node := EntryIndexNode{
                        EntryNumber: entryNumber,
                        Position:    position,
                }

                // Use binary search to find insertion point to maintain sorted order
                idx, found := slices.BinarySearchFunc(bi.Nodes, node, func(a, b EntryIndexNode) int </span><span class="cov8" title="1">{
                        if a.EntryNumber &lt; b.EntryNumber </span><span class="cov8" title="1">{
                                return -1
                        }</span>
                        <span class="cov8" title="1">if a.EntryNumber &gt; b.EntryNumber </span><span class="cov8" title="1">{
                                return 1
                        }</span>
                        <span class="cov0" title="0">return 0</span>
                })

                <span class="cov8" title="1">if !found </span><span class="cov8" title="1">{
                        // Insert at the correct position to maintain sorted order
                        bi.Nodes = slices.Insert(bi.Nodes, idx, node)

                        // Check if we need to prune old nodes
                        if bi.MaxNodes &gt; 0 &amp;&amp; len(bi.Nodes) &gt; bi.MaxNodes </span><span class="cov8" title="1">{
                                // Remove the oldest (first) node
                                bi.Nodes = bi.Nodes[1:]
                        }</span>
                } else<span class="cov0" title="0"> {
                        // Update existing node
                        bi.Nodes[idx] = node
                }</span>
        }
}

// FindEntry uses binary search to locate an entry position
func (bi *BinarySearchableIndex) FindEntry(entryNumber int64) (EntryPosition, bool) <span class="cov8" title="1">{
        if len(bi.Nodes) == 0 </span><span class="cov0" title="0">{
                return EntryPosition{}, false
        }</span>

        // Find the largest indexed entry &lt;= target
        <span class="cov8" title="1">target := EntryIndexNode{EntryNumber: entryNumber}
        idx, found := slices.BinarySearchFunc(bi.Nodes, target, func(a, b EntryIndexNode) int </span><span class="cov8" title="1">{
                if a.EntryNumber &lt; b.EntryNumber </span><span class="cov8" title="1">{
                        return -1
                }</span>
                <span class="cov8" title="1">if a.EntryNumber &gt; b.EntryNumber </span><span class="cov8" title="1">{
                        return 1
                }</span>
                <span class="cov8" title="1">return 0</span>
        })

        <span class="cov8" title="1">if found </span><span class="cov8" title="1">{
                // Exact match
                return bi.Nodes[idx].Position, true
        }</span>

        <span class="cov8" title="1">if idx == 0 </span><span class="cov0" title="0">{
                // Target is before the first indexed entry
                return EntryPosition{}, false
        }</span>

        // Return the position of the largest indexed entry before target
        // The caller will need to scan forward from this position
        <span class="cov8" title="1">return bi.Nodes[idx-1].Position, true</span>
}

// GetScanStartPosition returns the best starting position for scanning to find an entry
func (bi *BinarySearchableIndex) GetScanStartPosition(entryNumber int64) (EntryPosition, int64, bool) <span class="cov8" title="1">{
        if len(bi.Nodes) == 0 </span><span class="cov0" title="0">{
                return EntryPosition{}, 0, false
        }</span>

        // Find the largest indexed entry &lt;= target
        <span class="cov8" title="1">target := EntryIndexNode{EntryNumber: entryNumber}
        idx, found := slices.BinarySearchFunc(bi.Nodes, target, func(a, b EntryIndexNode) int </span><span class="cov8" title="1">{
                if a.EntryNumber &lt; b.EntryNumber </span><span class="cov8" title="1">{
                        return -1
                }</span>
                <span class="cov8" title="1">if a.EntryNumber &gt; b.EntryNumber </span><span class="cov8" title="1">{
                        return 1
                }</span>
                <span class="cov8" title="1">return 0</span>
        })

        <span class="cov8" title="1">if found </span><span class="cov8" title="1">{
                // Exact match
                return bi.Nodes[idx].Position, bi.Nodes[idx].EntryNumber, true
        }</span>

        <span class="cov8" title="1">if idx == 0 </span><span class="cov8" title="1">{
                // Target is before the first indexed entry, start from beginning
                return EntryPosition{FileIndex: 0, ByteOffset: 0}, 0, true
        }</span>

        // Return the position of the largest indexed entry before target
        <span class="cov8" title="1">node := bi.Nodes[idx-1]
        return node.Position, node.EntryNumber, true</span>
}

// EntryPosition tracks where an entry is located
// Fields ordered for optimal memory alignment
type EntryPosition struct {
        ByteOffset int64 `json:"byte_offset"` // Byte position within that file (8 bytes)
        FileIndex  int   `json:"file_index"`  // Which file in Files array (8 bytes)
}

// FileInfo tracks a single data file
// Fields ordered for optimal memory alignment
type FileInfo struct {
        // 64-bit aligned fields first
        StartOffset int64     `json:"start_offset"`
        EndOffset   int64     `json:"end_offset"`
        StartEntry  int64     `json:"start_entry"` // First entry number in this file
        Entries     int64     `json:"entries"`
        StartTime   time.Time `json:"start_time"`
        EndTime     time.Time `json:"end_time"`

        // String last (will use remaining space efficiently)
        Path string `json:"path"`
}

// NewClient creates a new comet client for local file-based streaming with default config
func NewClient(dataDir string) (*Client, error) <span class="cov8" title="1">{
        return NewClientWithConfig(dataDir, DefaultCometConfig())
}</span>

// NewClientWithConfig creates a new comet client with custom configuration
func NewClientWithConfig(dataDir string, config CometConfig) (*Client, error) <span class="cov8" title="1">{
        // Migrate deprecated fields to new structure
        migrateConfig(&amp;config)

        // Validate configuration
        if err := validateConfig(&amp;config); err != nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("invalid configuration: %w", err)
        }</span>

        <span class="cov8" title="1">if err := os.MkdirAll(dataDir, 0755); err != nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("failed to create data directory: %w", err)
        }</span>

        <span class="cov8" title="1">c := &amp;Client{
                dataDir: dataDir,
                config:  config,
                shards:  make(map[uint32]*Shard),
                stopCh:  make(chan struct{}),
        }

        // Start retention manager if configured
        c.startRetentionManager()

        return c, nil</span>
}

// Append adds entries to a stream shard (append-only semantics)
func (c *Client) Append(ctx context.Context, stream string, entries [][]byte) ([]MessageID, error) <span class="cov8" title="1">{
        // Extract shard from stream name (e.g., "events:v1:shard:0042" -&gt; 42)
        shardID, err := parseShardFromStream(stream)
        if err != nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("invalid stream name %s: %w", stream, err)
        }</span>

        <span class="cov8" title="1">shard, err := c.getOrCreateShard(shardID)
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>

        <span class="cov8" title="1">return shard.appendEntries(entries, &amp;c.metrics, &amp;c.config)</span>
}

// Len returns the number of entries in a stream shard
func (c *Client) Len(ctx context.Context, stream string) (int64, error) <span class="cov8" title="1">{
        shardID, err := parseShardFromStream(stream)
        if err != nil </span><span class="cov0" title="0">{
                return 0, fmt.Errorf("invalid stream name %s: %w", stream, err)
        }</span>

        <span class="cov8" title="1">shard, err := c.getOrCreateShard(shardID)
        if err != nil </span><span class="cov0" title="0">{
                return 0, err
        }</span>

        <span class="cov8" title="1">shard.mu.RLock()
        defer shard.mu.RUnlock()

        // If file locking is enabled, reload index to get latest state from other processes
        // Only reload if the index file exists and is newer than our last checkpoint
        if c.config.Concurrency.EnableMultiProcessMode &amp;&amp; shard.lockFile != nil </span><span class="cov0" title="0">{
                if indexStat, err := os.Stat(shard.indexPath); err == nil </span><span class="cov0" title="0">{
                        if indexStat.ModTime().After(shard.lastCheckpoint) </span><span class="cov0" title="0">{
                                // Acquire shared lock for reading
                                err := syscall.Flock(int(shard.lockFile.Fd()), syscall.LOCK_SH)
                                if err != nil </span><span class="cov0" title="0">{
                                        return 0, fmt.Errorf("failed to acquire shared lock for Len: %w", err)
                                }</span>
                                <span class="cov0" title="0">defer syscall.Flock(int(shard.lockFile.Fd()), syscall.LOCK_UN)

                                // Reload index to get current state
                                if err := shard.loadIndex(); err != nil </span><span class="cov0" title="0">{
                                        return 0, fmt.Errorf("failed to reload index for Len: %w", err)
                                }</span>
                        }
                }
        }

        // In multi-process mode, check if we need to rebuild index AFTER reloading
        <span class="cov8" title="1">if shard.mmapState != nil </span><span class="cov0" title="0">{
                // Check if rebuild needed while holding read lock
                needsRebuild := shard.checkIfRebuildNeeded()
                shard.mu.RUnlock()

                if needsRebuild </span><span class="cov0" title="0">{
                        // Acquire write lock for rebuild
                        shard.mu.Lock()
                        // Double-check after acquiring write lock
                        if shard.checkIfRebuildNeeded() </span><span class="cov0" title="0">{
                                shard.lazyRebuildIndexIfNeeded(c.config, filepath.Join(c.dataDir, fmt.Sprintf("shard-%04d", shard.shardID)))
                        }</span>
                        <span class="cov0" title="0">shard.mu.Unlock()</span>
                }

                // Re-acquire read lock
                <span class="cov0" title="0">shard.mu.RLock()</span>
        }

        <span class="cov8" title="1">var total int64
        for _, file := range shard.index.Files </span><span class="cov8" title="1">{
                total += file.Entries
        }</span>

        <span class="cov8" title="1">return total, nil</span>
}

// Sync ensures all buffered data is durably written to disk
func (c *Client) Sync(ctx context.Context) error <span class="cov8" title="1">{
        c.mu.RLock()
        shards := make([]*Shard, 0, len(c.shards))
        for _, shard := range c.shards </span><span class="cov8" title="1">{
                shards = append(shards, shard)
        }</span>
        <span class="cov8" title="1">c.mu.RUnlock()

        for _, shard := range shards </span><span class="cov8" title="1">{
                shard.mu.Lock()

                // Flush and sync writer
                if shard.writer != nil </span><span class="cov8" title="1">{
                        shard.writeMu.Lock()
                        err := shard.writer.Flush()
                        if err == nil &amp;&amp; shard.dataFile != nil </span><span class="cov8" title="1">{
                                err = shard.dataFile.Sync()
                        }</span>
                        <span class="cov8" title="1">shard.writeMu.Unlock()
                        if err != nil </span><span class="cov0" title="0">{
                                shard.mu.Unlock()
                                return fmt.Errorf("failed to sync shard %d: %w", shard.shardID, err)
                        }</span>
                }

                // Force checkpoint
                // Clone index while holding lock
                <span class="cov8" title="1">indexCopy := shard.cloneIndex()
                shard.writesSinceCheckpoint = 0
                shard.lastCheckpoint = time.Now()
                shard.mu.Unlock()

                // Persist outside of lock
                // For multi-process safety, use the separate index lock if enabled
                var err error
                if c.config.Concurrency.EnableMultiProcessMode &amp;&amp; shard.indexLockFile != nil </span><span class="cov8" title="1">{
                        // Acquire exclusive lock for index writes
                        if err := syscall.Flock(int(shard.indexLockFile.Fd()), syscall.LOCK_EX); err != nil </span><span class="cov0" title="0">{
                                return fmt.Errorf("failed to acquire index lock for shard %d: %w", shard.shardID, err)
                        }</span>

                        <span class="cov8" title="1">shard.indexMu.Lock()
                        // Reload index from disk to merge with other processes' changes
                        if _, statErr := os.Stat(shard.indexPath); statErr == nil </span><span class="cov8" title="1">{
                                // Index exists, load it to get latest state
                                if diskIndex, loadErr := shard.loadBinaryIndex(); loadErr == nil </span><span class="cov8" title="1">{
                                        // Merge our changes with the disk state
                                        // Important: We need to merge carefully to avoid losing entries
                                        // The disk state represents entries written by other processes

                                        // Always use the highest entry number and write offset
                                        if diskIndex.CurrentEntryNumber &gt; indexCopy.CurrentEntryNumber </span><span class="cov0" title="0">{
                                                indexCopy.CurrentEntryNumber = diskIndex.CurrentEntryNumber
                                        }</span>
                                        <span class="cov8" title="1">if diskIndex.CurrentWriteOffset &gt; indexCopy.CurrentWriteOffset </span><span class="cov0" title="0">{
                                                indexCopy.CurrentWriteOffset = diskIndex.CurrentWriteOffset
                                        }</span>

                                        // Merge consumer offsets - keep the highest offset for each consumer
                                        <span class="cov8" title="1">for group, offset := range diskIndex.ConsumerOffsets </span><span class="cov0" title="0">{
                                                if currentOffset, exists := indexCopy.ConsumerOffsets[group]; !exists || offset &gt; currentOffset </span><span class="cov0" title="0">{
                                                        indexCopy.ConsumerOffsets[group] = offset
                                                }</span>
                                        }

                                        // Merge file info - this is critical for multi-process coordination
                                        // The disk version should have the most complete file information
                                        <span class="cov8" title="1">if len(diskIndex.Files) &gt; 0 </span><span class="cov8" title="1">{
                                                // Always use the disk version as it represents the actual files
                                                indexCopy.Files = diskIndex.Files

                                                // Update the current file info if we have one
                                                if len(indexCopy.Files) &gt; 0 &amp;&amp; indexCopy.CurrentFile != "" </span><span class="cov8" title="1">{
                                                        lastFile := &amp;indexCopy.Files[len(indexCopy.Files)-1]
                                                        // Update the last file's end offset and entry count based on our writes
                                                        if lastFile.Path == indexCopy.CurrentFile </span><span class="cov8" title="1">{
                                                                lastFile.EndOffset = indexCopy.CurrentWriteOffset
                                                                // Calculate actual entries in the file
                                                                if diskIndex.CurrentEntryNumber &gt; 0 </span><span class="cov8" title="1">{
                                                                        entriesInOtherFiles := int64(0)
                                                                        for i := 0; i &lt; len(indexCopy.Files)-1; i++ </span><span class="cov0" title="0">{
                                                                                entriesInOtherFiles += indexCopy.Files[i].Entries
                                                                        }</span>
                                                                        <span class="cov8" title="1">lastFile.Entries = indexCopy.CurrentEntryNumber - entriesInOtherFiles</span>
                                                                }
                                                        }
                                                }
                                        }

                                        // Merge binary index nodes if needed
                                        <span class="cov8" title="1">if diskIndex.BinaryIndex.Nodes != nil &amp;&amp; len(diskIndex.BinaryIndex.Nodes) &gt; len(indexCopy.BinaryIndex.Nodes) </span><span class="cov0" title="0">{
                                                indexCopy.BinaryIndex = diskIndex.BinaryIndex
                                        }</span>
                                }
                        }

                        <span class="cov8" title="1">err = shard.saveBinaryIndex(indexCopy)
                        shard.indexMu.Unlock()

                        // Update mmap state BEFORE releasing lock
                        if err == nil </span><span class="cov8" title="1">{
                                shard.updateMmapState()
                        }</span>

                        // Release lock immediately after index save
                        <span class="cov8" title="1">syscall.Flock(int(shard.indexLockFile.Fd()), syscall.LOCK_UN)</span>
                } else<span class="cov8" title="1"> {
                        shard.indexMu.Lock()
                        err = shard.saveBinaryIndex(indexCopy)
                        shard.indexMu.Unlock()

                        if err == nil </span><span class="cov8" title="1">{
                                shard.updateMmapState()
                        }</span>
                }

                <span class="cov8" title="1">if err != nil </span><span class="cov0" title="0">{
                        return fmt.Errorf("failed to persist index for shard %d: %w", shard.shardID, err)
                }</span>
        }

        <span class="cov8" title="1">return nil</span>
}

// getOrCreateShard returns an existing shard or creates a new one
func (c *Client) getOrCreateShard(shardID uint32) (*Shard, error) <span class="cov8" title="1">{
        c.mu.RLock()
        shard, exists := c.shards[shardID]
        c.mu.RUnlock()

        if exists </span><span class="cov8" title="1">{
                return shard, nil
        }</span>

        <span class="cov8" title="1">c.mu.Lock()
        defer c.mu.Unlock()

        // Double-check after acquiring write lock
        if shard, exists = c.shards[shardID]; exists </span><span class="cov0" title="0">{
                return shard, nil
        }</span>

        // Create new shard
        <span class="cov8" title="1">shardDir := filepath.Join(c.dataDir, fmt.Sprintf("shard-%04d", shardID))
        if err := os.MkdirAll(shardDir, 0755); err != nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("failed to create shard directory: %w", err)
        }</span>

        <span class="cov8" title="1">shard = &amp;Shard{
                shardID:           shardID,
                indexPath:         filepath.Join(shardDir, "index.bin"),
                indexStatePath:    filepath.Join(shardDir, "index.state"),
                indexLockPath:     filepath.Join(shardDir, "index.lock"),
                sequenceStatePath: filepath.Join(shardDir, "sequence.state"),
                index: &amp;ShardIndex{
                        BoundaryInterval: c.config.Indexing.BoundaryInterval,
                        ConsumerOffsets:  make(map[string]int64),
                        BinaryIndex: BinarySearchableIndex{
                                IndexInterval: c.config.Indexing.BoundaryInterval,
                                MaxNodes:      c.config.Indexing.MaxIndexEntries, // Use full limit for binary index
                        },
                },
                lastCheckpoint: time.Now(),
        }

        // Create lock files for multi-writer coordination if enabled
        if c.config.Concurrency.EnableMultiProcessMode </span><span class="cov8" title="1">{
                // Data write lock
                lockPath := filepath.Join(shardDir, "shard.lock")
                lockFile, err := os.OpenFile(lockPath, os.O_CREATE|os.O_RDWR, 0644)
                if err != nil </span><span class="cov0" title="0">{
                        return nil, fmt.Errorf("failed to create lock file: %w", err)
                }</span>
                <span class="cov8" title="1">shard.lockFile = lockFile

                // Separate index write lock for better concurrency
                indexLockFile, err := os.OpenFile(shard.indexLockPath, os.O_CREATE|os.O_RDWR, 0644)
                if err != nil </span><span class="cov0" title="0">{
                        lockFile.Close()
                        return nil, fmt.Errorf("failed to create index lock file: %w", err)
                }</span>
                <span class="cov8" title="1">shard.indexLockFile = indexLockFile</span>
        }

        // Initialize mmap shared state only if file locking is enabled
        <span class="cov8" title="1">if c.config.Concurrency.EnableMultiProcessMode </span><span class="cov8" title="1">{
                if err := shard.initMmapState(); err != nil </span><span class="cov0" title="0">{
                        return nil, fmt.Errorf("failed to initialize mmap state: %w", err)
                }</span>
                <span class="cov8" title="1">if err := shard.initSequenceState(); err != nil </span><span class="cov0" title="0">{
                        return nil, fmt.Errorf("failed to initialize sequence state: %w", err)
                }</span>

                // Initialize memory-mapped writer for ultra-fast writes
                <span class="cov8" title="1">mmapWriter, err := NewMmapWriter(shardDir, c.config.Storage.MaxFileSize, shard.index, &amp;c.metrics)
                if err != nil </span><span class="cov0" title="0">{
                        return nil, fmt.Errorf("failed to initialize mmap writer: %w", err)
                }</span>
                <span class="cov8" title="1">shard.mmapWriter = mmapWriter</span>
        }

        // Load existing index if present
        <span class="cov8" title="1">if err := shard.loadIndex(); err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>

        // If we have an mmap writer, sync the index with the coordination state
        <span class="cov8" title="1">if shard.mmapWriter != nil </span><span class="cov8" title="1">{
                coordState := shard.mmapWriter.CoordinationState()
                writeOffset := coordState.WriteOffset.Load()
                if writeOffset &gt; shard.index.CurrentWriteOffset </span><span class="cov0" title="0">{
                        shard.index.CurrentWriteOffset = writeOffset
                }</span>
        }

        // Open or create current data file
        <span class="cov8" title="1">if err := shard.openDataFileWithConfig(shardDir, &amp;c.config); err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>

        // Recover from crash if needed
        <span class="cov8" title="1">if err := shard.recoverFromCrash(); err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>

        <span class="cov8" title="1">c.shards[shardID] = shard
        return shard, nil</span>
}

// CompressedEntry represents a pre-compressed entry ready for writing
type CompressedEntry struct {
        Data           []byte
        OriginalSize   uint64
        CompressedSize uint64
        WasCompressed  bool
}

// preCompressEntries compresses entries outside of any locks to reduce contention
func (s *Shard) preCompressEntries(entries [][]byte, config *CometConfig) []CompressedEntry <span class="cov8" title="1">{
        compressed := make([]CompressedEntry, len(entries))

        for i, data := range entries </span><span class="cov8" title="1">{
                originalSize := uint64(len(data))

                if len(data) &gt;= config.Compression.MinCompressSize &amp;&amp; s.compressor != nil </span><span class="cov8" title="1">{
                        // Compress the data
                        compressedData := s.compressor.EncodeAll(data, nil)
                        compressed[i] = CompressedEntry{
                                Data:           compressedData,
                                OriginalSize:   originalSize,
                                CompressedSize: uint64(len(compressedData)),
                                WasCompressed:  true,
                        }
                }</span> else<span class="cov8" title="1"> {
                        // Use original data directly (zero-copy)
                        compressed[i] = CompressedEntry{
                                Data:           data,
                                OriginalSize:   originalSize,
                                CompressedSize: originalSize,
                                WasCompressed:  false,
                        }
                }</span>
        }

        <span class="cov8" title="1">return compressed</span>
}

// WriteRequest represents a batch write operation
// Fields ordered for optimal memory alignment
type WriteRequest struct {
        UpdateFunc   func() error // Function to update index state after successful write (8 bytes)
        WriteBuffers [][]byte     // Buffers to write (24 bytes)
        IDs          []MessageID  // Message IDs for the batch (24 bytes)
}

// appendEntries adds raw entry bytes to the shard with I/O outside locks
func (s *Shard) appendEntries(entries [][]byte, clientMetrics *ClientMetrics, config *CometConfig) ([]MessageID, error) <span class="cov8" title="1">{
        startTime := time.Now()

        // Pre-compress entries OUTSIDE the lock to reduce contention
        compressedEntries := s.preCompressEntries(entries, config)

        // Prepare write request while holding lock
        var writeReq WriteRequest
        var totalOriginalBytes, totalCompressedBytes uint64
        var compressedCount, skippedCount uint64

        // Critical section: prepare write data and update indices
        func() </span><span class="cov8" title="1">{
                s.mu.Lock()
                defer s.mu.Unlock()

                // Check mmap state for instant change detection (no lock needed for read)
                if config.Concurrency.EnableMultiProcessMode &amp;&amp; s.mmapState != nil </span><span class="cov8" title="1">{
                        currentTimestamp := atomic.LoadInt64(&amp;s.mmapState.LastUpdateNanos)
                        if currentTimestamp != s.lastMmapCheck </span><span class="cov8" title="1">{
                                // Index changed - reload it with retry for EOF errors
                                if err := s.loadIndexWithRetry(); err != nil </span><span class="cov0" title="0">{
                                        panic(fmt.Errorf("failed to reload index after detecting mmap change: %w", err))</span>
                                }
                                <span class="cov8" title="1">s.lastMmapCheck = currentTimestamp</span>
                        }
                }

                <span class="cov8" title="1">writeReq.IDs = make([]MessageID, len(entries))
                now := startTime.UnixNano()

                // Build write buffers from pre-compressed data (minimal work under lock)
                writeReq.WriteBuffers = make([][]byte, 0, len(entries)*2) // headers + data

                // Allocate header buffer for this specific request (no sharing)
                requiredSize := len(entries) * headerSize
                allHeaders := make([]byte, requiredSize)

                // Store initial state for rollback if write fails
                initialEntryNumber := s.index.CurrentEntryNumber
                initialWriteOffset := s.index.CurrentWriteOffset
                initialWritesSinceCheckpoint := s.writesSinceCheckpoint

                // Single-process mode: fast path with immediate updates
                if !config.Concurrency.EnableMultiProcessMode </span><span class="cov8" title="1">{
                        for i, compressedEntry := range compressedEntries </span><span class="cov8" title="1">{
                                totalOriginalBytes += compressedEntry.OriginalSize
                                totalCompressedBytes += compressedEntry.CompressedSize

                                if compressedEntry.WasCompressed </span><span class="cov8" title="1">{
                                        compressedCount++
                                }</span> else<span class="cov8" title="1"> {
                                        skippedCount++
                                }</span>

                                // Use slice of pre-allocated header buffer
                                <span class="cov8" title="1">headerStart := i * headerSize
                                header := allHeaders[headerStart : headerStart+headerSize]
                                binary.LittleEndian.PutUint32(header[0:4], uint32(len(compressedEntry.Data)))
                                binary.LittleEndian.PutUint64(header[4:12], uint64(now))

                                // Add to vectored write batch
                                writeReq.WriteBuffers = append(writeReq.WriteBuffers, header, compressedEntry.Data)

                                // Track entry in binary index - only store at intervals for memory efficiency
                                entryNumber := s.index.CurrentEntryNumber
                                entrySize := int64(headerSize + len(compressedEntry.Data))

                                // Calculate the correct file index and offset for this entry
                                fileIndex := len(s.index.Files) - 1
                                byteOffset := s.index.CurrentWriteOffset

                                // If this entry will cause rotation and we're not at the start of a file,
                                // it will be written to the NEXT file at offset 0
                                willCauseRotation := s.index.CurrentWriteOffset+entrySize &gt; config.Storage.MaxFileSize
                                if willCauseRotation &amp;&amp; s.index.CurrentWriteOffset &gt; 0 </span><span class="cov8" title="1">{
                                        fileIndex = len(s.index.Files) // Will be the next file after rotation
                                        byteOffset = 0
                                }</span>

                                <span class="cov8" title="1">position := EntryPosition{
                                        FileIndex:  fileIndex,
                                        ByteOffset: byteOffset,
                                }

                                // Add to binary searchable index (it handles intervals and pruning internally)
                                s.index.BinaryIndex.AddIndexNode(entryNumber, position)

                                // Generate ID for this entry
                                writeReq.IDs[i] = MessageID{EntryNumber: entryNumber, ShardID: s.shardID}

                                // Update index state (will be rolled back if write fails)
                                s.index.CurrentEntryNumber++
                                s.index.CurrentWriteOffset += entrySize
                                s.writesSinceCheckpoint++</span>
                        }
                } else<span class="cov8" title="1"> {
                        // Multi-process mode: use atomic sequence for entry numbers
                        // Track binary index updates for deferred application
                        type binaryIndexUpdate struct {
                                entryNumber int64
                                position    EntryPosition
                        }
                        var binaryIndexUpdates []binaryIndexUpdate

                        // Pre-allocate entry numbers atomically
                        entryNumbers := make([]int64, len(entries))
                        if s.sequenceState != nil </span><span class="cov8" title="1">{
                                for i := range entries </span><span class="cov8" title="1">{
                                        oldVal := atomic.LoadInt64(&amp;s.sequenceState.LastEntryNumber)
                                        newVal := atomic.AddInt64(&amp;s.sequenceState.LastEntryNumber, 1)
                                        entryNumbers[i] = newVal - 1
                                        if false </span><span class="cov0" title="0">{ // Enable for debugging
                                                log.Printf("Shard %d: seq %d-&gt;%d, allocated entry %d",
                                                        s.shardID, oldVal, newVal, entryNumbers[i])
                                        }</span>
                                }
                        } else<span class="cov0" title="0"> {
                                // Fallback if sequence state not initialized
                                baseEntry := s.index.CurrentEntryNumber
                                for i := range entries </span><span class="cov0" title="0">{
                                        entryNumbers[i] = baseEntry + int64(i)
                                }</span>
                        }

                        // Track write position
                        <span class="cov8" title="1">writeOffset := s.index.CurrentWriteOffset

                        for i, compressedEntry := range compressedEntries </span><span class="cov8" title="1">{
                                totalOriginalBytes += compressedEntry.OriginalSize
                                totalCompressedBytes += compressedEntry.CompressedSize

                                if compressedEntry.WasCompressed </span><span class="cov8" title="1">{
                                        compressedCount++
                                }</span> else<span class="cov8" title="1"> {
                                        skippedCount++
                                }</span>

                                // Use slice of pre-allocated header buffer
                                <span class="cov8" title="1">headerStart := i * headerSize
                                header := allHeaders[headerStart : headerStart+headerSize]
                                binary.LittleEndian.PutUint32(header[0:4], uint32(len(compressedEntry.Data)))
                                binary.LittleEndian.PutUint64(header[4:12], uint64(now))

                                // Add to vectored write batch
                                writeReq.WriteBuffers = append(writeReq.WriteBuffers, header, compressedEntry.Data)

                                // Track entry in binary index - only store at intervals for memory efficiency
                                entrySize := int64(headerSize + len(compressedEntry.Data))
                                entryNumber := entryNumbers[i]

                                // Calculate the correct file index and offset for this entry
                                fileIndex := len(s.index.Files) - 1
                                byteOffset := writeOffset

                                // If this entry will cause rotation and we're not at the start of a file,
                                // it will be written to the NEXT file at offset 0
                                willCauseRotation := writeOffset+entrySize &gt; config.Storage.MaxFileSize
                                if willCauseRotation &amp;&amp; writeOffset &gt; 0 </span><span class="cov8" title="1">{
                                        fileIndex = len(s.index.Files) // Will be the next file after rotation
                                        byteOffset = 0
                                }</span>

                                <span class="cov8" title="1">position := EntryPosition{
                                        FileIndex:  fileIndex,
                                        ByteOffset: byteOffset,
                                }

                                // Store for later update
                                binaryIndexUpdates = append(binaryIndexUpdates, binaryIndexUpdate{
                                        entryNumber: entryNumber,
                                        position:    position,
                                })

                                // Generate ID for this entry
                                writeReq.IDs[i] = MessageID{EntryNumber: entryNumber, ShardID: s.shardID}

                                // Track write position for next entry
                                writeOffset += entrySize</span>
                        }

                        // Multi-process mode: set up deferred update function
                        // Calculate final entry number (highest allocated + 1)
                        <span class="cov8" title="1">finalEntryNumber := int64(0)
                        if len(entryNumbers) &gt; 0 </span><span class="cov8" title="1">{
                                finalEntryNumber = entryNumbers[len(entryNumbers)-1] + 1
                        }</span>
                        <span class="cov8" title="1">finalWriteOffset := writeOffset
                        writeReq.UpdateFunc = func() error </span><span class="cov8" title="1">{
                                s.mu.Lock()
                                defer s.mu.Unlock()

                                // Apply the binary index updates
                                for _, update := range binaryIndexUpdates </span><span class="cov8" title="1">{
                                        s.index.BinaryIndex.AddIndexNode(update.entryNumber, update.position)
                                }</span>

                                // Apply the index updates
                                <span class="cov8" title="1">s.index.CurrentEntryNumber = finalEntryNumber
                                s.index.CurrentWriteOffset = finalWriteOffset
                                s.writesSinceCheckpoint = initialWritesSinceCheckpoint + len(entries)

                                // Update the current file's entry count and end offset
                                if len(s.index.Files) &gt; 0 </span><span class="cov8" title="1">{
                                        s.index.Files[len(s.index.Files)-1].Entries += int64(len(entries))
                                        s.index.Files[len(s.index.Files)-1].EndOffset = finalWriteOffset
                                }</span>

                                // Update mmap state to notify readers
                                <span class="cov8" title="1">if s.mmapState != nil </span><span class="cov8" title="1">{
                                        s.updateMmapState()
                                }</span>

                                // Schedule async checkpoint instead of synchronous
                                <span class="cov8" title="1">if s.writesSinceCheckpoint &gt; 0 &amp;&amp; time.Since(s.lastCheckpoint) &gt; 10*time.Millisecond </span><span class="cov8" title="1">{
                                        s.scheduleAsyncCheckpoint(clientMetrics, config)
                                }</span>

                                <span class="cov8" title="1">return nil</span>
                        }
                }

                // Define rollback function in case write fails (only for single-process mode)
                <span class="cov8" title="1">if !config.Concurrency.EnableMultiProcessMode </span><span class="cov8" title="1">{
                        writeReq.UpdateFunc = func() error </span><span class="cov0" title="0">{
                                // On failure, rollback index state
                                s.mu.Lock()
                                defer s.mu.Unlock()
                                s.index.CurrentEntryNumber = initialEntryNumber
                                s.index.CurrentWriteOffset = initialWriteOffset
                                s.writesSinceCheckpoint = initialWritesSinceCheckpoint
                                return nil
                        }</span>
                }
        }() // End of critical section

        // Perform I/O OUTSIDE the lock
        <span class="cov8" title="1">var writeErr error

        // Use memory-mapped writer for ultra-fast multi-process writes if available
        if config.Concurrency.EnableMultiProcessMode &amp;&amp; s.mmapWriter != nil </span><span class="cov8" title="1">{
                // Extract entry numbers from IDs
                entryNumbers := make([]int64, len(writeReq.IDs))
                for i, id := range writeReq.IDs </span><span class="cov8" title="1">{
                        entryNumbers[i] = id.EntryNumber
                }</span>

                // Extract raw data from write buffers (skip headers, they will be recreated)
                <span class="cov8" title="1">rawEntries := make([][]byte, len(writeReq.IDs))
                for i := 0; i &lt; len(writeReq.IDs); i++ </span><span class="cov8" title="1">{
                        // WriteBuffers contains [header, data, header, data, ...]
                        // Skip the header (index i*2) and get the data (index i*2+1)
                        rawEntries[i] = writeReq.WriteBuffers[i*2+1]
                }</span>

                // Memory-mapped write (handles its own coordination)
                <span class="cov8" title="1">writeErr = s.mmapWriter.Write(rawEntries, entryNumbers)

                // Handle rotation needed error
                if writeErr != nil &amp;&amp; strings.Contains(writeErr.Error(), "rotation needed") </span><span class="cov8" title="1">{
                        // Must acquire mutex before rotating to avoid race conditions
                        s.mu.Lock()
                        rotErr := s.rotateFile(clientMetrics, config)
                        s.mu.Unlock()

                        if rotErr != nil </span><span class="cov0" title="0">{
                                return nil, fmt.Errorf("failed to rotate file: %w", rotErr)
                        }</span>

                        // Retry the write after rotation
                        <span class="cov8" title="1">writeErr = s.mmapWriter.Write(rawEntries, entryNumbers)</span>
                }

                // Update index after successful mmap write - must be protected by mutex
                <span class="cov8" title="1">if writeErr == nil </span><span class="cov8" title="1">{
                        s.mu.Lock()
                        s.index.CurrentWriteOffset = s.mmapWriter.CoordinationState().WriteOffset.Load()
                        s.mu.Unlock()
                }</span>
        } else<span class="cov8" title="1"> {
                // Regular write path with file locking
                if config.Concurrency.EnableMultiProcessMode &amp;&amp; s.lockFile != nil </span><span class="cov0" title="0">{
                        if err := syscall.Flock(int(s.lockFile.Fd()), syscall.LOCK_EX); err != nil </span><span class="cov0" title="0">{
                                return nil, fmt.Errorf("failed to acquire shard lock for write: %w", err)
                        }</span>
                        <span class="cov0" title="0">defer syscall.Flock(int(s.lockFile.Fd()), syscall.LOCK_UN)</span>
                }

                <span class="cov8" title="1">s.writeMu.Lock()
                // Write all buffers
                for _, buf := range writeReq.WriteBuffers </span><span class="cov8" title="1">{
                        if _, err := s.writer.Write(buf); err != nil </span><span class="cov0" title="0">{
                                writeErr = err
                                break</span>
                        }
                }
                <span class="cov8" title="1">if writeErr == nil </span><span class="cov8" title="1">{
                        writeErr = s.writer.Flush()
                        // In multi-process mode, sync to ensure data hits disk before releasing lock
                        if writeErr == nil &amp;&amp; config.Concurrency.EnableMultiProcessMode &amp;&amp; s.dataFile != nil </span><span class="cov0" title="0">{
                                writeErr = s.dataFile.Sync()
                        }</span>
                }
                <span class="cov8" title="1">s.writeMu.Unlock()</span>
        }

        // Handle write error
        <span class="cov8" title="1">if writeErr != nil </span><span class="cov0" title="0">{
                // Track error
                clientMetrics.ErrorCount.Add(1)
                clientMetrics.LastErrorNano.Store(uint64(time.Now().UnixNano()))

                // Rollback index state
                if writeReq.UpdateFunc != nil </span><span class="cov0" title="0">{
                        writeReq.UpdateFunc()
                }</span>

                <span class="cov0" title="0">return nil, fmt.Errorf("failed write: %w", writeErr)</span>
        }

        // Apply deferred updates for multi-process mode BEFORE post-write operations
        <span class="cov8" title="1">if config.Concurrency.EnableMultiProcessMode &amp;&amp; writeReq.UpdateFunc != nil </span><span class="cov8" title="1">{
                if err := writeReq.UpdateFunc(); err != nil </span><span class="cov0" title="0">{
                        return nil, fmt.Errorf("failed to update index after write: %w", err)
                }</span>
        }

        // Track metrics
        <span class="cov8" title="1">writeLatency := uint64(time.Since(startTime).Nanoseconds())
        clientMetrics.TotalEntries.Add(uint64(len(entries)))
        clientMetrics.TotalBytes.Add(totalOriginalBytes)
        clientMetrics.TotalCompressed.Add(totalCompressedBytes)
        clientMetrics.CompressedEntries.Add(compressedCount)
        clientMetrics.SkippedCompression.Add(skippedCount)

        // Update latency metrics using EMA
        if totalOriginalBytes &gt; 0 &amp;&amp; totalCompressedBytes &gt; 0 </span><span class="cov8" title="1">{
                ratio := (totalOriginalBytes * 100) / totalCompressedBytes // x100 for fixed point
                clientMetrics.CompressionRatio.Store(ratio)
        }</span>

        // Track write latency with min/max
        <span class="cov8" title="1">clientMetrics.WriteLatencyNano.Store(writeLatency)
        for </span><span class="cov8" title="1">{
                currentMin := clientMetrics.MinWriteLatency.Load()
                if currentMin == 0 || writeLatency &lt; currentMin </span><span class="cov8" title="1">{
                        if clientMetrics.MinWriteLatency.CompareAndSwap(currentMin, writeLatency) </span><span class="cov8" title="1">{
                                break</span>
                        }
                } else<span class="cov8" title="1"> {
                        break</span>
                }
        }
        <span class="cov8" title="1">for </span><span class="cov8" title="1">{
                currentMax := clientMetrics.MaxWriteLatency.Load()
                if writeLatency &gt; currentMax </span><span class="cov8" title="1">{
                        if clientMetrics.MaxWriteLatency.CompareAndSwap(currentMax, writeLatency) </span><span class="cov8" title="1">{
                                break</span>
                        }
                } else<span class="cov8" title="1"> {
                        break</span>
                }
        }

        // Post-write operations
        <span class="cov8" title="1">func() </span><span class="cov8" title="1">{
                s.mu.Lock()
                defer s.mu.Unlock()

                // In single-process mode, update file entry count after successful write
                // (Multi-process mode updates this in the UpdateFunc)
                if !config.Concurrency.EnableMultiProcessMode &amp;&amp; len(s.index.Files) &gt; 0 </span><span class="cov8" title="1">{
                        currentFile := &amp;s.index.Files[len(s.index.Files)-1]
                        currentFile.Entries += int64(len(entries))
                }</span>

                // In multi-process mode, update mmap state immediately for visibility
                <span class="cov8" title="1">if config.Concurrency.EnableMultiProcessMode &amp;&amp; s.mmapState != nil </span><span class="cov8" title="1">{
                        // Fast path: Update coordination state immediately (~10ns total)
                        atomic.StoreInt64(&amp;s.mmapState.LastUpdateNanos, time.Now().UnixNano())
                        if s.sequenceState != nil </span><span class="cov8" title="1">{
                                atomic.StoreInt64(&amp;s.sequenceState.LastEntryNumber, s.index.CurrentEntryNumber)
                        }</span>

                        // Note: Async checkpointing happens in UpdateFunc now
                } else<span class="cov8" title="1"> {
                        // Single-process mode: use regular checkpointing
                        s.maybeCheckpoint(clientMetrics, config)
                }</span>

                // Check if we need to rotate file
                <span class="cov8" title="1">if s.index.CurrentWriteOffset &gt; config.Storage.MaxFileSize </span><span class="cov8" title="1">{
                        if err := s.rotateFile(clientMetrics, config); err != nil </span><span class="cov0" title="0">{
                                panic(err)</span> // Will be caught by deferred recover
                        }
                }
        }()

        <span class="cov8" title="1">return writeReq.IDs, nil</span>
}

// maybeCheckpoint persists the index if needed
func (s *Shard) maybeCheckpoint(clientMetrics *ClientMetrics, config *CometConfig) <span class="cov8" title="1">{
        if time.Since(s.lastCheckpoint) &gt; time.Duration(config.Storage.CheckpointTime)*time.Millisecond </span><span class="cov8" title="1">{

                // Flush and sync writer
                if s.writer != nil </span><span class="cov8" title="1">{
                        s.writeMu.Lock()
                        s.writer.Flush()
                        if s.dataFile != nil </span><span class="cov8" title="1">{
                                s.dataFile.Sync()
                        }</span>
                        <span class="cov8" title="1">s.writeMu.Unlock()</span>
                }

                // Update current file info
                <span class="cov8" title="1">if len(s.index.Files) &gt; 0 </span><span class="cov8" title="1">{
                        current := &amp;s.index.Files[len(s.index.Files)-1]
                        current.EndOffset = s.index.CurrentWriteOffset
                        current.EndTime = time.Now()
                }</span>

                // Clone index while holding lock (caller holds the lock)
                <span class="cov8" title="1">indexCopy := s.cloneIndex()
                s.writesSinceCheckpoint = 0
                s.lastCheckpoint = time.Now()

                // Persist index in background to avoid blocking
                s.wg.Add(1)
                go func() </span><span class="cov8" title="1">{
                        defer s.wg.Done()

                        // For multi-process safety, use the separate index lock if enabled
                        var err error
                        if config.Concurrency.EnableMultiProcessMode &amp;&amp; s.indexLockFile != nil </span><span class="cov0" title="0">{
                                // Acquire exclusive lock for index writes
                                if err := syscall.Flock(int(s.indexLockFile.Fd()), syscall.LOCK_EX); err != nil </span><span class="cov0" title="0">{
                                        if clientMetrics != nil </span><span class="cov0" title="0">{
                                                clientMetrics.IndexPersistErrors.Add(1)
                                                clientMetrics.ErrorCount.Add(1)
                                                clientMetrics.LastErrorNano.Store(uint64(time.Now().UnixNano()))
                                        }</span>
                                        <span class="cov0" title="0">return</span>
                                }

                                // Reload index from disk to merge with other processes' changes
                                <span class="cov0" title="0">s.indexMu.Lock()
                                if _, statErr := os.Stat(s.indexPath); statErr == nil </span><span class="cov0" title="0">{
                                        // Index exists, load it to get latest state
                                        if diskIndex, loadErr := s.loadBinaryIndex(); loadErr == nil </span><span class="cov0" title="0">{
                                                // Merge our changes with the disk state
                                                // Keep the highest entry number and write offset
                                                if diskIndex.CurrentEntryNumber &gt; indexCopy.CurrentEntryNumber </span><span class="cov0" title="0">{
                                                        indexCopy.CurrentEntryNumber = diskIndex.CurrentEntryNumber
                                                }</span>
                                                <span class="cov0" title="0">if diskIndex.CurrentWriteOffset &gt; indexCopy.CurrentWriteOffset </span><span class="cov0" title="0">{
                                                        indexCopy.CurrentWriteOffset = diskIndex.CurrentWriteOffset
                                                }</span>

                                                // Merge consumer offsets - keep the highest offset for each consumer
                                                <span class="cov0" title="0">for group, offset := range diskIndex.ConsumerOffsets </span><span class="cov0" title="0">{
                                                        if currentOffset, exists := indexCopy.ConsumerOffsets[group]; !exists || offset &gt; currentOffset </span><span class="cov0" title="0">{
                                                                indexCopy.ConsumerOffsets[group] = offset
                                                        }</span>
                                                }

                                                // Merge file info - use the disk version as base
                                                <span class="cov0" title="0">if len(diskIndex.Files) &gt; 0 </span><span class="cov0" title="0">{
                                                        indexCopy.Files = diskIndex.Files
                                                }</span>

                                                // Merge binary index nodes if needed
                                                <span class="cov0" title="0">if diskIndex.BinaryIndex.Nodes != nil &amp;&amp; len(diskIndex.BinaryIndex.Nodes) &gt; len(indexCopy.BinaryIndex.Nodes) </span><span class="cov0" title="0">{
                                                        indexCopy.BinaryIndex = diskIndex.BinaryIndex
                                                }</span>
                                        }
                                }

                                // Now save the merged index
                                <span class="cov0" title="0">err = s.saveBinaryIndex(indexCopy)
                                s.indexMu.Unlock()

                                // Update mmap state BEFORE releasing lock
                                if err == nil </span><span class="cov0" title="0">{
                                        s.updateMmapState()
                                }</span>

                                // Release lock immediately
                                <span class="cov0" title="0">syscall.Flock(int(s.indexLockFile.Fd()), syscall.LOCK_UN)</span>
                        } else<span class="cov8" title="1"> {
                                // No file locking - just use process-local mutex
                                s.indexMu.Lock()
                                err = s.saveBinaryIndex(indexCopy)
                                s.indexMu.Unlock()

                                if err == nil </span><span class="cov8" title="1">{
                                        s.updateMmapState()
                                }</span>
                        }

                        <span class="cov8" title="1">if err != nil </span><span class="cov0" title="0">{
                                // Track error in metrics - next checkpoint will retry
                                if clientMetrics != nil </span><span class="cov0" title="0">{
                                        clientMetrics.IndexPersistErrors.Add(1)
                                }</span>
                        }
                }()

                // Track checkpoint metrics (only if metrics are available)
                <span class="cov8" title="1">if clientMetrics != nil </span><span class="cov0" title="0">{
                        clientMetrics.CheckpointCount.Add(1)
                        clientMetrics.LastCheckpoint.Store(uint64(time.Now().UnixNano()))
                }</span>
        }
}

// loadIndexWithRetry loads the index with retry logic for EOF errors
// This handles the race condition between file writes and mmap state updates
func (s *Shard) loadIndexWithRetry() error <span class="cov8" title="1">{
        var err error
        for attempt := 0; attempt &lt; 3; attempt++ </span><span class="cov8" title="1">{
                err = s.loadIndex()
                if err == nil </span><span class="cov8" title="1">{
                        return nil
                }</span>
                // Retry on EOF and file size errors to handle race conditions
                <span class="cov0" title="0">if (strings.Contains(err.Error(), "unexpected EOF") ||
                        strings.Contains(err.Error(), "index file too small")) &amp;&amp; attempt &lt; 2 </span><span class="cov0" title="0">{
                        time.Sleep(time.Duration(attempt+1) * time.Millisecond) // 1ms, 2ms
                        continue</span>
                }
                <span class="cov0" title="0">break</span>
        }
        <span class="cov0" title="0">return err</span>
}

// lazyRebuildIndexIfNeeded checks if the index needs rebuilding based on file sizes
// This is called on read path in multi-process mode to ensure consistency
// Caller must hold the shard lock
// checkIfRebuildNeeded checks if the index needs rebuilding without acquiring locks
// Must be called while holding at least a read lock
func (s *Shard) checkIfRebuildNeeded() bool <span class="cov0" title="0">{
        if s.mmapWriter == nil || s.mmapWriter.CoordinationState() == nil </span><span class="cov0" title="0">{
                return false
        }</span>

        <span class="cov0" title="0">coordState := s.mmapWriter.CoordinationState()
        currentTotalWrites := coordState.TotalWrites.Load()

        // Check if total writes in coordination state exceed what we have in index
        return currentTotalWrites &gt; s.index.CurrentEntryNumber</span>
}

func (s *Shard) lazyRebuildIndexIfNeeded(config CometConfig, shardDir string) <span class="cov0" title="0">{
        if len(s.index.Files) == 0 </span><span class="cov0" title="0">{
                return
        }</span>

        // Check if any file has grown beyond what the index knows
        <span class="cov0" title="0">needsRebuild := false
        for _, file := range s.index.Files </span><span class="cov0" title="0">{
                if stat, err := os.Stat(file.Path); err == nil </span><span class="cov0" title="0">{
                        actualSize := stat.Size()
                        if actualSize &gt; file.EndOffset </span><span class="cov0" title="0">{
                                needsRebuild = true
                                break</span>
                        }
                }
        }

        <span class="cov0" title="0">if !needsRebuild </span><span class="cov0" title="0">{
                return
        }</span>

        // Rebuild the index by scanning ALL files
        // In multi-process mode, we can't trust entry counts from partial indexes
        <span class="cov0" title="0">totalEntries := int64(0)
        newBinaryNodes := make([]EntryIndexNode, 0)
        currentEntryNum := int64(0)

        for i := range s.index.Files </span><span class="cov0" title="0">{
                file := &amp;s.index.Files[i]

                // Get actual file size
                stat, err := os.Stat(file.Path)
                if err != nil </span><span class="cov0" title="0">{
                        continue</span>
                }
                <span class="cov0" title="0">actualSize := stat.Size()

                // In multi-process mode, read the shared coordination state to get actual data size
                scanSize := actualSize
                if config.Concurrency.EnableMultiProcessMode &amp;&amp; i == len(s.index.Files)-1 </span><span class="cov0" title="0">{
                        // For the current file, check the coordination state
                        coordPath := filepath.Join(shardDir, "coordination.state")
                        if coordFile, err := os.Open(coordPath); err == nil </span><span class="cov0" title="0">{
                                defer coordFile.Close()

                                // Map the coordination state temporarily to read it
                                const stateSize = 256
                                if data, err := syscall.Mmap(int(coordFile.Fd()), 0, stateSize,
                                        syscall.PROT_READ, syscall.MAP_SHARED); err == nil </span><span class="cov0" title="0">{
                                        defer syscall.Munmap(data)

                                        // Read the write offset from the shared state
                                        // In multi-process scenarios, retry a few times to handle timing issues
                                        coordState := (*MmapCoordinationState)(unsafe.Pointer(&amp;data[0]))
                                        var writeOffset int64
                                        maxRetries := 3

                                        for retry := 0; retry &lt; maxRetries; retry++ </span><span class="cov0" title="0">{
                                                writeOffset = coordState.WriteOffset.Load()
                                                lastWrite := coordState.LastWriteNanos.Load()
                                                totalWrites := coordState.TotalWrites.Load()

                                                // Check if we expect more writes than we see in the offset
                                                // If TotalWrites is significantly higher than what the offset suggests, wait for coordination to stabilize
                                                averageBytesPerWrite := int64(50) // Rough estimate
                                                expectedMinOffset := totalWrites * averageBytesPerWrite

                                                if totalWrites &gt; 100 &amp;&amp; writeOffset &lt; expectedMinOffset/2 </span><span class="cov0" title="0">{
                                                        time.Sleep(100 * time.Millisecond)
                                                        continue</span>
                                                }

                                                // If we see a recent write timestamp, the offset should be stable
                                                <span class="cov0" title="0">if lastWrite &gt; 0 &amp;&amp; time.Since(time.Unix(0, lastWrite)) &lt; 100*time.Millisecond </span><span class="cov0" title="0">{
                                                        break</span>
                                                }

                                                // If this is our first try and offset seems small, wait a bit
                                                <span class="cov0" title="0">if retry == 0 &amp;&amp; writeOffset &lt; 1000 </span><span class="cov0" title="0">{ // Less than 1KB suggests incomplete writes
                                                        time.Sleep(50 * time.Millisecond)
                                                        continue</span>
                                                }

                                                <span class="cov0" title="0">break</span>
                                        }

                                        // Get the latest coordination state values for stabilization check
                                        <span class="cov0" title="0">latestTotalWrites := coordState.TotalWrites.Load()

                                        // Wait for coordination state to stabilize if we're in a high-contention scenario
                                        if latestTotalWrites &gt; 100 </span><span class="cov0" title="0">{

                                                // Wait for a brief period to let any pending writes complete
                                                stableTotalWrites := latestTotalWrites
                                                stableWriteOffset := writeOffset

                                                for attempts := 0; attempts &lt; 10; attempts++ </span><span class="cov0" title="0">{
                                                        time.Sleep(50 * time.Millisecond)
                                                        currentWrites := coordState.TotalWrites.Load()
                                                        currentOffset := coordState.WriteOffset.Load()

                                                        if currentWrites == stableTotalWrites &amp;&amp; currentOffset == stableWriteOffset </span><span class="cov0" title="0">{
                                                                // Coordination state is stable
                                                                break</span>
                                                        }

                                                        <span class="cov0" title="0">stableTotalWrites = currentWrites
                                                        stableWriteOffset = currentOffset</span>
                                                }

                                                // Use the stabilized values
                                                <span class="cov0" title="0">writeOffset = stableWriteOffset</span>
                                        }

                                        // In multi-process mode, if we have a large pre-allocated file (&gt;100KB),
                                        // scan the entire file rather than trusting coordination state for scanning bounds
                                        // This handles cases where coordination state updates are delayed
                                        <span class="cov0" title="0">if actualSize &gt; 100*1024 </span><span class="cov0" title="0">{ // File is larger than 100KB - likely pre-allocated
                                                scanSize = actualSize
                                        }</span> else<span class="cov0" title="0"> if writeOffset &gt; 0 &amp;&amp; writeOffset &lt; actualSize </span><span class="cov0" title="0">{
                                                scanSize = writeOffset
                                        }</span> else<span class="cov0" title="0"> if writeOffset &gt;= actualSize </span>{<span class="cov0" title="0">
                                                // Use file scan - this handles large pre-allocated files
                                        }</span>
                                }
                        } else<span class="cov0" title="0"> if s.mmapWriter != nil </span><span class="cov0" title="0">{
                                // Fallback to local mmapWriter if available
                                coordState := s.mmapWriter.CoordinationState()
                                writeOffset := coordState.WriteOffset.Load()
                                if writeOffset &gt; 0 &amp;&amp; writeOffset &lt; actualSize </span><span class="cov0" title="0">{
                                        scanSize = writeOffset
                                }</span>
                        }
                }

                // Always scan the entire file in multi-process mode
                // Don't trust the entry count from the partial index
                <span class="cov0" title="0">scanResult := s.scanFileForEntries(file.Path, scanSize, i, currentEntryNum)

                if scanResult.entryCount &gt; 0 </span><span class="cov0" title="0">{
                        file.Entries = scanResult.entryCount
                        file.EndOffset = actualSize
                        newBinaryNodes = append(newBinaryNodes, scanResult.indexNodes...)
                        totalEntries += scanResult.entryCount
                        currentEntryNum += scanResult.entryCount
                }</span>
        }

        // Update index state
        <span class="cov0" title="0">if totalEntries &gt; s.index.CurrentEntryNumber </span><span class="cov0" title="0">{
                s.index.CurrentEntryNumber = totalEntries
        }</span>

        // Update binary index if we found new nodes
        <span class="cov0" title="0">if len(newBinaryNodes) &gt; 0 </span><span class="cov0" title="0">{
                // Merge with existing nodes
                nodeMap := make(map[int64]EntryIndexNode)
                for _, node := range s.index.BinaryIndex.Nodes </span><span class="cov0" title="0">{
                        nodeMap[node.EntryNumber] = node
                }</span>
                <span class="cov0" title="0">for _, node := range newBinaryNodes </span><span class="cov0" title="0">{
                        nodeMap[node.EntryNumber] = node
                }</span>

                // Convert back to sorted slice
                <span class="cov0" title="0">s.index.BinaryIndex.Nodes = make([]EntryIndexNode, 0, len(nodeMap))
                for _, node := range nodeMap </span><span class="cov0" title="0">{
                        s.index.BinaryIndex.Nodes = append(s.index.BinaryIndex.Nodes, node)
                }</span>
                <span class="cov0" title="0">sort.Slice(s.index.BinaryIndex.Nodes, func(i, j int) bool </span><span class="cov0" title="0">{
                        return s.index.BinaryIndex.Nodes[i].EntryNumber &lt; s.index.BinaryIndex.Nodes[j].EntryNumber
                }</span>)
        }

        // Update write offset to match last file
        <span class="cov0" title="0">if len(s.index.Files) &gt; 0 </span><span class="cov0" title="0">{
                lastFile := &amp;s.index.Files[len(s.index.Files)-1]
                s.index.CurrentWriteOffset = lastFile.EndOffset
        }</span>
}

// scanFileResult holds the results of scanning a data file
type scanFileResult struct {
        entryCount int64
        indexNodes []EntryIndexNode
}

// scanFileForEntries scans a data file to count entries and rebuild index
// This is used in multi-process mode to ensure we capture all entries
func (s *Shard) scanFileForEntries(filePath string, fileSize int64, fileIndex int, startEntryNum int64) scanFileResult <span class="cov0" title="0">{
        f, err := os.Open(filePath)
        if err != nil </span><span class="cov0" title="0">{
                return scanFileResult{}
        }</span>
        <span class="cov0" title="0">defer f.Close()

        result := scanFileResult{
                indexNodes: make([]EntryIndexNode, 0),
        }

        var offset int64
        var entryNum int64 = startEntryNum
        interval := s.index.BinaryIndex.IndexInterval
        if interval &lt;= 0 </span><span class="cov0" title="0">{
                interval = 100 // Default interval
        }</span>

        <span class="cov0" title="0">for offset &lt; fileSize </span><span class="cov0" title="0">{
                // Read header
                headerBuf := make([]byte, headerSize)
                n, err := f.ReadAt(headerBuf, offset)
                if err != nil || n != headerSize </span><span class="cov0" title="0">{
                        break</span>
                }

                // Parse header
                <span class="cov0" title="0">length := binary.LittleEndian.Uint32(headerBuf[0:4])
                timestamp := binary.LittleEndian.Uint64(headerBuf[4:12])

                // Check for uninitialized memory (zeros) - AGGRESSIVE GAP SKIPPING
                if length == 0 &amp;&amp; timestamp == 0 </span><span class="cov0" title="0">{

                        found := false

                        // NUCLEAR APPROACH: Search every 4 bytes until we find a valid header
                        // This ensures we NEVER miss entries due to gaps
                        for searchOffset := offset + 4; searchOffset &lt;= fileSize-headerSize; searchOffset += 4 </span><span class="cov0" title="0">{
                                searchBuf := make([]byte, headerSize)
                                if n, err := f.ReadAt(searchBuf, searchOffset); err == nil &amp;&amp; n == headerSize </span><span class="cov0" title="0">{
                                        searchLength := binary.LittleEndian.Uint32(searchBuf[0:4])
                                        searchTimestamp := binary.LittleEndian.Uint64(searchBuf[4:12])

                                        // More lenient validation for gap recovery
                                        if searchLength &gt; 0 &amp;&amp; searchLength &lt;= 10*1024*1024 &amp;&amp; searchTimestamp &gt; 0 </span><span class="cov0" title="0">{
                                                // Found a valid entry!
                                                offset = searchOffset
                                                found = true
                                                break</span>
                                        }
                                }
                        }

                        <span class="cov0" title="0">if !found </span><span class="cov0" title="0">{
                                break</span>
                        }

                        // Continue scanning from the recovered position
                        <span class="cov0" title="0">continue</span>
                }

                // Validate entry bounds
                <span class="cov0" title="0">if length == 0 || length &gt; 100*1024*1024 </span><span class="cov0" title="0">{ // Sanity check: max 100MB per entry
                        break</span>
                }

                // Check if full entry fits in file
                <span class="cov0" title="0">nextOffset := offset + headerSize + int64(length)
                if nextOffset &gt; fileSize </span><span class="cov0" title="0">{
                        break</span>
                }

                // Add to binary index at intervals
                <span class="cov0" title="0">if entryNum%int64(interval) == 0 </span><span class="cov0" title="0">{
                        result.indexNodes = append(result.indexNodes, EntryIndexNode{
                                EntryNumber: entryNum,
                                Position: EntryPosition{
                                        FileIndex:  fileIndex,
                                        ByteOffset: offset,
                                },
                        })
                }</span>

                <span class="cov0" title="0">result.entryCount++
                entryNum++
                offset = nextOffset</span>
        }

        <span class="cov0" title="0">return result</span>
}

// scheduleAsyncCheckpoint schedules an asynchronous checkpoint for multi-process mode
// This allows writes to return immediately while index persistence happens in background
func (s *Shard) scheduleAsyncCheckpoint(clientMetrics *ClientMetrics, config *CometConfig) <span class="cov8" title="1">{
        // Clone index while holding lock (caller holds the lock)
        indexCopy := s.cloneIndex()
        s.writesSinceCheckpoint = 0
        s.lastCheckpoint = time.Now()

        // Persist index in background to avoid blocking writes
        s.wg.Add(1)
        go func() </span><span class="cov8" title="1">{
                defer s.wg.Done()

                // For multi-process safety, use the separate index lock
                if s.indexLockFile != nil </span><span class="cov8" title="1">{
                        // Acquire exclusive lock for index writes
                        if err := syscall.Flock(int(s.indexLockFile.Fd()), syscall.LOCK_EX); err != nil </span><span class="cov0" title="0">{
                                if clientMetrics != nil </span><span class="cov0" title="0">{
                                        clientMetrics.IndexPersistErrors.Add(1)
                                        clientMetrics.ErrorCount.Add(1)
                                        clientMetrics.LastErrorNano.Store(uint64(time.Now().UnixNano()))
                                }</span>
                                <span class="cov0" title="0">return</span>
                        }
                        <span class="cov8" title="1">defer syscall.Flock(int(s.indexLockFile.Fd()), syscall.LOCK_UN)</span>
                }

                // Serialize index writes to prevent file corruption
                <span class="cov8" title="1">s.indexMu.Lock()
                err := s.saveBinaryIndex(indexCopy)
                s.indexMu.Unlock()

                if err != nil </span><span class="cov0" title="0">{
                        // Track error in metrics - next checkpoint will retry
                        if clientMetrics != nil </span><span class="cov0" title="0">{
                                clientMetrics.IndexPersistErrors.Add(1)
                                clientMetrics.ErrorCount.Add(1)
                                clientMetrics.LastErrorNano.Store(uint64(time.Now().UnixNano()))
                        }</span>
                } else<span class="cov8" title="1"> {
                        // Track checkpoint metrics
                        if clientMetrics != nil </span><span class="cov8" title="1">{
                                clientMetrics.CheckpointCount.Add(1)
                                clientMetrics.LastCheckpoint.Store(uint64(time.Now().UnixNano()))
                        }</span>
                }
        }()
}

// cloneIndex creates a deep copy of the index for safe serialization
// IMPORTANT: Caller must hold the shard mutex (either read or write lock)
func (s *Shard) cloneIndex() *ShardIndex <span class="cov8" title="1">{
        // Create a deep copy of the index
        clone := &amp;ShardIndex{
                CurrentEntryNumber: s.index.CurrentEntryNumber,
                CurrentWriteOffset: s.index.CurrentWriteOffset,
                BoundaryInterval:   s.index.BoundaryInterval,
                CurrentFile:        s.index.CurrentFile,
        }

        // Deep copy maps

        clone.ConsumerOffsets = make(map[string]int64, len(s.index.ConsumerOffsets))
        for k, v := range s.index.ConsumerOffsets </span><span class="cov8" title="1">{
                clone.ConsumerOffsets[k] = v
        }</span>

        // Deep copy binary index
        <span class="cov8" title="1">clone.BinaryIndex.IndexInterval = s.index.BinaryIndex.IndexInterval
        clone.BinaryIndex.MaxNodes = s.index.BinaryIndex.MaxNodes
        clone.BinaryIndex.Nodes = make([]EntryIndexNode, len(s.index.BinaryIndex.Nodes))
        copy(clone.BinaryIndex.Nodes, s.index.BinaryIndex.Nodes)

        // Deep copy files
        clone.Files = make([]FileInfo, len(s.index.Files))
        copy(clone.Files, s.index.Files)

        return clone</span>
}

// persistIndex atomically writes the index to disk
// IMPORTANT: Caller must hold the shard mutex (either read or write lock)
func (s *Shard) persistIndex() error <span class="cov8" title="1">{
        // Clone the index - caller already holds lock
        indexCopy := s.cloneIndex()

        // Use binary format for efficiency
        err := s.saveBinaryIndex(indexCopy)
        if err == nil </span><span class="cov8" title="1">{
                // Only update mmap state after successful persistence
                s.updateMmapState()
        }</span>
        <span class="cov8" title="1">return err</span>
}

// discoverDataFiles scans the shard directory for all data files
// This is critical for multi-process mode where other processes may have created files
func (s *Shard) discoverDataFiles() error <span class="cov8" title="1">{
        shardDir := filepath.Dir(s.indexPath)
        entries, err := os.ReadDir(shardDir)
        if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to read shard directory: %w", err)
        }</span>

        // Collect all data files
        <span class="cov8" title="1">var dataFiles []string
        for _, entry := range entries </span><span class="cov8" title="1">{
                if entry.IsDir() </span><span class="cov0" title="0">{
                        continue</span>
                }
                <span class="cov8" title="1">name := entry.Name()
                if strings.HasPrefix(name, "log-") &amp;&amp; strings.HasSuffix(name, ".comet") </span><span class="cov8" title="1">{
                        dataFiles = append(dataFiles, filepath.Join(shardDir, name))
                }</span>
        }

        <span class="cov8" title="1">if len(dataFiles) == 0 </span><span class="cov0" title="0">{
                return nil // No files yet
        }</span>

        // Sort files by name (which includes sequence number)
        <span class="cov8" title="1">sort.Strings(dataFiles)

        // Build file info for any files not already in our index
        existingFiles := make(map[string]bool)
        for _, f := range s.index.Files </span><span class="cov8" title="1">{
                existingFiles[f.Path] = true
        }</span>

        // Add any newly discovered files
        <span class="cov8" title="1">for _, filePath := range dataFiles </span><span class="cov8" title="1">{
                if !existingFiles[filePath] </span><span class="cov8" title="1">{
                        // Get file info
                        info, err := os.Stat(filePath)
                        if err != nil </span><span class="cov0" title="0">{
                                continue</span> // Skip files we can't stat
                        }

                        // Add to index with basic info
                        // The actual entry count and offsets will be updated during scanning
                        <span class="cov8" title="1">s.index.Files = append(s.index.Files, FileInfo{
                                Path:        filePath,
                                StartOffset: 0,
                                EndOffset:   info.Size(),
                                StartTime:   info.ModTime(),
                                EndTime:     info.ModTime(),
                                Entries:     0, // Will be updated during scan
                        })</span>
                }
        }

        // Update current file to the latest one
        <span class="cov8" title="1">if len(dataFiles) &gt; 0 </span><span class="cov8" title="1">{
                s.index.CurrentFile = dataFiles[len(dataFiles)-1]
        }</span>

        <span class="cov8" title="1">return nil</span>
}

// loadIndex loads the index from disk
func (s *Shard) loadIndex() error <span class="cov8" title="1">{
        // In multi-process mode, always scan for new files first
        if s.lockFile != nil </span><span class="cov8" title="1">{ // Multi-process mode indicator
                if err := s.discoverDataFiles(); err != nil </span><span class="cov0" title="0">{
                        return fmt.Errorf("failed to discover data files: %w", err)
                }</span>
        }

        // Check if index file exists
        <span class="cov8" title="1">if _, err := os.Stat(s.indexPath); os.IsNotExist(err) </span><span class="cov8" title="1">{
                // No index yet, keep the already configured index
                if s.index == nil </span><span class="cov0" title="0">{
                        // Only create default if no index was configured
                        s.index = &amp;ShardIndex{
                                BoundaryInterval: defaultBoundaryInterval,
                                ConsumerOffsets:  make(map[string]int64),
                        }
                }</span>
                <span class="cov8" title="1">return nil</span>
        }

        // Save current config values before loading
        <span class="cov8" title="1">boundaryInterval := s.index.BoundaryInterval
        maxIndexEntries := s.index.BinaryIndex.MaxNodes

        // Load binary index
        index, err := s.loadBinaryIndex()
        if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to load binary index: %w", err)
        }</span>

        <span class="cov8" title="1">s.index = index
        // Restore config values
        s.index.BoundaryInterval = boundaryInterval
        s.index.BinaryIndex.IndexInterval = boundaryInterval
        s.index.BinaryIndex.MaxNodes = maxIndexEntries

        // In multi-process mode, verify and update file sizes
        if s.lockFile != nil </span><span class="cov8" title="1">{
                for i := range s.index.Files </span><span class="cov8" title="1">{
                        file := &amp;s.index.Files[i]
                        if info, err := os.Stat(file.Path); err == nil </span><span class="cov8" title="1">{
                                actualSize := info.Size()
                                if actualSize &gt; file.EndOffset </span><span class="cov8" title="1">{
                                        file.EndOffset = actualSize
                                }</span>
                        }
                }

                // Don't update current write offset based on file size
                // The index already has the correct write offset from when it was saved
                // For mmap files, the file size doesn't reflect the actual data written
        }

        <span class="cov8" title="1">return nil</span>
}

// initMmapState initializes the memory-mapped shared state for multi-process coordination
func (s *Shard) initMmapState() error <span class="cov8" title="1">{
        // Create or open the mmap state file (8 bytes)
        file, err := os.OpenFile(s.indexStatePath, os.O_CREATE|os.O_RDWR, 0644)
        if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to create mmap state file: %w", err)
        }</span>
        <span class="cov8" title="1">defer file.Close()

        // Ensure file is exactly 8 bytes
        stat, err := file.Stat()
        if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to stat mmap state file: %w", err)
        }</span>

        <span class="cov8" title="1">if stat.Size() == 0 </span><span class="cov8" title="1">{
                // New file - initialize with current timestamp
                now := time.Now().UnixNano()
                if err := binary.Write(file, binary.LittleEndian, now); err != nil </span><span class="cov0" title="0">{
                        return fmt.Errorf("failed to initialize mmap state: %w", err)
                }</span>
        } else<span class="cov8" title="1"> if stat.Size() != 8 </span><span class="cov0" title="0">{
                return fmt.Errorf("mmap state file has invalid size %d, expected 8", stat.Size())
        }</span>

        // Memory-map the file
        <span class="cov8" title="1">data, err := syscall.Mmap(int(file.Fd()), 0, 8, syscall.PROT_READ|syscall.PROT_WRITE, syscall.MAP_SHARED)
        if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to mmap state file: %w", err)
        }</span>

        <span class="cov8" title="1">s.indexStateData = data
        s.mmapState = (*MmapSharedState)(unsafe.Pointer(&amp;data[0]))

        // Initialize our last seen timestamp
        s.lastMmapCheck = atomic.LoadInt64(&amp;s.mmapState.LastUpdateNanos)

        return nil</span>
}

// initSequenceState initializes the memory-mapped sequence counter for file naming
func (s *Shard) initSequenceState() error <span class="cov8" title="1">{
        // Create or open the sequence state file (8 bytes)
        file, err := os.OpenFile(s.sequenceStatePath, os.O_CREATE|os.O_RDWR, 0644)
        if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to create sequence state file: %w", err)
        }</span>
        <span class="cov8" title="1">s.sequenceFile = file

        // Ensure file is exactly 8 bytes
        stat, err := file.Stat()
        if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to stat sequence state file: %w", err)
        }</span>

        <span class="cov8" title="1">if stat.Size() == 0 </span><span class="cov8" title="1">{
                // New file - initialize with current entry number from index
                // This ensures multi-process coordination starts from the right point
                initialSeq := s.index.CurrentEntryNumber
                // In multi-process mode, we want entries to start from 1, not 0
                // This matches the behavior of traditional file-based sequence numbering
                if initialSeq == 0 &amp;&amp; s.index.CurrentFile == "" </span><span class="cov8" title="1">{
                        initialSeq = 1
                }</span>
                <span class="cov8" title="1">if err := binary.Write(file, binary.LittleEndian, initialSeq); err != nil </span><span class="cov0" title="0">{
                        return fmt.Errorf("failed to initialize sequence state: %w", err)
                }</span>
        } else<span class="cov8" title="1"> if stat.Size() != 8 </span><span class="cov0" title="0">{
                return fmt.Errorf("sequence state file has invalid size %d, expected 8", stat.Size())
        }</span>

        // Memory-map the file
        <span class="cov8" title="1">data, err := syscall.Mmap(int(file.Fd()), 0, 8, syscall.PROT_READ|syscall.PROT_WRITE, syscall.MAP_SHARED)
        if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to mmap sequence state file: %w", err)
        }</span>

        <span class="cov8" title="1">s.sequenceStateData = data
        s.sequenceCounter = (*int64)(unsafe.Pointer(&amp;data[0]))

        // Also set up the SequenceState pointer for entry number tracking
        s.sequenceState = (*SequenceState)(unsafe.Pointer(&amp;data[0]))

        // For now, we're using the same counter for files and entries
        // This means entry numbers will start from 1 if a file has been created
        // TODO: Use separate counters for files and entries

        return nil</span>
}

// getNextSequence atomically increments and returns the next sequence number for file naming
func (s *Shard) getNextSequence() int64 <span class="cov8" title="1">{
        if s.sequenceCounter != nil </span><span class="cov8" title="1">{
                return atomic.AddInt64(s.sequenceCounter, 1)
        }</span>
        // Fallback to timestamp if sequence counter not available (single process mode)
        <span class="cov8" title="1">return time.Now().Unix()</span>
}

// updateMmapState atomically updates the shared timestamp to notify other processes of index changes
func (s *Shard) updateMmapState() <span class="cov8" title="1">{
        // Check if mmapState is still valid (not nil and not unmapped)
        if s.mmapState != nil &amp;&amp; s.indexStateData != nil </span><span class="cov8" title="1">{
                atomic.StoreInt64(&amp;s.mmapState.LastUpdateNanos, time.Now().UnixNano())
        }</span>
}

// openDataFileWithConfig opens the current data file for appending with optional config for multi-process safety
func (s *Shard) openDataFileWithConfig(shardDir string, config *CometConfig) error <span class="cov8" title="1">{
        // For multi-process safety, acquire exclusive lock when creating files
        if config != nil &amp;&amp; config.Concurrency.EnableMultiProcessMode &amp;&amp; s.lockFile != nil </span><span class="cov8" title="1">{
                if err := syscall.Flock(int(s.lockFile.Fd()), syscall.LOCK_EX); err != nil </span><span class="cov0" title="0">{
                        return fmt.Errorf("failed to acquire lock for file creation: %w", err)
                }</span>
                <span class="cov8" title="1">defer syscall.Flock(int(s.lockFile.Fd()), syscall.LOCK_UN)</span>
        }

        <span class="cov8" title="1">if s.index.CurrentFile == "" </span><span class="cov8" title="1">{
                // Create first file with sequential number
                seqNum := s.getNextSequence()
                s.index.CurrentFile = filepath.Join(shardDir, fmt.Sprintf("log-%016d.comet", seqNum))
                s.index.Files = append(s.index.Files, FileInfo{
                        Path:        s.index.CurrentFile,
                        StartOffset: 0,
                        StartEntry:  0,
                        StartTime:   time.Now(),
                        Entries:     0,
                })
        }</span>

        // Open file for appending
        <span class="cov8" title="1">file, err := os.OpenFile(s.index.CurrentFile, os.O_APPEND|os.O_CREATE|os.O_WRONLY, 0644)
        if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to open data file: %w", err)
        }</span>

        <span class="cov8" title="1">s.dataFile = file

        // Create buffered writer
        // In multi-process mode, use smaller buffer to reduce conflicts
        bufSize := defaultBufSize
        if s.lockFile != nil </span><span class="cov8" title="1">{ // Multi-process mode
                bufSize = 8192 // 8KB buffer for more frequent flushes
        }</span>
        <span class="cov8" title="1">s.writer = bufio.NewWriterSize(s.dataFile, bufSize)

        // Set up compressor with fastest level for better throughput
        enc, err := zstd.NewWriter(nil, zstd.WithEncoderLevel(zstd.SpeedFastest))
        if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to create compressor: %w", err)
        }</span>
        <span class="cov8" title="1">s.compressor = enc

        return nil</span>
}

// recoverFromCrash scans from the last checkpoint to find actual EOF
func (s *Shard) recoverFromCrash() error <span class="cov8" title="1">{
        info, err := s.dataFile.Stat()
        if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to stat data file: %w", err)
        }</span>

        <span class="cov8" title="1">actualSize := info.Size()
        if actualSize &lt;= s.index.CurrentWriteOffset </span><span class="cov8" title="1">{
                s.index.CurrentWriteOffset = actualSize
                return nil
        }</span>

        // Scan forward from index offset to validate entries
        <span class="cov8" title="1">offset := s.index.CurrentWriteOffset
        validEntries := int64(0)
        var lastEntryOffset int64

        // Open file for reading
        f, err := os.Open(s.index.CurrentFile)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov8" title="1">defer f.Close()

        // Scan entries until EOF or corruption
        for offset &lt; actualSize </span><span class="cov8" title="1">{
                if offset+headerSize &gt; actualSize </span><span class="cov0" title="0">{
                        break</span> // Incomplete header
                }

                <span class="cov8" title="1">header := make([]byte, headerSize)
                if _, err := f.ReadAt(header, offset); err != nil </span><span class="cov0" title="0">{
                        break</span>
                }

                <span class="cov8" title="1">length := binary.LittleEndian.Uint32(header[0:4])
                if offset+headerSize+int64(length) &gt; actualSize </span><span class="cov0" title="0">{
                        break</span> // Incomplete entry
                }

                // Skip zero-length entries (likely uninitialized file regions)
                <span class="cov8" title="1">if length == 0 </span><span class="cov8" title="1">{
                        // Check if this is just zeros (uninitialized data)
                        timestamp := binary.LittleEndian.Uint64(header[4:12])
                        if timestamp == 0 </span><span class="cov8" title="1">{
                                // This is uninitialized data, not a real entry
                                break</span>
                        }
                }

                // Entry looks valid
                <span class="cov8" title="1">lastEntryOffset = offset
                offset += headerSize + int64(length)
                validEntries++</span>
        }

        // Update state
        <span class="cov8" title="1">s.index.CurrentWriteOffset = offset
        s.index.CurrentEntryNumber += validEntries

        // Update the current file's entry count
        if len(s.index.Files) &gt; 0 &amp;&amp; validEntries &gt; 0 </span><span class="cov8" title="1">{
                s.index.Files[len(s.index.Files)-1].Entries += validEntries
        }</span>

        // Store the last valid entry position in binary index if needed
        <span class="cov8" title="1">if validEntries &gt; 0 </span><span class="cov8" title="1">{
                s.index.BinaryIndex.AddIndexNode(s.index.CurrentEntryNumber-1, EntryPosition{
                        FileIndex:  len(s.index.Files) - 1,
                        ByteOffset: lastEntryOffset,
                })
        }</span>

        <span class="cov8" title="1">return nil</span>
}

// rotateFile closes current file and starts a new one
// NOTE: This method assumes the caller holds s.mu (main shard mutex)
func (s *Shard) rotateFile(clientMetrics *ClientMetrics, config *CometConfig) error <span class="cov8" title="1">{
        // Handle mmap writer rotation
        if s.mmapWriter != nil </span><span class="cov8" title="1">{
                // Let mmap writer handle its own file rotation
                if err := s.mmapWriter.rotateFile(); err != nil </span><span class="cov0" title="0">{
                        return fmt.Errorf("failed to rotate mmap file: %w", err)
                }</span>

                // Update index to reflect new file from mmap writer
                // The mmap writer will have updated its internal path
                <span class="cov8" title="1">shardDir := filepath.Dir(s.index.CurrentFile)
                seqNum := s.getNextSequence()
                newPath := filepath.Join(shardDir, fmt.Sprintf("log-%016d.comet", seqNum))
                s.index.CurrentFile = newPath

                // Add new file to index
                s.index.Files = append(s.index.Files, FileInfo{
                        Path:        newPath,
                        StartOffset: 0, // Mmap files always start at 0
                        StartEntry:  s.index.CurrentEntryNumber,
                        StartTime:   time.Now(),
                        Entries:     0,
                })

                // Track file rotation
                clientMetrics.FileRotations.Add(1)
                clientMetrics.TotalFiles.Add(1)

                return nil</span>
        }

        // Regular file writer rotation - acquire write lock to ensure no writes are in progress
        <span class="cov8" title="1">s.writeMu.Lock()
        defer s.writeMu.Unlock()

        // Regular file writer rotation
        // Close direct writer
        if s.writer != nil </span><span class="cov8" title="1">{
                s.writer.Flush()
        }</span>

        // Close compressor
        <span class="cov8" title="1">if s.compressor != nil </span><span class="cov8" title="1">{
                s.compressor.Close()
        }</span>

        // Close current file (safe now that we hold writeMu)
        <span class="cov8" title="1">if err := s.dataFile.Close(); err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to close data file: %w", err)
        }</span>

        // Update final stats for current file
        <span class="cov8" title="1">if len(s.index.Files) &gt; 0 </span><span class="cov8" title="1">{
                current := &amp;s.index.Files[len(s.index.Files)-1]
                current.EndOffset = s.index.CurrentWriteOffset
                current.EndTime = time.Now()
        }</span>

        // For multi-process safety, acquire exclusive lock when creating new file
        <span class="cov8" title="1">if config.Concurrency.EnableMultiProcessMode &amp;&amp; s.lockFile != nil </span><span class="cov0" title="0">{
                if err := syscall.Flock(int(s.lockFile.Fd()), syscall.LOCK_EX); err != nil </span><span class="cov0" title="0">{
                        return fmt.Errorf("failed to acquire lock for file rotation: %w", err)
                }</span>
                <span class="cov0" title="0">defer syscall.Flock(int(s.lockFile.Fd()), syscall.LOCK_UN)</span>
        }

        // Create new file with sequential number
        <span class="cov8" title="1">shardDir := filepath.Dir(s.index.CurrentFile)
        seqNum := s.getNextSequence()
        s.index.CurrentFile = filepath.Join(shardDir, fmt.Sprintf("log-%016d.comet", seqNum))

        // Add new file to index
        s.index.Files = append(s.index.Files, FileInfo{
                Path:        s.index.CurrentFile,
                StartOffset: s.index.CurrentWriteOffset,
                StartEntry:  s.index.CurrentEntryNumber,
                StartTime:   time.Now(),
                Entries:     0,
        })

        // Track file rotation
        clientMetrics.FileRotations.Add(1)
        clientMetrics.TotalFiles.Add(1)

        // Open with preallocation and setup
        return s.openDataFileWithConfig(shardDir, config)</span>
}

// parseShardFromStream extracts shard ID from stream name
func parseShardFromStream(stream string) (uint32, error) <span class="cov8" title="1">{
        // Expected format: "events:v1:shard:0042"
        // Use strings.LastIndex to find the last colon, then parse what follows
        lastColonIdx := -1
        for i := len(stream) - 1; i &gt;= 0; i-- </span><span class="cov8" title="1">{
                if stream[i] == ':' </span><span class="cov8" title="1">{
                        lastColonIdx = i
                        break</span>
                }
        }

        <span class="cov8" title="1">if lastColonIdx == -1 || lastColonIdx == len(stream)-1 </span><span class="cov0" title="0">{
                return 0, fmt.Errorf("invalid stream format: missing shard number")
        }</span>

        // Parse the number after the last colon
        <span class="cov8" title="1">numStr := stream[lastColonIdx+1:]
        var shardID uint32

        // Manual parsing to avoid fmt.Sscanf overhead
        for _, char := range numStr </span><span class="cov8" title="1">{
                if char &lt; '0' || char &gt; '9' </span><span class="cov0" title="0">{
                        return 0, fmt.Errorf("invalid shard number: contains non-digit")
                }</span>
                <span class="cov8" title="1">digit := uint32(char - '0')
                if shardID &gt; (^uint32(0)-digit)/10 </span><span class="cov0" title="0">{
                        return 0, fmt.Errorf("invalid shard number: overflow")
                }</span>
                <span class="cov8" title="1">shardID = shardID*10 + digit</span>
        }

        <span class="cov8" title="1">return shardID, nil</span>
}

// Close gracefully shuts down the client
func (c *Client) Close() error <span class="cov8" title="1">{
        c.mu.Lock()
        defer c.mu.Unlock()

        if c.closed </span><span class="cov8" title="1">{
                return nil
        }</span>

        <span class="cov8" title="1">c.closed = true

        // Stop retention manager
        if c.stopCh != nil </span><span class="cov8" title="1">{
                close(c.stopCh)
                c.retentionWg.Wait()
        }</span>

        // Close all shards
        <span class="cov8" title="1">for _, shard := range c.shards </span><span class="cov8" title="1">{
                shard.mu.Lock()

                // Final checkpoint - pass nil for metrics since we're shutting down
                // Use the actual config for final checkpoint
                shard.maybeCheckpoint(nil, &amp;c.config)

                // Acquire write lock to ensure no writes are in progress
                shard.writeMu.Lock()

                // Close direct writer
                if shard.writer != nil </span><span class="cov8" title="1">{
                        shard.writer.Flush()
                }</span>

                // Close compressor
                <span class="cov8" title="1">if shard.compressor != nil </span><span class="cov8" title="1">{
                        shard.compressor.Close()
                }</span>

                // Close file (safe now that we hold writeMu)
                <span class="cov8" title="1">if shard.dataFile != nil </span><span class="cov8" title="1">{
                        shard.dataFile.Close()
                }</span>

                // Close memory-mapped writer
                <span class="cov8" title="1">if shard.mmapWriter != nil </span><span class="cov8" title="1">{
                        shard.mmapWriter.Close()
                }</span>

                <span class="cov8" title="1">shard.writeMu.Unlock()
                shard.mu.Unlock()

                // Wait for background operations to complete BEFORE closing files
                shard.wg.Wait()

                // Now safe to close lock files (no more background goroutines using them)
                shard.mu.Lock()
                if shard.lockFile != nil </span><span class="cov8" title="1">{
                        shard.lockFile.Close()
                }</span>
                <span class="cov8" title="1">if shard.indexLockFile != nil </span><span class="cov8" title="1">{
                        shard.indexLockFile.Close()
                }</span>
                <span class="cov8" title="1">if shard.sequenceFile != nil </span><span class="cov8" title="1">{
                        shard.sequenceFile.Close()
                }</span>
                <span class="cov8" title="1">shard.mu.Unlock()

                // Now safe to unmap shared state
                if shard.indexStateData != nil </span><span class="cov8" title="1">{
                        syscall.Munmap(shard.indexStateData)
                        shard.indexStateData = nil
                        shard.mmapState = nil
                }</span>
                <span class="cov8" title="1">if shard.sequenceStateData != nil </span><span class="cov8" title="1">{
                        syscall.Munmap(shard.sequenceStateData)
                        shard.sequenceStateData = nil
                        shard.sequenceCounter = nil
                }</span>
        }

        <span class="cov8" title="1">return nil</span>
}

// GetStats returns current metrics for monitoring
func (c *Client) GetStats() CometStats <span class="cov8" title="1">{
        var totalReaders uint64
        var maxLag uint64
        var totalFiles uint64

        // Aggregate stats from all shards
        c.mu.RLock()
        for _, shard := range c.shards </span><span class="cov8" title="1">{
                readerCount := atomic.LoadInt64(&amp;shard.readerCount)
                totalReaders += uint64(readerCount)

                shard.mu.RLock()
                totalFiles += uint64(len(shard.index.Files))

                // Calculate max consumer lag in ENTRIES (not bytes!)
                for _, consumerEntry := range shard.index.ConsumerOffsets </span><span class="cov0" title="0">{
                        lag := uint64(shard.index.CurrentEntryNumber - consumerEntry)
                        if lag &gt; maxLag </span><span class="cov0" title="0">{
                                maxLag = lag
                        }</span>
                }
                <span class="cov8" title="1">shard.mu.RUnlock()</span>
        }
        <span class="cov8" title="1">c.mu.RUnlock()

        // Update aggregated metrics
        c.metrics.ActiveReaders.Store(totalReaders)
        c.metrics.ConsumerLag.Store(maxLag) // This is now entry lag, not byte lag
        c.metrics.TotalFiles.Store(totalFiles)

        return CometStats{
                TotalEntries:        c.metrics.TotalEntries.Load(),
                TotalBytes:          c.metrics.TotalBytes.Load(),
                TotalCompressed:     c.metrics.TotalCompressed.Load(),
                WriteLatencyNano:    c.metrics.WriteLatencyNano.Load(),
                MinWriteLatency:     c.metrics.MinWriteLatency.Load(),
                MaxWriteLatency:     c.metrics.MaxWriteLatency.Load(),
                CompressionRatio:    c.metrics.CompressionRatio.Load(),
                CompressedEntries:   c.metrics.CompressedEntries.Load(),
                SkippedCompression:  c.metrics.SkippedCompression.Load(),
                TotalFiles:          totalFiles,
                FileRotations:       c.metrics.FileRotations.Load(),
                CheckpointCount:     c.metrics.CheckpointCount.Load(),
                LastCheckpoint:      c.metrics.LastCheckpoint.Load(),
                ActiveReaders:       totalReaders,
                ConsumerLag:         maxLag,
                ErrorCount:          c.metrics.ErrorCount.Load(),
                LastErrorNano:       c.metrics.LastErrorNano.Load(),
                IndexPersistErrors:  c.metrics.IndexPersistErrors.Load(),
                CompressionWaitNano: c.metrics.CompressionWait.Load(),
        }</span>
}
</pre>
		
		<pre class="file" id="file1" style="display: none">package main

import (
        "context"
        "encoding/json"
        "flag"
        "fmt"
        "log"
        "os"
        "time"

        "github.com/orbiterhq/comet"
)

func main() <span class="cov0" title="0">{
        var (
                mode     = flag.String("mode", "writer", "Mode: writer, reader, or benchmark")
                dir      = flag.String("dir", "", "Data directory")
                id       = flag.String("id", "", "Process ID")
                duration = flag.Duration("duration", 10*time.Second, "How long to run")
        )
        flag.Parse()

        if *dir == "" </span><span class="cov0" title="0">{
                log.Fatal("--dir is required")
        }</span>

        <span class="cov0" title="0">switch *mode </span>{
        case "writer":<span class="cov0" title="0">
                runWriter(*dir, *id, *duration)</span>
        case "reader":<span class="cov0" title="0">
                runReader(*dir, *id, *duration)</span>
        case "benchmark":<span class="cov0" title="0">
                runBenchmark(*dir, *id, *duration)</span>
        default:<span class="cov0" title="0">
                log.Fatalf("unknown mode: %s", *mode)</span>
        }
}

func runWriter(dir, id string, duration time.Duration) <span class="cov0" title="0">{
        config := comet.MultiProcessConfig()
        client, err := comet.NewClientWithConfig(dir, config)
        if err != nil </span><span class="cov0" title="0">{
                log.Fatalf("failed to create client: %v", err)
        }</span>
        <span class="cov0" title="0">defer client.Close()

        ctx, cancel := context.WithTimeout(context.Background(), duration)
        defer cancel()

        streamName := "test:v1:shard:0001"
        ticker := time.NewTicker(10 * time.Millisecond)
        defer ticker.Stop()

        count := 0
        for </span><span class="cov0" title="0">{
                select </span>{
                case &lt;-ctx.Done():<span class="cov0" title="0">
                        log.Printf("Writer %s completed: wrote %d entries", id, count)
                        return</span>
                case &lt;-ticker.C:<span class="cov0" title="0">
                        // Write a batch of entries
                        batch := make([][]byte, 10)
                        for i := 0; i &lt; 10; i++ </span><span class="cov0" title="0">{
                                entry := map[string]interface{}{
                                        "writer_id": id,
                                        "sequence":  count + i,
                                        "timestamp": time.Now().UnixNano(),
                                        "data":      fmt.Sprintf("test data from %s entry %d", id, count+i),
                                }
                                data, _ := json.Marshal(entry)
                                batch[i] = data
                        }</span>

                        <span class="cov0" title="0">if _, err := client.Append(ctx, streamName, batch); err != nil </span><span class="cov0" title="0">{
                                log.Printf("Writer %s append error: %v", id, err)
                        }</span> else<span class="cov0" title="0"> {
                                count += 10
                        }</span>
                }
        }
}

func runReader(dir, id string, duration time.Duration) <span class="cov0" title="0">{
        config := comet.MultiProcessConfig()
        client, err := comet.NewClientWithConfig(dir, config)
        if err != nil </span><span class="cov0" title="0">{
                log.Fatalf("failed to create client: %v", err)
        }</span>
        <span class="cov0" title="0">defer client.Close()

        consumer := comet.NewConsumer(client, comet.ConsumerOptions{
                Group: fmt.Sprintf("reader-%s", id),
        })
        defer consumer.Close()

        ctx, cancel := context.WithTimeout(context.Background(), duration)
        defer cancel()

        totalRead := 0
        latencies := []time.Duration{}

        for ctx.Err() == nil </span><span class="cov0" title="0">{
                start := time.Now()
                messages, err := consumer.Read(ctx, []uint32{1}, 100)
                if err != nil </span><span class="cov0" title="0">{
                        log.Printf("Reader %s error: %v", id, err)
                        continue</span>
                }

                <span class="cov0" title="0">latency := time.Since(start)
                if len(messages) &gt; 0 </span><span class="cov0" title="0">{
                        latencies = append(latencies, latency)
                        totalRead += len(messages)

                        // Verify data integrity
                        for _, msg := range messages </span><span class="cov0" title="0">{
                                var data map[string]interface{}
                                if err := json.Unmarshal(msg.Data, &amp;data); err != nil </span><span class="cov0" title="0">{
                                        log.Printf("Reader %s: corrupted message: %v", id, err)
                                }</span>
                        }

                        // Ack messages
                        <span class="cov0" title="0">for _, msg := range messages </span><span class="cov0" title="0">{
                                consumer.Ack(ctx, msg.ID)
                        }</span>
                } else<span class="cov0" title="0"> {
                        time.Sleep(10 * time.Millisecond)
                }</span>
        }

        // Calculate average latency
        <span class="cov0" title="0">var avgLatency time.Duration
        if len(latencies) &gt; 0 </span><span class="cov0" title="0">{
                var total time.Duration
                for _, l := range latencies </span><span class="cov0" title="0">{
                        total += l
                }</span>
                <span class="cov0" title="0">avgLatency = total / time.Duration(len(latencies))</span>
        }

        <span class="cov0" title="0">log.Printf("Reader %s completed: read %d entries, avg latency: %v", id, totalRead, avgLatency)</span>
}

func runBenchmark(dir, id string, duration time.Duration) <span class="cov0" title="0">{
        config := comet.MultiProcessConfig()
        client, err := comet.NewClientWithConfig(dir, config)
        if err != nil </span><span class="cov0" title="0">{
                log.Fatalf("failed to create client: %v", err)
        }</span>
        <span class="cov0" title="0">defer client.Close()

        ctx, cancel := context.WithTimeout(context.Background(), duration)
        defer cancel()

        streamName := fmt.Sprintf("bench:v1:shard:%04x", os.Getpid()%16)

        // Prepare batch
        batch := make([][]byte, 100)
        for i := 0; i &lt; 100; i++ </span><span class="cov0" title="0">{
                entry := map[string]interface{}{
                        "writer_id": id,
                        "sequence":  i,
                        "timestamp": time.Now().UnixNano(),
                        "data":      fmt.Sprintf("benchmark data entry %d with some padding to make it realistic size", i),
                }
                data, _ := json.Marshal(entry)
                batch[i] = data
        }</span>

        <span class="cov0" title="0">count := 0
        start := time.Now()

        for ctx.Err() == nil </span><span class="cov0" title="0">{
                if _, err := client.Append(ctx, streamName, batch); err != nil </span><span class="cov0" title="0">{
                        log.Printf("Benchmark %s append error: %v", id, err)
                }</span> else<span class="cov0" title="0"> {
                        count += len(batch)
                }</span>
        }

        <span class="cov0" title="0">elapsed := time.Since(start)
        rate := float64(count) / elapsed.Seconds()
        log.Printf("Benchmark %s completed: wrote %d entries in %v (%.0f entries/sec)", id, count, elapsed, rate)</span>
}
</pre>
		
		<pre class="file" id="file2" style="display: none">package comet

import (
        "context"
        "encoding/binary"
        "fmt"
        "maps"
        "path/filepath"
        "slices"
        "strings"
        "sync"
        "sync/atomic"
        "time"
)

// MessageID represents a structured message ID
// Fields ordered for optimal memory alignment: int64 first, then uint32
type MessageID struct {
        EntryNumber int64  `json:"entry_number"`
        ShardID     uint32 `json:"shard_id"`
}

// String returns the string representation of the ID (ShardID-EntryNumber format)
func (id MessageID) String() string <span class="cov8" title="1">{
        return fmt.Sprintf("%d-%d", id.ShardID, id.EntryNumber)
}</span>

// ParseMessageID parses a string ID back to MessageID
func ParseMessageID(str string) (MessageID, error) <span class="cov0" title="0">{
        var shardID uint32
        var entryNumber int64
        if _, err := fmt.Sscanf(str, "%d-%d", &amp;shardID, &amp;entryNumber); err != nil </span><span class="cov0" title="0">{
                return MessageID{}, fmt.Errorf("invalid message ID format: %w", err)
        }</span>
        <span class="cov0" title="0">return MessageID{EntryNumber: entryNumber, ShardID: shardID}, nil</span>
}

// StreamMessage represents a message read from a stream
type StreamMessage struct {
        Stream string    // Stream name/identifier
        ID     MessageID // Unique message ID
        Data   []byte    // Raw message data
}

// Consumer reads from comet stream shards
// Fields ordered for optimal memory alignment
type Consumer struct {
        // Pointers first (8 bytes on 64-bit)
        client *Client

        // Composite types
        readers sync.Map // Cached readers per shard (optimized for read-heavy workload)

        // Strings last
        group string
}

// ConsumerOptions configures a consumer
type ConsumerOptions struct {
        Group string
}

// ProcessFunc handles a batch of messages, returning error to trigger retry
type ProcessFunc func(messages []StreamMessage) error

// ProcessOption configures the Process method
type ProcessOption func(*processConfig)

// processConfig holds internal configuration built from options
type processConfig struct {
        // Core processing
        handler ProcessFunc
        autoAck bool

        // Callbacks
        onError func(err error, retryCount int)
        onBatch func(size int, duration time.Duration)

        // Behavior
        batchSize    int
        maxRetries   int
        pollInterval time.Duration
        retryDelay   time.Duration

        // Sharding
        stream        string
        shards        []uint32
        consumerID    int
        consumerCount int
}

// WithStream specifies a stream pattern for shard discovery
func WithStream(pattern string) ProcessOption <span class="cov8" title="1">{
        return func(cfg *processConfig) </span><span class="cov8" title="1">{
                cfg.stream = pattern
        }</span>
}

// WithShards specifies explicit shards to process
func WithShards(shards ...uint32) ProcessOption <span class="cov8" title="1">{
        return func(cfg *processConfig) </span><span class="cov8" title="1">{
                cfg.shards = shards
        }</span>
}

// WithBatchSize sets the number of messages to read at once
func WithBatchSize(size int) ProcessOption <span class="cov8" title="1">{
        return func(cfg *processConfig) </span><span class="cov8" title="1">{
                cfg.batchSize = size
        }</span>
}

// WithMaxRetries sets the number of retry attempts for failed batches
func WithMaxRetries(retries int) ProcessOption <span class="cov8" title="1">{
        return func(cfg *processConfig) </span><span class="cov8" title="1">{
                cfg.maxRetries = retries
        }</span>
}

// WithPollInterval sets how long to wait when no messages are available
func WithPollInterval(interval time.Duration) ProcessOption <span class="cov0" title="0">{
        return func(cfg *processConfig) </span><span class="cov0" title="0">{
                cfg.pollInterval = interval
        }</span>
}

// WithRetryDelay sets the base delay between retries
func WithRetryDelay(delay time.Duration) ProcessOption <span class="cov8" title="1">{
        return func(cfg *processConfig) </span><span class="cov8" title="1">{
                cfg.retryDelay = delay
        }</span>
}

// WithAutoAck controls automatic acknowledgment (default: true)
func WithAutoAck(enabled bool) ProcessOption <span class="cov8" title="1">{
        return func(cfg *processConfig) </span><span class="cov8" title="1">{
                cfg.autoAck = enabled
        }</span>
}

// WithErrorHandler sets a callback for processing errors
func WithErrorHandler(handler func(err error, retryCount int)) ProcessOption <span class="cov8" title="1">{
        return func(cfg *processConfig) </span><span class="cov8" title="1">{
                cfg.onError = handler
        }</span>
}

// WithBatchCallback sets a callback after each batch completes
func WithBatchCallback(callback func(size int, duration time.Duration)) ProcessOption <span class="cov0" title="0">{
        return func(cfg *processConfig) </span><span class="cov0" title="0">{
                cfg.onBatch = callback
        }</span>
}

// WithConsumerAssignment configures distributed processing
func WithConsumerAssignment(id, total int) ProcessOption <span class="cov0" title="0">{
        return func(cfg *processConfig) </span><span class="cov0" title="0">{
                cfg.consumerID = id
                cfg.consumerCount = total
        }</span>
}

// buildProcessConfig applies options and defaults
func buildProcessConfig(handler ProcessFunc, opts []ProcessOption) *processConfig <span class="cov8" title="1">{
        cfg := &amp;processConfig{
                handler:       handler,
                autoAck:       true,
                batchSize:     100,
                maxRetries:    3,
                pollInterval:  100 * time.Millisecond,
                retryDelay:    time.Second,
                consumerCount: 1,
                consumerID:    0,
        }

        for _, opt := range opts </span><span class="cov8" title="1">{
                opt(cfg)
        }</span>

        // If no stream or shards specified, discover all shards
        <span class="cov8" title="1">if cfg.stream == "" &amp;&amp; len(cfg.shards) == 0 </span><span class="cov0" title="0">{
                cfg.stream = "*:*:*:*" // Match any stream pattern
        }</span>

        <span class="cov8" title="1">return cfg</span>
}

// NewConsumer creates a new consumer for comet streams
func NewConsumer(client *Client, opts ConsumerOptions) *Consumer <span class="cov8" title="1">{
        return &amp;Consumer{
                client: client,
                group:  opts.Group,
                // readers sync.Map is zero-initialized and ready to use
        }
}</span>

// Close closes the consumer and releases all resources
func (c *Consumer) Close() error <span class="cov8" title="1">{
        // Close all cached readers
        c.readers.Range(func(key, value any) bool </span><span class="cov8" title="1">{
                if reader, ok := value.(*Reader); ok </span><span class="cov8" title="1">{
                        reader.Close()
                }</span>
                <span class="cov8" title="1">return true</span>
        })
        <span class="cov8" title="1">return nil</span>
}

// Read reads up to count entries from the specified shards
func (c *Consumer) Read(ctx context.Context, shards []uint32, count int) ([]StreamMessage, error) <span class="cov8" title="1">{
        var messages []StreamMessage
        remaining := count

        for _, shardID := range shards </span><span class="cov8" title="1">{
                if remaining &lt;= 0 </span><span class="cov8" title="1">{
                        break</span>
                }

                <span class="cov8" title="1">shard, err := c.client.getOrCreateShard(shardID)
                if err != nil </span><span class="cov0" title="0">{
                        return nil, fmt.Errorf("failed to get shard %d: %w", shardID, err)
                }</span>

                <span class="cov8" title="1">shardMessages, err := c.readFromShard(ctx, shard, remaining)
                if err != nil </span><span class="cov8" title="1">{
                        return nil, fmt.Errorf("failed to read from shard %d: %w", shardID, err)
                }</span>

                <span class="cov8" title="1">messages = append(messages, shardMessages...)
                remaining -= len(shardMessages)</span>
        }

        <span class="cov8" title="1">return messages, nil</span>
}

// Process continuously reads and processes messages from shards.
// The simplest usage processes all discoverable shards:
//
//        err := consumer.Process(ctx, handleMessages)
//
// With options:
//
//        err := consumer.Process(ctx, handleMessages,
//            comet.WithStream("events:v1:shard:*"),
//            comet.WithBatchSize(1000),
//            comet.WithErrorHandler(logError),
//        )
//
// For distributed processing:
//
//        err := consumer.Process(ctx, handleMessages,
//            comet.WithStream("events:v1:shard:*"),
//            comet.WithConsumerAssignment(workerID, totalWorkers),
//        )
func (c *Consumer) Process(ctx context.Context, handler ProcessFunc, opts ...ProcessOption) error <span class="cov8" title="1">{
        // Build config from options
        cfg := buildProcessConfig(handler, opts)

        // Validate required fields
        if cfg.handler == nil </span><span class="cov0" title="0">{
                return fmt.Errorf("handler is required")
        }</span>

        // Determine shards to process
        <span class="cov8" title="1">var shards []uint32
        if len(cfg.shards) &gt; 0 </span><span class="cov8" title="1">{
                shards = cfg.shards
        }</span> else<span class="cov8" title="1"> if cfg.stream != "" </span><span class="cov8" title="1">{
                // Auto-discover shards from stream pattern
                discoveredShards, err := c.discoverShards(cfg.stream, cfg.consumerID, cfg.consumerCount)
                if err != nil </span><span class="cov0" title="0">{
                        return fmt.Errorf("failed to discover shards: %w", err)
                }</span>
                <span class="cov8" title="1">shards = discoveredShards</span>
        } else<span class="cov0" title="0"> {
                return fmt.Errorf("either Shards or Stream must be specified")
        }</span>

        // Apply defaults (already set in buildProcessConfig)
        <span class="cov8" title="1">autoAck := cfg.autoAck

        // Main processing loop
        for </span><span class="cov8" title="1">{
                select </span>{
                case &lt;-ctx.Done():<span class="cov8" title="1">
                        return ctx.Err()</span>
                default:<span class="cov8" title="1"></span>
                }

                <span class="cov8" title="1">start := time.Now()
                messages, err := c.Read(ctx, shards, cfg.batchSize)
                if err != nil </span><span class="cov0" title="0">{
                        if cfg.onError != nil </span><span class="cov0" title="0">{
                                cfg.onError(err, 0)
                        }</span>
                        // For read errors, sleep and retry
                        <span class="cov0" title="0">select </span>{
                        case &lt;-ctx.Done():<span class="cov0" title="0">
                                return ctx.Err()</span>
                        case &lt;-time.After(cfg.pollInterval):<span class="cov0" title="0">
                                continue</span>
                        }
                }

                <span class="cov8" title="1">if len(messages) == 0 </span><span class="cov8" title="1">{
                        // No messages, wait before polling again
                        select </span>{
                        case &lt;-ctx.Done():<span class="cov8" title="1">
                                return ctx.Err()</span>
                        case &lt;-time.After(cfg.pollInterval):<span class="cov0" title="0">
                                continue</span>
                        }
                }

                // Process batch with retries
                <span class="cov8" title="1">var processErr error
                for retry := 0; retry &lt;= cfg.maxRetries; retry++ </span><span class="cov8" title="1">{
                        processErr = cfg.handler(messages)
                        if processErr == nil </span><span class="cov8" title="1">{
                                break</span>
                        }

                        <span class="cov8" title="1">if cfg.onError != nil </span><span class="cov8" title="1">{
                                cfg.onError(processErr, retry)
                        }</span>

                        <span class="cov8" title="1">if retry &lt; cfg.maxRetries </span><span class="cov8" title="1">{
                                select </span>{
                                case &lt;-ctx.Done():<span class="cov0" title="0">
                                        return ctx.Err()</span>
                                case &lt;-time.After(cfg.retryDelay * time.Duration(retry+1)):<span class="cov8" title="1"></span>
                                        // Exponential backoff
                                }
                        }
                }

                // Auto-ack if enabled and processing succeeded
                <span class="cov8" title="1">if autoAck &amp;&amp; processErr == nil </span><span class="cov8" title="1">{
                        for _, msg := range messages </span><span class="cov8" title="1">{
                                if err := c.Ack(ctx, msg.ID); err != nil &amp;&amp; cfg.onError != nil </span><span class="cov0" title="0">{
                                        cfg.onError(fmt.Errorf("ack failed for message %s: %w", msg.ID, err), 0)
                                }</span>
                        }
                }

                // Call batch callback if provided
                <span class="cov8" title="1">if cfg.onBatch != nil </span><span class="cov0" title="0">{
                        cfg.onBatch(len(messages), time.Since(start))
                }</span>
        }
}

// getOrCreateReader gets or creates a reader for a shard
func (c *Consumer) getOrCreateReader(shard *Shard) (*Reader, error) <span class="cov8" title="1">{
        // Fast path: check if reader exists using sync.Map
        if value, ok := c.readers.Load(shard.shardID); ok </span><span class="cov8" title="1">{
                reader := value.(*Reader)

                // Check if reader is still valid by comparing file lists
                shard.mu.RLock()
                currentFiles := shard.index.Files
                shard.mu.RUnlock()

                reader.mu.RLock()
                readerFiles := reader.files
                reader.mu.RUnlock()

                // Reader is valid only if it has the exact same files as the shard
                // This handles both additions (rotation) and deletions (retention)
                if len(currentFiles) == len(readerFiles) </span><span class="cov8" title="1">{
                        // Check if all files match
                        filesMatch := true
                        for i := 0; i &lt; len(currentFiles) &amp;&amp; filesMatch; i++ </span><span class="cov8" title="1">{
                                if currentFiles[i].Path != readerFiles[i].Path </span><span class="cov0" title="0">{
                                        filesMatch = false
                                }</span>
                        }
                        <span class="cov8" title="1">if filesMatch </span><span class="cov8" title="1">{
                                return reader, nil
                        }</span>
                }

                // Reader is stale, need to recreate
                // Delete the old one so only one goroutine recreates
                <span class="cov8" title="1">c.readers.Delete(shard.shardID)
                reader.Close()</span>
        }

        // Create new reader with a snapshot of the current index
        <span class="cov8" title="1">shard.mu.RLock()
        // Make a copy of the index to avoid race conditions
        indexCopy := &amp;ShardIndex{
                Files:              make([]FileInfo, len(shard.index.Files)),
                CurrentFile:        shard.index.CurrentFile,
                CurrentWriteOffset: shard.index.CurrentWriteOffset,
                CurrentEntryNumber: shard.index.CurrentEntryNumber,
                ConsumerOffsets:    make(map[string]int64),
                BinaryIndex: BinarySearchableIndex{
                        IndexInterval: shard.index.BinaryIndex.IndexInterval,
                        MaxNodes:      shard.index.BinaryIndex.MaxNodes,
                        Nodes:         make([]EntryIndexNode, len(shard.index.BinaryIndex.Nodes)),
                },
        }
        copy(indexCopy.Files, shard.index.Files)
        copy(indexCopy.BinaryIndex.Nodes, shard.index.BinaryIndex.Nodes)
        maps.Copy(indexCopy.ConsumerOffsets, shard.index.ConsumerOffsets)
        shard.mu.RUnlock()

        newReader, err := NewReader(shard.shardID, indexCopy)
        if err != nil </span><span class="cov8" title="1">{
                return nil, err
        }</span>

        // Use LoadOrStore to handle race where multiple goroutines try to create
        <span class="cov8" title="1">if actual, loaded := c.readers.LoadOrStore(shard.shardID, newReader); loaded </span><span class="cov0" title="0">{
                // Another goroutine created a reader, close ours and use theirs
                newReader.Close()
                return actual.(*Reader), nil
        }</span>

        <span class="cov8" title="1">return newReader, nil</span>
}

// findEntryPosition finds the position of an entry using binary searchable index (O(log n))
func (c *Consumer) findEntryPosition(shard *Shard, entryNum int64) (EntryPosition, error) <span class="cov8" title="1">{
        // Use the binary searchable index for O(log n) lookup
        if len(shard.index.BinaryIndex.Nodes) &gt; 0 </span><span class="cov8" title="1">{
                // Get the best starting position for scanning
                if startPos, startEntry, found := shard.index.BinaryIndex.GetScanStartPosition(entryNum); found </span><span class="cov8" title="1">{
                        // Safety check: if the position is invalid, fall back to linear scan
                        if startPos.FileIndex &lt; 0 || startPos.FileIndex &gt;= len(shard.index.Files) </span>{<span class="cov0" title="0">
                                // Binary index has stale data, fall back to scanning from beginning
                        }</span> else<span class="cov8" title="1"> {
                                if startEntry == entryNum </span><span class="cov8" title="1">{
                                        return startPos, nil
                                }</span>
                                // Scan forward from the closest indexed entry
                                <span class="cov8" title="1">return c.scanForwardToEntry(shard, startPos, startEntry, entryNum)</span>
                        }
                }
        }

        // If no index nodes exist, start from the beginning
        <span class="cov8" title="1">if len(shard.index.Files) == 0 </span><span class="cov0" title="0">{
                return EntryPosition{}, fmt.Errorf("no files in shard")
        }</span>

        // Start from entry 0 in the first file
        <span class="cov8" title="1">startPos := EntryPosition{
                FileIndex:  0,
                ByteOffset: 0,
        }

        // In multi-process mode, entries might not start from 0
        // Calculate the first entry number based on total entries written
        totalEntries := int64(0)
        for _, f := range shard.index.Files </span><span class="cov8" title="1">{
                totalEntries += f.Entries
        }</span>
        <span class="cov8" title="1">firstEntryNum := shard.index.CurrentEntryNumber - totalEntries

        // If looking for an entry before the first one, it doesn't exist
        if entryNum &lt; firstEntryNum </span><span class="cov8" title="1">{
                return EntryPosition{}, fmt.Errorf("entry %d does not exist (first entry is %d)", entryNum, firstEntryNum)
        }</span>

        // If looking for the first entry, return the start position
        <span class="cov8" title="1">if entryNum == firstEntryNum </span><span class="cov8" title="1">{
                return startPos, nil
        }</span>

        // Otherwise scan forward from the first entry
        <span class="cov8" title="1">return c.scanForwardToEntry(shard, startPos, firstEntryNum, entryNum)</span>
}

// scanForwardToEntry scans forward from a known position to find the target entry
func (c *Consumer) scanForwardToEntry(shard *Shard, startPos EntryPosition, startEntry, targetEntry int64) (EntryPosition, error) <span class="cov8" title="1">{
        if startEntry &gt;= targetEntry </span><span class="cov0" title="0">{
                return startPos, nil // No scanning needed
        }</span>

        // Get reader for this shard
        <span class="cov8" title="1">reader, err := c.getOrCreateReader(shard)
        if err != nil </span><span class="cov0" title="0">{
                return EntryPosition{}, fmt.Errorf("failed to get reader: %w", err)
        }</span>

        // Hold reader lock during the entire scan to prevent races with remapFile
        <span class="cov8" title="1">reader.mu.RLock()
        defer reader.mu.RUnlock()

        currentPos := startPos
        currentEntry := startEntry
        maxScanEntries := targetEntry - startEntry + 100 // Safety limit
        scannedEntries := int64(0)

        // Scan forward entry by entry until we reach the target
        for currentEntry &lt; targetEntry </span><span class="cov8" title="1">{
                // Safety check to prevent infinite loops
                scannedEntries++
                if scannedEntries &gt; maxScanEntries </span><span class="cov0" title="0">{
                        return EntryPosition{}, fmt.Errorf("scan limit exceeded: scanned %d entries looking for entry %d from %d", scannedEntries, targetEntry, startEntry)
                }</span>

                // Validate file index under lock
                <span class="cov8" title="1">if currentPos.FileIndex &gt;= len(reader.files) </span><span class="cov0" title="0">{
                        return EntryPosition{}, fmt.Errorf("file index %d out of range (have %d files)", currentPos.FileIndex, len(reader.files))
                }</span>

                <span class="cov8" title="1">file := reader.files[currentPos.FileIndex]
                fileData := file.data.Load() // Get data atomically
                if fileData == nil </span><span class="cov0" title="0">{
                        return EntryPosition{}, fmt.Errorf("file %d has no data", currentPos.FileIndex)
                }</span>

                // Check if we have enough data for the header
                <span class="cov8" title="1">if currentPos.ByteOffset+12 &gt; int64(len(fileData)) </span><span class="cov0" title="0">{
                        // Move to next file if available
                        if currentPos.FileIndex+1 &lt; len(reader.files) </span><span class="cov0" title="0">{
                                currentPos.FileIndex++
                                currentPos.ByteOffset = 0
                                continue</span>
                        } else<span class="cov0" title="0"> {
                                return EntryPosition{}, fmt.Errorf("entry header extends beyond final file (fileData len=%d, offset=%d)", len(fileData), currentPos.ByteOffset)
                        }</span>
                }

                // Read length from header (safe under reader lock)
                <span class="cov8" title="1">header := fileData[currentPos.ByteOffset : currentPos.ByteOffset+12]
                length := binary.LittleEndian.Uint32(header[0:4])

                // Validate entry length is reasonable
                if length &gt; 100*1024*1024 </span><span class="cov0" title="0">{ // 100MB max entry size
                        return EntryPosition{}, fmt.Errorf("entry %d has invalid length %d at file %d offset %d", currentEntry, length, currentPos.FileIndex, currentPos.ByteOffset)
                }</span>

                // Check if the full entry fits in current file
                <span class="cov8" title="1">entryEnd := currentPos.ByteOffset + 12 + int64(length)
                if entryEnd &gt; int64(len(fileData)) </span><span class="cov0" title="0">{
                        // Move to next file if available
                        if currentPos.FileIndex+1 &lt; len(reader.files) </span><span class="cov0" title="0">{
                                currentPos.FileIndex++
                                currentPos.ByteOffset = 0
                                continue</span>
                        } else<span class="cov0" title="0"> {
                                return EntryPosition{}, fmt.Errorf("entry %d data extends beyond final file", currentEntry)
                        }</span>
                }

                // Move to next entry
                <span class="cov8" title="1">currentEntry++
                currentPos.ByteOffset = entryEnd

                // Check if we've moved beyond the current file's data
                if currentPos.ByteOffset &gt;= int64(len(fileData)) </span><span class="cov0" title="0">{
                        // Move to next file if one exists
                        if currentPos.FileIndex+1 &lt; len(reader.files) </span><span class="cov0" title="0">{
                                currentPos.FileIndex++
                                currentPos.ByteOffset = 0
                        }</span> else<span class="cov0" title="0"> {
                                // We've reached the actual end
                                break</span>
                        }
                }
        }

        <span class="cov8" title="1">return currentPos, nil</span>
}

// readFromShard reads entries from a single shard using entry-based positioning
func (c *Consumer) readFromShard(ctx context.Context, shard *Shard, maxCount int) ([]StreamMessage, error) <span class="cov8" title="1">{
        // Lock-free reader tracking
        atomic.AddInt64(&amp;shard.readerCount, 1)
        defer atomic.AddInt64(&amp;shard.readerCount, -1)

        // Check mmap state for instant change detection
        if shard.mmapState != nil </span><span class="cov8" title="1">{
                currentTimestamp := atomic.LoadInt64(&amp;shard.mmapState.LastUpdateNanos)
                if currentTimestamp != shard.lastMmapCheck </span><span class="cov0" title="0">{
                        // Index changed - reload it under write lock
                        shard.mu.Lock()
                        // Double-check after acquiring lock
                        if currentTimestamp != shard.lastMmapCheck </span><span class="cov0" title="0">{
                                if err := shard.loadIndexWithRetry(); err != nil </span><span class="cov0" title="0">{
                                        shard.mu.Unlock()
                                        return nil, fmt.Errorf("failed to reload index after detecting mmap change: %w", err)
                                }</span>
                                <span class="cov0" title="0">shard.lastMmapCheck = currentTimestamp

                                // In multi-process mode, check if we need to rebuild index from files
                                // We can tell we're in multi-process mode if mmapState exists
                                if shard.mmapState != nil </span><span class="cov0" title="0">{
                                        shardDir := filepath.Join(c.client.dataDir, fmt.Sprintf("shard-%04d", shard.shardID))
                                        shard.lazyRebuildIndexIfNeeded(c.client.config, shardDir)
                                }</span>

                                // Also invalidate any cached readers since the index changed
                                <span class="cov0" title="0">c.readers.Range(func(key, value any) bool </span><span class="cov0" title="0">{
                                        if key.(uint32) == shard.shardID </span><span class="cov0" title="0">{
                                                if reader, ok := value.(*Reader); ok </span><span class="cov0" title="0">{
                                                        reader.Close()
                                                }</span>
                                                <span class="cov0" title="0">c.readers.Delete(key)
                                                return false</span> // Stop after finding this shard's reader
                                        }
                                        <span class="cov0" title="0">return true</span>
                                })
                        }
                        <span class="cov0" title="0">shard.mu.Unlock()</span>
                }
        }

        <span class="cov8" title="1">shard.mu.RLock()
        // Get consumer entry offset (not byte offset!)
        startEntryNum, exists := shard.index.ConsumerOffsets[c.group]
        if !exists </span><span class="cov8" title="1">{
                startEntryNum = 0
        }</span>
        <span class="cov8" title="1">endEntryNum := shard.index.CurrentEntryNumber
        fileCount := len(shard.index.Files)
        shard.mu.RUnlock()

        // In multi-process mode, check if index might be stale by comparing with coordination state
        if shard.mmapState != nil &amp;&amp; shard.mmapWriter != nil </span><span class="cov8" title="1">{
                totalWrites := shard.mmapWriter.CoordinationState().TotalWrites.Load()
                if totalWrites &gt; endEntryNum </span><span class="cov0" title="0">{
                        // Need write lock for rebuild
                        shard.mu.Lock()
                        shardDir := filepath.Join(c.client.dataDir, fmt.Sprintf("shard-%04d", shard.shardID))
                        // Force a full rebuild by manually updating the rebuild trigger
                        // Temporarily modify the file end offset to trigger a rebuild
                        oldEndOffset := int64(0)
                        if len(shard.index.Files) &gt; 0 </span><span class="cov0" title="0">{
                                oldEndOffset = shard.index.Files[len(shard.index.Files)-1].EndOffset
                                // Set end offset to 0 to force rebuild detection
                                shard.index.Files[len(shard.index.Files)-1].EndOffset = 0
                        }</span>

                        <span class="cov0" title="0">shard.lazyRebuildIndexIfNeeded(c.client.config, shardDir)

                        // Restore if rebuild didn't happen (shouldn't occur)
                        if len(shard.index.Files) &gt; 0 &amp;&amp; shard.index.Files[len(shard.index.Files)-1].EndOffset == 0 </span><span class="cov0" title="0">{
                                shard.index.Files[len(shard.index.Files)-1].EndOffset = oldEndOffset
                        }</span>
                        // Reload the updated values
                        <span class="cov0" title="0">startEntryNum, exists = shard.index.ConsumerOffsets[c.group]
                        if !exists </span><span class="cov0" title="0">{
                                startEntryNum = 0
                        }</span>
                        <span class="cov0" title="0">endEntryNum = shard.index.CurrentEntryNumber
                        fileCount = len(shard.index.Files)
                        shard.mu.Unlock()</span>
                }
        }

        <span class="cov8" title="1">if startEntryNum &gt;= endEntryNum </span><span class="cov8" title="1">{
                return nil, nil // No new data
        }</span>

        // Safety check: if we have entries but no files, something is wrong
        <span class="cov8" title="1">if endEntryNum &gt; 0 &amp;&amp; fileCount == 0 </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("shard %d has %d entries but no data files", shard.shardID, endEntryNum)
        }</span>

        // Preallocate slice capacity based on available entries and maxCount
        <span class="cov8" title="1">availableEntries := endEntryNum - startEntryNum
        expectedCount := availableEntries
        if maxCount &gt; 0 &amp;&amp; expectedCount &gt; int64(maxCount) </span><span class="cov8" title="1">{
                expectedCount = int64(maxCount)
        }</span>
        <span class="cov8" title="1">messages := make([]StreamMessage, 0, expectedCount)

        // Read entries by entry number, looking up positions from index
        for entryNum := startEntryNum; entryNum &lt; endEntryNum &amp;&amp; len(messages) &lt; maxCount; entryNum++ </span><span class="cov8" title="1">{
                // Check context
                if ctx.Err() != nil </span><span class="cov0" title="0">{
                        return messages, ctx.Err()
                }</span>

                // Find where this entry is stored using interval-based boundaries
                <span class="cov8" title="1">shard.mu.RLock()
                position, err := c.findEntryPosition(shard, entryNum)
                shard.mu.RUnlock()

                if err != nil </span><span class="cov8" title="1">{
                        // In multi-process mode, if index-based lookup fails, try direct file scanning
                        // This handles the case where the index is incomplete but the data exists in files
                        if shard.mmapState != nil &amp;&amp; !strings.Contains(err.Error(), "no files in shard") </span><span class="cov8" title="1">{
                                position, err = c.scanDataFilesForEntry(shard, entryNum)
                        }</span>

                        <span class="cov8" title="1">if err != nil </span><span class="cov0" title="0">{
                                // In multi-process mode, some entries might not exist due to gaps in the sequence
                                // Skip these entries instead of failing the entire read
                                if strings.Contains(err.Error(), "does not exist") </span><span class="cov0" title="0">{
                                        continue</span>
                                }
                                <span class="cov0" title="0">return nil, fmt.Errorf("failed to find entry %d position in shard %d: %w", entryNum, shard.shardID, err)</span>
                        }
                }

                // Safety check for invalid position
                <span class="cov8" title="1">if position.FileIndex &lt; 0 </span><span class="cov0" title="0">{
                        return nil, fmt.Errorf("invalid position for entry %d in shard %d: FileIndex=%d, ByteOffset=%d", entryNum, shard.shardID, position.FileIndex, position.ByteOffset)
                }</span>

                // Get reader for this shard
                <span class="cov8" title="1">reader, err := c.getOrCreateReader(shard)
                if err != nil </span><span class="cov0" title="0">{
                        return nil, fmt.Errorf("failed to get reader for shard %d: %w", shard.shardID, err)
                }</span>

                // Read the specific entry
                <span class="cov8" title="1">data, err := reader.ReadEntryAtPosition(position)
                if err != nil </span><span class="cov8" title="1">{
                        return nil, fmt.Errorf("failed to read entry %d from shard %d: %w", entryNum, shard.shardID, err)
                }</span>

                <span class="cov8" title="1">message := StreamMessage{
                        Stream: fmt.Sprintf("shard:%04d", shard.shardID),
                        ID:     MessageID{EntryNumber: entryNum, ShardID: shard.shardID},
                        Data:   data,
                }

                messages = append(messages, message)</span>
        }

        <span class="cov8" title="1">return messages, nil</span>
}

// scanDataFilesForEntry directly scans data files to find an entry when index is incomplete
func (c *Consumer) scanDataFilesForEntry(shard *Shard, targetEntry int64) (EntryPosition, error) <span class="cov8" title="1">{
        // This is the fallback when index-based lookup fails in multi-process mode
        // We scan the actual append-only data files directly

        shard.mu.RLock()
        files := make([]FileInfo, len(shard.index.Files))
        copy(files, shard.index.Files)
        shard.mu.RUnlock()

        if len(files) == 0 </span><span class="cov0" title="0">{
                return EntryPosition{}, fmt.Errorf("no files to scan")
        }</span>

        // Get reader for file access
        <span class="cov8" title="1">reader, err := c.getOrCreateReader(shard)
        if err != nil </span><span class="cov0" title="0">{
                return EntryPosition{}, fmt.Errorf("failed to get reader: %w", err)
        }</span>

        <span class="cov8" title="1">reader.mu.RLock()
        defer reader.mu.RUnlock()

        currentEntry := int64(0)

        // Scan each file sequentially
        for fileIdx := range files </span><span class="cov8" title="1">{
                if fileIdx &gt;= len(reader.files) </span><span class="cov0" title="0">{
                        break</span> // Reader doesn't have this file yet
                }

                <span class="cov8" title="1">file := reader.files[fileIdx]
                fileData := file.data.Load()
                if fileData == nil </span><span class="cov0" title="0">{
                        continue</span>
                }

                <span class="cov8" title="1">offset := int64(0)
                fileSize := int64(len(fileData))

                // Scan entries in this file
                for offset &lt; fileSize </span><span class="cov8" title="1">{
                        // Check if we have enough data for header
                        if offset+12 &gt; fileSize </span><span class="cov0" title="0">{
                                break</span>
                        }

                        // Read entry header
                        <span class="cov8" title="1">header := fileData[offset : offset+12]
                        length := binary.LittleEndian.Uint32(header[0:4])
                        timestamp := binary.LittleEndian.Uint64(header[4:12])

                        // Validate entry
                        if length == 0 || length &gt; 100*1024*1024 || timestamp == 0 </span><span class="cov0" title="0">{
                                // Invalid entry - try to find next valid entry
                                offset += 4
                                continue</span>
                        }

                        // Check if full entry is available
                        <span class="cov8" title="1">entryEnd := offset + 12 + int64(length)
                        if entryEnd &gt; fileSize </span><span class="cov0" title="0">{
                                break</span> // Entry extends beyond file
                        }

                        // Found a valid entry - check if it's the target
                        <span class="cov8" title="1">if currentEntry == targetEntry </span><span class="cov8" title="1">{
                                return EntryPosition{
                                        FileIndex:  fileIdx,
                                        ByteOffset: offset,
                                }, nil
                        }</span>

                        // Move to next entry
                        <span class="cov0" title="0">currentEntry++
                        offset = entryEnd</span>
                }
        }

        <span class="cov0" title="0">return EntryPosition{}, fmt.Errorf("entry %d not found in data files (scanned %d entries)", targetEntry, currentEntry)</span>
}

// Ack acknowledges one or more processed messages and updates consumer offset
func (c *Consumer) Ack(ctx context.Context, messageIDs ...MessageID) error <span class="cov8" title="1">{
        if len(messageIDs) == 0 </span><span class="cov0" title="0">{
                return nil
        }</span>

        // Single message fast path
        <span class="cov8" title="1">if len(messageIDs) == 1 </span><span class="cov8" title="1">{
                messageID := messageIDs[0]
                shard, err := c.client.getOrCreateShard(messageID.ShardID)
                if err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>

                <span class="cov8" title="1">shard.mu.Lock()
                // Update consumer offset to the next entry number
                // This ensures we won't re-read this entry
                nextEntry := messageID.EntryNumber + 1
                shard.index.ConsumerOffsets[c.group] = nextEntry
                // Mark that we need a checkpoint
                shard.writesSinceCheckpoint++
                shard.mu.Unlock()

                // Don't persist immediately - let periodic checkpoint handle it
                return nil</span>
        }

        // Multiple messages - use batch logic
        <span class="cov8" title="1">return c.ackBatch(messageIDs)</span>
}

// ackBatch is a helper for batch acknowledgments
func (c *Consumer) ackBatch(messageIDs []MessageID) error <span class="cov8" title="1">{
        // Group messages by shard
        shardGroups := make(map[uint32][]MessageID)
        for _, id := range messageIDs </span><span class="cov8" title="1">{
                shardGroups[id.ShardID] = append(shardGroups[id.ShardID], id)
        }</span>

        // Process each shard group
        <span class="cov8" title="1">for shardID, ids := range shardGroups </span><span class="cov8" title="1">{
                shard, err := c.client.getOrCreateShard(shardID)
                if err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>

                <span class="cov8" title="1">shard.mu.Lock()

                // Find the highest entry number in this shard's batch
                var maxEntry int64 = -1
                for _, id := range ids </span><span class="cov8" title="1">{
                        if id.EntryNumber &gt; maxEntry </span><span class="cov8" title="1">{
                                maxEntry = id.EntryNumber
                        }</span>
                }

                // Update to one past the highest ACK'd entry
                <span class="cov8" title="1">if maxEntry &gt;= 0 </span><span class="cov8" title="1">{
                        shard.index.ConsumerOffsets[c.group] = maxEntry + 1
                        // Mark that we need a checkpoint
                        shard.writesSinceCheckpoint++
                }</span>

                <span class="cov8" title="1">shard.mu.Unlock()</span>
        }

        <span class="cov8" title="1">return nil</span>
}

// GetLag returns how many entries behind this consumer group is
func (c *Consumer) GetLag(ctx context.Context, shardID uint32) (int64, error) <span class="cov8" title="1">{
        shard, err := c.client.getOrCreateShard(shardID)
        if err != nil </span><span class="cov0" title="0">{
                return 0, err
        }</span>

        // Check mmap state for instant change detection
        <span class="cov8" title="1">if shard.mmapState != nil </span><span class="cov0" title="0">{
                currentTimestamp := atomic.LoadInt64(&amp;shard.mmapState.LastUpdateNanos)
                if currentTimestamp != shard.lastMmapCheck </span><span class="cov0" title="0">{
                        // Index changed - reload it under write lock
                        shard.mu.Lock()
                        // Double-check after acquiring lock
                        if currentTimestamp != shard.lastMmapCheck </span><span class="cov0" title="0">{
                                if err := shard.loadIndexWithRetry(); err != nil </span><span class="cov0" title="0">{
                                        shard.mu.Unlock()
                                        return 0, fmt.Errorf("failed to reload index after detecting mmap change: %w", err)
                                }</span>
                                <span class="cov0" title="0">shard.lastMmapCheck = currentTimestamp</span>
                        }
                        <span class="cov0" title="0">shard.mu.Unlock()</span>
                }
        }

        <span class="cov8" title="1">shard.mu.RLock()
        defer shard.mu.RUnlock()

        consumerEntry, exists := shard.index.ConsumerOffsets[c.group]
        if !exists </span><span class="cov0" title="0">{
                consumerEntry = 0
        }</span>

        // Entry-based lag calculation
        <span class="cov8" title="1">lag := shard.index.CurrentEntryNumber - consumerEntry
        return lag, nil</span>
}

// ResetOffset sets the consumer offset to a specific entry number
func (c *Consumer) ResetOffset(ctx context.Context, shardID uint32, entryNumber int64) error <span class="cov8" title="1">{
        shard, err := c.client.getOrCreateShard(shardID)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov8" title="1">shard.mu.Lock()
        if entryNumber &lt; 0 </span><span class="cov0" title="0">{
                // Negative means from end
                entryNumber = shard.index.CurrentEntryNumber + entryNumber
                if entryNumber &lt; 0 </span><span class="cov0" title="0">{
                        entryNumber = 0
                }</span>
        }

        <span class="cov8" title="1">shard.index.ConsumerOffsets[c.group] = entryNumber
        // Mark that we need a checkpoint
        shard.writesSinceCheckpoint++
        shard.mu.Unlock()

        // Don't persist immediately - let periodic checkpoint handle it
        return nil</span>
}

// AckRange acknowledges all messages in a contiguous range for a shard
// This is more efficient than individual acks for bulk processing
func (c *Consumer) AckRange(ctx context.Context, shardID uint32, fromEntry, toEntry int64) error <span class="cov8" title="1">{
        if fromEntry &gt; toEntry </span><span class="cov0" title="0">{
                return fmt.Errorf("invalid range: from %d &gt; to %d", fromEntry, toEntry)
        }</span>

        <span class="cov8" title="1">shard, err := c.client.getOrCreateShard(shardID)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov8" title="1">shard.mu.Lock()
        // Update to one past the end of the range
        newOffset := toEntry + 1
        currentOffset, exists := shard.index.ConsumerOffsets[c.group]

        // Only update if this advances the offset (no going backwards)
        if !exists || newOffset &gt; currentOffset </span><span class="cov8" title="1">{
                shard.index.ConsumerOffsets[c.group] = newOffset
                // Mark that we need a checkpoint
                shard.writesSinceCheckpoint++
        }</span>
        <span class="cov8" title="1">shard.mu.Unlock()

        return nil</span>
}

// StreamStats returns statistics about a shard
// Fields ordered for optimal memory alignment
type StreamStats struct {
        // 64-bit aligned fields first
        TotalEntries int64
        TotalBytes   int64
        OldestEntry  time.Time
        NewestEntry  time.Time

        // Composite types
        ConsumerOffsets map[string]int64

        // Smaller fields last
        FileCount int    // 8 bytes
        ShardID   uint32 // 4 bytes
        // 4 bytes padding
}

// GetShardStats returns statistics for a specific shard
func (c *Consumer) GetShardStats(ctx context.Context, shardID uint32) (*StreamStats, error) <span class="cov0" title="0">{
        shard, err := c.client.getOrCreateShard(shardID)
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>

        // Check mmap state for instant change detection
        <span class="cov0" title="0">if shard.mmapState != nil </span><span class="cov0" title="0">{
                currentTimestamp := atomic.LoadInt64(&amp;shard.mmapState.LastUpdateNanos)
                if currentTimestamp != shard.lastMmapCheck </span><span class="cov0" title="0">{
                        // Index changed - reload it under write lock
                        shard.mu.Lock()
                        // Double-check after acquiring lock
                        if currentTimestamp != shard.lastMmapCheck </span><span class="cov0" title="0">{
                                if err := shard.loadIndexWithRetry(); err != nil </span><span class="cov0" title="0">{
                                        shard.mu.Unlock()
                                        return nil, fmt.Errorf("failed to reload index after detecting mmap change: %w", err)
                                }</span>
                                <span class="cov0" title="0">shard.lastMmapCheck = currentTimestamp</span>
                        }
                        <span class="cov0" title="0">shard.mu.Unlock()</span>
                }
        }

        <span class="cov0" title="0">shard.mu.RLock()
        defer shard.mu.RUnlock()

        stats := &amp;StreamStats{
                ShardID:         shardID,
                FileCount:       len(shard.index.Files),
                ConsumerOffsets: make(map[string]int64),
        }

        // Copy consumer offsets
        for group, offset := range shard.index.ConsumerOffsets </span><span class="cov0" title="0">{
                stats.ConsumerOffsets[group] = offset
        }</span>

        // Calculate totals from files
        <span class="cov0" title="0">for _, file := range shard.index.Files </span><span class="cov0" title="0">{
                stats.TotalEntries += file.Entries
                stats.TotalBytes += file.EndOffset - file.StartOffset

                if stats.OldestEntry.IsZero() || file.StartTime.Before(stats.OldestEntry) </span><span class="cov0" title="0">{
                        stats.OldestEntry = file.StartTime
                }</span>

                <span class="cov0" title="0">if file.EndTime.After(stats.NewestEntry) </span><span class="cov0" title="0">{
                        stats.NewestEntry = file.EndTime
                }</span>
        }

        <span class="cov0" title="0">return stats, nil</span>
}

// discoverShards discovers available shards based on stream pattern and consumer assignment
func (c *Consumer) discoverShards(streamPattern string, consumerID, consumerCount int) ([]uint32, error) <span class="cov8" title="1">{
        // Apply defaults
        if consumerCount &lt;= 0 </span><span class="cov0" title="0">{
                consumerCount = 1
        }</span>
        <span class="cov8" title="1">if consumerID &lt; 0 || consumerID &gt;= consumerCount </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("invalid consumerID %d for consumerCount %d", consumerID, consumerCount)
        }</span>

        // Parse stream pattern (e.g., "events:v1:shard:*")
        <span class="cov8" title="1">if !strings.HasSuffix(streamPattern, ":*") </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("stream pattern must end with :* (e.g., events:v1:shard:*)")
        }</span>

        <span class="cov8" title="1">baseStream := strings.TrimSuffix(streamPattern, "*")

        // Discover all available shards
        // Check up to 32 shards by default (can be increased if needed)
        maxShards := uint32(32)
        var allShards []uint32

        // Try to discover shards in parallel for better performance
        type result struct {
                shardID uint32
                exists  bool
        }

        results := make(chan result, maxShards)
        var wg sync.WaitGroup

        for shardID := uint32(0); shardID &lt; maxShards; shardID++ </span><span class="cov8" title="1">{
                wg.Add(1)
                go func(id uint32) </span><span class="cov8" title="1">{
                        defer wg.Done()
                        streamName := fmt.Sprintf("%s%04d", baseStream, id)
                        _, err := c.client.Len(context.Background(), streamName)
                        results &lt;- result{shardID: id, exists: err == nil}
                }</span>(shardID)
        }

        <span class="cov8" title="1">go func() </span><span class="cov8" title="1">{
                wg.Wait()
                close(results)
        }</span>()

        <span class="cov8" title="1">for res := range results </span><span class="cov8" title="1">{
                if res.exists </span><span class="cov8" title="1">{
                        allShards = append(allShards, res.shardID)
                }</span>
        }

        <span class="cov8" title="1">if len(allShards) == 0 </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("no shards found for pattern %s", streamPattern)
        }</span>

        // Sort shards for predictable assignment
        <span class="cov8" title="1">slices.Sort(allShards)

        // Assign shards to this consumer using modulo distribution
        var assignedShards []uint32
        for _, shardID := range allShards </span><span class="cov8" title="1">{
                if int(shardID)%consumerCount == consumerID </span><span class="cov8" title="1">{
                        assignedShards = append(assignedShards, shardID)
                }</span>
        }

        <span class="cov8" title="1">return assignedShards, nil</span>
}
</pre>
		
		<pre class="file" id="file3" style="display: none">package comet

import (
        "encoding/binary"
        "fmt"
        "io"
        "os"
        "time"
)

// Binary index format:
// Header:
//   [4] Magic number (0x434F4D54 = "COMT")
//   [4] Version (1)
//   [8] Current entry number
//   [8] Current write offset
//   [4] Consumer count
//   [4] Binary index node count
// Consumers:
//   For each consumer:
//     [1] Group name length
//     [N] Group name (UTF-8)
//     [8] Offset
// Binary index nodes:
//   For each node:
//     [8] Entry number
//     [4] File index
//     [8] Byte offset

const (
        indexMagic   = 0x434F4D54 // "COMT"
        indexVersion = 1
)

// saveBinaryIndex writes the index in binary format
func (s *Shard) saveBinaryIndex(index *ShardIndex) error <span class="cov8" title="1">{
        // Create temp file
        tempPath := s.indexPath + ".tmp"
        f, err := os.Create(tempPath)
        if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to create temp index: %w", err)
        }</span>
        <span class="cov8" title="1">defer f.Close()

        // Write header
        if err := binary.Write(f, binary.LittleEndian, uint32(indexMagic)); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov8" title="1">if err := binary.Write(f, binary.LittleEndian, uint32(indexVersion)); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov8" title="1">if err := binary.Write(f, binary.LittleEndian, uint64(index.CurrentEntryNumber)); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov8" title="1">if err := binary.Write(f, binary.LittleEndian, uint64(index.CurrentWriteOffset)); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov8" title="1">if err := binary.Write(f, binary.LittleEndian, uint32(len(index.ConsumerOffsets))); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov8" title="1">if err := binary.Write(f, binary.LittleEndian, uint32(len(index.BinaryIndex.Nodes))); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov8" title="1">if err := binary.Write(f, binary.LittleEndian, uint32(len(index.Files))); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        // Write consumer offsets
        <span class="cov8" title="1">for group, offset := range index.ConsumerOffsets </span><span class="cov8" title="1">{
                groupBytes := []byte(group)
                if err := binary.Write(f, binary.LittleEndian, uint8(len(groupBytes))); err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>
                <span class="cov8" title="1">if _, err := f.Write(groupBytes); err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>
                <span class="cov8" title="1">if err := binary.Write(f, binary.LittleEndian, uint64(offset)); err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>
        }

        // Write binary index nodes
        <span class="cov8" title="1">for _, node := range index.BinaryIndex.Nodes </span><span class="cov8" title="1">{
                if err := binary.Write(f, binary.LittleEndian, uint64(node.EntryNumber)); err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>
                <span class="cov8" title="1">if err := binary.Write(f, binary.LittleEndian, uint32(node.Position.FileIndex)); err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>
                <span class="cov8" title="1">if err := binary.Write(f, binary.LittleEndian, uint64(node.Position.ByteOffset)); err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>
        }

        // Write files info
        <span class="cov8" title="1">for _, file := range index.Files </span><span class="cov8" title="1">{
                // Write path length and path
                pathBytes := []byte(file.Path)
                if err := binary.Write(f, binary.LittleEndian, uint16(len(pathBytes))); err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>
                <span class="cov8" title="1">if _, err := f.Write(pathBytes); err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>
                // Write file metadata
                <span class="cov8" title="1">if err := binary.Write(f, binary.LittleEndian, uint64(file.StartOffset)); err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>
                <span class="cov8" title="1">if err := binary.Write(f, binary.LittleEndian, uint64(file.EndOffset)); err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>
                <span class="cov8" title="1">if err := binary.Write(f, binary.LittleEndian, uint64(file.StartEntry)); err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>
                <span class="cov8" title="1">if err := binary.Write(f, binary.LittleEndian, uint64(file.Entries)); err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>
                <span class="cov8" title="1">if err := binary.Write(f, binary.LittleEndian, uint64(file.StartTime.UnixNano())); err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>
                <span class="cov8" title="1">if err := binary.Write(f, binary.LittleEndian, uint64(file.EndTime.UnixNano())); err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>
        }

        // Sync to disk
        <span class="cov8" title="1">if err := f.Sync(); err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to sync index: %w", err)
        }</span>

        // Atomic rename
        <span class="cov8" title="1">return os.Rename(tempPath, s.indexPath)</span>
}

// loadBinaryIndex reads the index from binary format
func (s *Shard) loadBinaryIndex() (*ShardIndex, error) <span class="cov8" title="1">{
        data, err := os.ReadFile(s.indexPath)
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>

        <span class="cov8" title="1">if len(data) &lt; 32 </span><span class="cov0" title="0">{ // Minimum header size
                return nil, fmt.Errorf("index file too small")
        }</span>

        // Read and verify header
        <span class="cov8" title="1">offset := 0
        magic := binary.LittleEndian.Uint32(data[offset:])
        offset += 4
        if magic != indexMagic </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("invalid index magic: %x", magic)
        }</span>

        <span class="cov8" title="1">version := binary.LittleEndian.Uint32(data[offset:])
        offset += 4
        if version != indexVersion </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("unsupported index version: %d", version)
        }</span>

        <span class="cov8" title="1">index := &amp;ShardIndex{
                ConsumerOffsets: make(map[string]int64),
                BinaryIndex: BinarySearchableIndex{
                        IndexInterval: s.index.BinaryIndex.IndexInterval,
                        MaxNodes:      s.index.BinaryIndex.MaxNodes,
                },
        }

        index.CurrentEntryNumber = int64(binary.LittleEndian.Uint64(data[offset:]))
        offset += 8
        index.CurrentWriteOffset = int64(binary.LittleEndian.Uint64(data[offset:]))
        offset += 8

        consumerCount := binary.LittleEndian.Uint32(data[offset:])
        offset += 4
        nodeCount := binary.LittleEndian.Uint32(data[offset:])
        offset += 4
        fileCount := uint32(0)
        if offset &lt; len(data)-4 </span><span class="cov8" title="1">{
                fileCount = binary.LittleEndian.Uint32(data[offset:])
                offset += 4
        }</span>

        // Read consumer offsets
        <span class="cov8" title="1">for i := uint32(0); i &lt; consumerCount; i++ </span><span class="cov8" title="1">{
                if offset &gt;= len(data) </span><span class="cov0" title="0">{
                        return nil, io.ErrUnexpectedEOF
                }</span>

                <span class="cov8" title="1">groupLen := int(data[offset])
                offset++

                if offset+groupLen+8 &gt; len(data) </span><span class="cov0" title="0">{
                        return nil, io.ErrUnexpectedEOF
                }</span>

                <span class="cov8" title="1">group := string(data[offset : offset+groupLen])
                offset += groupLen

                consumerOffset := int64(binary.LittleEndian.Uint64(data[offset:]))
                offset += 8

                index.ConsumerOffsets[group] = consumerOffset</span>
        }

        // Read binary index nodes
        <span class="cov8" title="1">index.BinaryIndex.Nodes = make([]EntryIndexNode, 0, nodeCount)
        for i := uint32(0); i &lt; nodeCount; i++ </span><span class="cov8" title="1">{
                if offset+20 &gt; len(data) </span><span class="cov0" title="0">{
                        return nil, io.ErrUnexpectedEOF
                }</span>

                <span class="cov8" title="1">node := EntryIndexNode{
                        EntryNumber: int64(binary.LittleEndian.Uint64(data[offset:])),
                        Position: EntryPosition{
                                FileIndex:  int(binary.LittleEndian.Uint32(data[offset+8:])),
                                ByteOffset: int64(binary.LittleEndian.Uint64(data[offset+12:])),
                        },
                }
                offset += 20

                index.BinaryIndex.Nodes = append(index.BinaryIndex.Nodes, node)</span>
        }

        // Read files info
        <span class="cov8" title="1">index.Files = make([]FileInfo, 0, fileCount)
        for i := uint32(0); i &lt; fileCount; i++ </span><span class="cov8" title="1">{
                if offset+2 &gt; len(data) </span><span class="cov0" title="0">{
                        return nil, io.ErrUnexpectedEOF
                }</span>

                <span class="cov8" title="1">pathLen := int(binary.LittleEndian.Uint16(data[offset:]))
                offset += 2

                if offset+pathLen &gt; len(data) </span><span class="cov0" title="0">{
                        return nil, io.ErrUnexpectedEOF
                }</span>

                <span class="cov8" title="1">path := string(data[offset : offset+pathLen])
                offset += pathLen

                if offset+48 &gt; len(data) </span><span class="cov0" title="0">{ // 6 uint64 fields
                        return nil, io.ErrUnexpectedEOF
                }</span>

                <span class="cov8" title="1">file := FileInfo{
                        Path:        path,
                        StartOffset: int64(binary.LittleEndian.Uint64(data[offset:])),
                        EndOffset:   int64(binary.LittleEndian.Uint64(data[offset+8:])),
                        StartEntry:  int64(binary.LittleEndian.Uint64(data[offset+16:])),
                        Entries:     int64(binary.LittleEndian.Uint64(data[offset+24:])),
                        StartTime:   time.Unix(0, int64(binary.LittleEndian.Uint64(data[offset+32:]))),
                        EndTime:     time.Unix(0, int64(binary.LittleEndian.Uint64(data[offset+40:]))),
                }
                offset += 48

                index.Files = append(index.Files, file)</span>
        }

        // Set current file from last file if available
        <span class="cov8" title="1">if len(index.Files) &gt; 0 </span><span class="cov8" title="1">{
                index.CurrentFile = index.Files[len(index.Files)-1].Path
        }</span> else<span class="cov8" title="1"> {
                index.CurrentFile = s.index.CurrentFile
        }</span>
        <span class="cov8" title="1">index.BoundaryInterval = s.index.BoundaryInterval

        return index, nil</span>
}
</pre>
		
		<pre class="file" id="file4" style="display: none">package comet

import (
        "encoding/binary"
        "fmt"
        "os"
        "sync"
        "sync/atomic"
        "syscall"
        "time"
        "unsafe"
)

// MmapCoordinationState represents shared state for ultra-fast multi-process coordination
type MmapCoordinationState struct {
        // Core coordination fields
        ActiveFileIndex    atomic.Int64 // Current file being written to
        WriteOffset        atomic.Int64 // Current write position in active file
        FileSize           atomic.Int64 // Current size of active file
        RotationInProgress atomic.Int64 // 1 if rotation is happening, 0 otherwise

        // Performance tracking
        LastWriteNanos atomic.Int64 // Timestamp of last write
        TotalWrites    atomic.Int64 // Total number of writes

        // Reserved for future use
        _reserved [168]byte // Adjusted for atomic types (each atomic.Int64 is 8 bytes)
}

// CoordinationState returns the coordination state for external access
func (w *MmapWriter) CoordinationState() *MmapCoordinationState <span class="cov8" title="1">{
        return w.coordinationState
}</span>

// MmapWriter implements ultra-fast memory-mapped writes for multi-process mode
type MmapWriter struct {
        mu sync.Mutex

        // Coordination state
        coordinationPath  string
        coordinationData  []byte
        coordinationState *MmapCoordinationState

        // Current mapped region
        dataFile     *os.File
        dataPath     string
        mappedData   []byte
        mappedOffset int64 // Where this mapping starts in the file
        mappedSize   int64 // Size of current mapping

        // Configuration
        shardDir        string
        initialSize     int64 // Initial file size (default: 128MB)
        growthIncrement int64 // How much to grow (default: 128MB)
        mappingWindow   int64 // Size of active mapping (default: 32MB)
        maxFileSize     int64 // Max file size before rotation (default: 1GB)

        // References for index updates
        index   *ShardIndex
        metrics *ClientMetrics

        // Metrics
        remapCount    int64
        rotationCount int64
}

// NewMmapWriter creates a new memory-mapped writer for a shard
func NewMmapWriter(shardDir string, maxFileSize int64, index *ShardIndex, metrics *ClientMetrics) (*MmapWriter, error) <span class="cov8" title="1">{
        w := &amp;MmapWriter{
                shardDir:         shardDir,
                coordinationPath: shardDir + "/coordination.state",
                initialSize:      4 * 1024,        // 4KB initial
                growthIncrement:  1 * 1024 * 1024, // 1MB growth
                mappingWindow:    1 * 1024 * 1024, // 1MB active window
                maxFileSize:      maxFileSize,
                index:            index,
                metrics:          metrics,
        }

        // Initialize coordination state
        if err := w.initCoordinationState(); err != nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("failed to init coordination state: %w", err)
        }</span>

        // Open or create current data file
        <span class="cov8" title="1">if err := w.openCurrentFile(); err != nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("failed to open data file: %w", err)
        }</span>

        <span class="cov8" title="1">return w, nil</span>
}

// initCoordinationState initializes the memory-mapped coordination state
func (w *MmapWriter) initCoordinationState() error <span class="cov8" title="1">{
        // Create or open state file
        file, err := os.OpenFile(w.coordinationPath, os.O_CREATE|os.O_RDWR, 0644)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov8" title="1">defer file.Close()

        // Ensure file is correct size
        const stateSize = 256
        stat, err := file.Stat()
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov8" title="1">if stat.Size() == 0 </span><span class="cov8" title="1">{
                // New file - initialize
                if err := file.Truncate(stateSize); err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>
        }

        // Memory map the state
        <span class="cov8" title="1">data, err := syscall.Mmap(int(file.Fd()), 0, stateSize,
                syscall.PROT_READ|syscall.PROT_WRITE, syscall.MAP_SHARED)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov8" title="1">w.coordinationData = data
        w.coordinationState = (*MmapCoordinationState)(unsafe.Pointer(&amp;data[0]))

        return nil</span>
}

// openCurrentFile opens the current data file and maps the active region
func (w *MmapWriter) openCurrentFile() error <span class="cov8" title="1">{
        // Get current file index
        fileIndex := w.coordinationState.ActiveFileIndex.Load()
        if fileIndex == 0 </span><span class="cov8" title="1">{
                fileIndex = 1
                w.coordinationState.ActiveFileIndex.Store(1)
        }</span>

        // Construct file path using standard naming convention
        <span class="cov8" title="1">w.dataPath = fmt.Sprintf("%s/log-%016d.comet", w.shardDir, fileIndex)

        // Check if this is the current file in the index
        createNew := w.index.CurrentFile != w.dataPath

        // Open or create file
        file, err := os.OpenFile(w.dataPath, os.O_CREATE|os.O_RDWR, 0644)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        // Get file size
        <span class="cov8" title="1">stat, err := file.Stat()
        if err != nil </span><span class="cov0" title="0">{
                file.Close()
                return err
        }</span>

        <span class="cov8" title="1">fileSize := stat.Size()

        // Always ensure we have at least a minimal size to map
        minSize := int64(4096) // 4KB minimum
        if fileSize &lt; minSize </span><span class="cov8" title="1">{
                if err := file.Truncate(minSize); err != nil </span><span class="cov0" title="0">{
                        file.Close()
                        return err
                }</span>
                <span class="cov8" title="1">fileSize = minSize</span>
        }

        // Store the actual file size
        <span class="cov8" title="1">w.coordinationState.FileSize.Store(fileSize)

        // Update index if this is a new file or we're initializing
        if createNew || w.index.CurrentFile == "" </span><span class="cov8" title="1">{
                w.index.CurrentFile = w.dataPath

                // Check if we need to add this file to the index
                found := false
                for _, f := range w.index.Files </span><span class="cov0" title="0">{
                        if f.Path == w.dataPath </span><span class="cov0" title="0">{
                                found = true
                                break</span>
                        }
                }

                <span class="cov8" title="1">if !found </span><span class="cov8" title="1">{
                        w.index.Files = append(w.index.Files, FileInfo{
                                Path:        w.dataPath,
                                StartOffset: 0,
                                StartEntry:  w.index.CurrentEntryNumber,
                                StartTime:   time.Now(),
                                Entries:     0,
                        })
                }</span>
        }

        <span class="cov8" title="1">w.dataFile = file

        // Map the active window (last portion of file)
        return w.remapActiveWindow()</span>
}

// remapActiveWindow maps or remaps the active portion of the file
func (w *MmapWriter) remapActiveWindow() error <span class="cov8" title="1">{
        // Get current file size
        fileSize := w.coordinationState.FileSize.Load()

        // Calculate mapping window
        mappingStart := int64(0) // Always start from beginning for simplicity
        mappingSize := fileSize  // Map the entire file

        // Ensure we have something to map
        if mappingSize &lt;= 0 </span><span class="cov0" title="0">{
                return fmt.Errorf("file size is 0, cannot map")
        }</span>

        // Unmap previous mapping if exists
        <span class="cov8" title="1">if w.mappedData != nil </span><span class="cov8" title="1">{
                if err := syscall.Munmap(w.mappedData); err != nil </span><span class="cov0" title="0">{
                        return fmt.Errorf("failed to unmap: %w", err)
                }</span>
        }

        // Map new window
        <span class="cov8" title="1">data, err := syscall.Mmap(int(w.dataFile.Fd()), mappingStart, int(mappingSize),
                syscall.PROT_READ|syscall.PROT_WRITE, syscall.MAP_SHARED)
        if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to mmap window: %w", err)
        }</span>

        <span class="cov8" title="1">w.mappedData = data
        w.mappedOffset = mappingStart
        w.mappedSize = mappingSize
        atomic.AddInt64(&amp;w.remapCount, 1)

        return nil</span>
}

// Write appends entries using memory-mapped I/O
func (w *MmapWriter) Write(entries [][]byte, entryNumbers []int64) error <span class="cov8" title="1">{
        if len(entries) == 0 </span><span class="cov0" title="0">{
                return nil
        }</span>

        // Calculate total size needed
        <span class="cov8" title="1">totalSize := int64(0)
        for _, entry := range entries </span><span class="cov8" title="1">{
                totalSize += 12 + int64(len(entry)) // header + data
        }</span>

        // Atomically allocate write space
        <span class="cov8" title="1">writeOffset := w.coordinationState.WriteOffset.Add(totalSize) - totalSize

        // Check if we need rotation - return special error to let shard handle it
        if writeOffset+totalSize &gt; w.maxFileSize </span><span class="cov8" title="1">{
                // Roll back the allocation
                w.coordinationState.WriteOffset.Add(-totalSize)

                // Return special error to indicate rotation is needed
                // The shard will handle rotation and index updates properly
                return fmt.Errorf("rotation needed: current file would exceed size limit")
        }</span>

        // Write entries - hold lock for entire write operation including growth check
        <span class="cov8" title="1">w.mu.Lock()
        defer w.mu.Unlock()

        // Check if we need to grow the file (inside lock to prevent races)
        currentFileSize := w.coordinationState.FileSize.Load()
        if writeOffset+totalSize &gt; currentFileSize </span><span class="cov8" title="1">{
                if err := w.growFile(writeOffset + totalSize); err != nil </span><span class="cov0" title="0">{
                        // Roll back the allocation on error
                        w.coordinationState.WriteOffset.Add(-totalSize)
                        return err
                }</span>
        }

        // Check if we have a valid file
        <span class="cov8" title="1">if w.dataFile == nil </span><span class="cov0" title="0">{
                return fmt.Errorf("data file is nil")
        }</span>

        // Since we're mapping the entire file, just check if we have enough space
        <span class="cov8" title="1">if writeOffset+totalSize &gt; w.mappedSize </span><span class="cov0" title="0">{
                // Need to remap after growing
                if err := w.remapActiveWindow(); err != nil </span><span class="cov0" title="0">{
                        return fmt.Errorf("failed to remap for write: %w", err)
                }</span>
        }

        // Verify we have enough mapped space after potential remap
        <span class="cov8" title="1">if writeOffset+totalSize &gt; w.mappedSize </span><span class="cov0" title="0">{
                return fmt.Errorf("insufficient mapped space: need %d bytes at offset %d, but only %d bytes mapped",
                        totalSize, writeOffset, w.mappedSize)
        }</span>

        // Write directly to mapped memory
        <span class="cov8" title="1">offset := writeOffset
        now := time.Now().UnixNano()
        for i, entry := range entries </span><span class="cov8" title="1">{
                // Since we map from beginning, offset is the position
                pos := offset

                // Double-check bounds before writing
                if pos+12+int64(len(entry)) &gt; int64(len(w.mappedData)) </span><span class="cov0" title="0">{
                        return fmt.Errorf("write would exceed mapped region: pos=%d, entry=%d bytes, mapped=%d bytes",
                                pos, len(entry)+12, len(w.mappedData))
                }</span>

                // Write header (12 bytes)
                <span class="cov8" title="1">binary.LittleEndian.PutUint32(w.mappedData[pos:pos+4], uint32(len(entry)))
                binary.LittleEndian.PutUint64(w.mappedData[pos+4:pos+12], uint64(now))

                // Write data
                copy(w.mappedData[pos+12:pos+12+int64(len(entry))], entry)

                offset += 12 + int64(len(entry))
                _ = entryNumbers[i]</span> // Entry numbers already allocated by caller
        }

        // CRITICAL: Ensure file size reflects what we wrote
        // This is what makes the data visible to readers
        <span class="cov8" title="1">finalOffset := writeOffset + totalSize
        if stat, err := w.dataFile.Stat(); err == nil </span><span class="cov8" title="1">{
                if stat.Size() &lt; finalOffset </span><span class="cov0" title="0">{
                        // File is smaller than what we wrote - extend it
                        if err := w.dataFile.Truncate(finalOffset); err != nil </span><span class="cov0" title="0">{
                                return fmt.Errorf("failed to extend file to written size: %w", err)
                        }</span>
                }
        }

        // Update coordination state
        <span class="cov8" title="1">w.coordinationState.LastWriteNanos.Store(now)
        w.coordinationState.TotalWrites.Add(int64(len(entries)))

        // Update client metrics if available
        if w.metrics != nil </span><span class="cov8" title="1">{
                // Track total bytes written
                totalBytes := uint64(0)
                for _, entry := range entries </span><span class="cov8" title="1">{
                        totalBytes += uint64(len(entry))
                }</span>

                <span class="cov8" title="1">w.metrics.TotalEntries.Add(uint64(len(entries)))
                w.metrics.TotalBytes.Add(totalBytes)</span>

                // Note: Compression is already handled by the caller, so we don't track it here
        }

        // Note: Index updates are handled by the caller (shard) which holds the appropriate locks
        // We only update the coordination state here

        <span class="cov8" title="1">return nil</span>
}

// growFile grows the file to accommodate more data
func (w *MmapWriter) growFile(minSize int64) error <span class="cov8" title="1">{
        // Calculate new size (round up to growth increment)
        newSize := ((minSize + w.growthIncrement - 1) / w.growthIncrement) * w.growthIncrement

        // Grow the file (already holding lock)
        if w.dataFile == nil </span><span class="cov0" title="0">{
                return fmt.Errorf("data file is nil")
        }</span>

        <span class="cov8" title="1">if err := w.dataFile.Truncate(newSize); err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to grow file: %w", err)
        }</span>

        // Update coordination state
        <span class="cov8" title="1">w.coordinationState.FileSize.Store(newSize)

        // Remap if needed
        if w.mappedOffset+w.mappedSize &lt; minSize </span><span class="cov8" title="1">{
                return w.remapActiveWindow()
        }</span>

        <span class="cov0" title="0">return nil</span>
}

// rotateFile handles file rotation when the current file is full
func (w *MmapWriter) rotateFile() error <span class="cov8" title="1">{
        // Try to acquire rotation lock
        if !w.coordinationState.RotationInProgress.CompareAndSwap(0, 1) </span><span class="cov0" title="0">{
                // Another process is rotating, wait for it
                for w.coordinationState.RotationInProgress.Load() == 1 </span><span class="cov0" title="0">{
                        time.Sleep(1 * time.Millisecond)
                }</span>
                <span class="cov0" title="0">return nil</span>
        }
        <span class="cov8" title="1">defer w.coordinationState.RotationInProgress.Store(0)

        // Close current file
        w.mu.Lock()
        defer w.mu.Unlock()

        if w.mappedData != nil </span><span class="cov8" title="1">{
                syscall.Munmap(w.mappedData)
                w.mappedData = nil
        }</span>

        <span class="cov8" title="1">if w.dataFile != nil </span><span class="cov8" title="1">{
                w.dataFile.Close()
                w.dataFile = nil
        }</span>

        // Increment file index
        <span class="cov8" title="1">newIndex := w.coordinationState.ActiveFileIndex.Add(1)

        // Reset write offset
        w.coordinationState.WriteOffset.Store(0)
        w.coordinationState.FileSize.Store(0)

        // Open new file using standard naming convention
        w.dataPath = fmt.Sprintf("%s/log-%016d.comet", w.shardDir, newIndex)
        file, err := os.OpenFile(w.dataPath, os.O_CREATE|os.O_RDWR|os.O_EXCL, 0644)
        if err != nil </span><span class="cov0" title="0">{
                if os.IsExist(err) </span><span class="cov0" title="0">{
                        // Another process created it, just open it
                        file, err = os.OpenFile(w.dataPath, os.O_RDWR, 0644)
                        if err != nil </span><span class="cov0" title="0">{
                                return fmt.Errorf("failed to open existing file: %w", err)
                        }</span>
                } else<span class="cov0" title="0"> {
                        return fmt.Errorf("failed to create new file: %w", err)
                }</span>
        }

        // Initial size for new file - start small
        <span class="cov8" title="1">initialSize := int64(4096) // 4KB
        if err := file.Truncate(initialSize); err != nil </span><span class="cov0" title="0">{
                file.Close()
                return err
        }</span>

        <span class="cov8" title="1">w.dataFile = file
        w.coordinationState.FileSize.Store(initialSize)
        atomic.AddInt64(&amp;w.rotationCount, 1)

        // Update client metrics
        if w.metrics != nil </span><span class="cov8" title="1">{
                w.metrics.FileRotations.Add(1)
                w.metrics.TotalFiles.Add(1)
        }</span>

        // Note: Index updates are handled by the caller (shard) which holds the appropriate locks
        // We don't directly modify the index here to avoid race conditions

        // Map the new file
        <span class="cov8" title="1">return w.remapActiveWindow()</span>
}

// Sync ensures data is persisted to disk
func (w *MmapWriter) Sync() error <span class="cov0" title="0">{
        w.mu.Lock()
        defer w.mu.Unlock()

        // On macOS, use file sync instead of msync
        if w.dataFile != nil </span><span class="cov0" title="0">{
                return w.dataFile.Sync()
        }</span>

        <span class="cov0" title="0">return nil</span>
}

// Close cleans up resources
func (w *MmapWriter) Close() error <span class="cov8" title="1">{
        w.mu.Lock()
        defer w.mu.Unlock()

        // Unmap data
        if w.mappedData != nil </span><span class="cov8" title="1">{
                syscall.Munmap(w.mappedData)
                w.mappedData = nil
        }</span>

        // Unmap coordination state
        <span class="cov8" title="1">if w.coordinationData != nil </span><span class="cov8" title="1">{
                syscall.Munmap(w.coordinationData)
                w.coordinationData = nil
        }</span>

        // Close file
        <span class="cov8" title="1">if w.dataFile != nil </span><span class="cov8" title="1">{
                w.dataFile.Close()
                w.dataFile = nil
        }</span>

        <span class="cov8" title="1">return nil</span>
}
</pre>
		
		<pre class="file" id="file5" style="display: none">package comet

import (
        "encoding/binary"
        "fmt"
        "os"
        "strings"
        "sync"
        "sync/atomic"
        "syscall"

        "github.com/klauspost/compress/zstd"
)

// Reader provides memory-mapped read access to a shard
// Fields ordered for optimal memory alignment
type Reader struct {
        // Pointers first (8 bytes on 64-bit)
        decompressor *zstd.Decoder
        bufferPool   *sync.Pool

        // Slices (24 bytes: ptr + len + cap)
        files []*MappedFile

        // Mutex (platform-specific size)
        mu sync.RWMutex

        // Smaller fields last
        shardID uint32 // 4 bytes
        // 4 bytes padding
}

// AtomicSlice provides atomic access to a byte slice using atomic.Value
type AtomicSlice struct {
        value atomic.Value // Stores []byte
}

// Load atomically loads the slice
func (a *AtomicSlice) Load() []byte <span class="cov8" title="1">{
        if v := a.value.Load(); v != nil </span><span class="cov8" title="1">{
                return v.([]byte)
        }</span>
        <span class="cov0" title="0">return nil</span>
}

// Store atomically stores a new slice
func (a *AtomicSlice) Store(data []byte) <span class="cov8" title="1">{
        a.value.Store(data)
}</span>

// MappedFile represents a memory-mapped data file with atomic data updates
// Fields ordered for optimal memory alignment (embedded struct first)
type MappedFile struct {
        FileInfo             // Embedded struct (already aligned)
        data     AtomicSlice // Atomic slice for lock-free updates
        file     *os.File    // Pointer (8 bytes)
        remapMu  sync.Mutex  // Mutex for remapping operations only
        lastSize int64       // Last known size for growth detection
}

// NewReader creates a new reader for a shard
func NewReader(shardID uint32, index *ShardIndex) (*Reader, error) <span class="cov8" title="1">{
        r := &amp;Reader{
                shardID: shardID,
                files:   make([]*MappedFile, 0, len(index.Files)),
                bufferPool: &amp;sync.Pool{
                        New: func() any </span><span class="cov0" title="0">{
                                // Start with 64KB buffer, will grow as needed
                                return make([]byte, 0, 64*1024)
                        }</span>,
                },
        }

        // Create decompressor
        <span class="cov8" title="1">dec, err := zstd.NewReader(nil)
        if err != nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("failed to create decompressor: %w", err)
        }</span>
        <span class="cov8" title="1">r.decompressor = dec

        // Memory map all files
        for _, fileInfo := range index.Files </span><span class="cov8" title="1">{
                mapped, err := r.mapFile(fileInfo)
                if err != nil </span><span class="cov8" title="1">{
                        // Clean up already mapped files
                        r.Close()
                        return nil, fmt.Errorf("failed to map file %s: %w", fileInfo.Path, err)
                }</span>
                <span class="cov8" title="1">r.files = append(r.files, mapped)</span>
        }

        <span class="cov8" title="1">return r, nil</span>
}

// mapFile memory maps a single data file
func (r *Reader) mapFile(info FileInfo) (*MappedFile, error) <span class="cov8" title="1">{
        file, err := os.Open(info.Path)
        if err != nil </span><span class="cov8" title="1">{
                return nil, err
        }</span>

        <span class="cov8" title="1">stat, err := file.Stat()
        if err != nil </span><span class="cov0" title="0">{
                file.Close()
                return nil, err
        }</span>

        <span class="cov8" title="1">size := stat.Size()
        mappedFile := &amp;MappedFile{
                FileInfo: info,
                file:     file,
                lastSize: size,
        }

        if size == 0 </span><span class="cov0" title="0">{
                return mappedFile, nil
        }</span>

        // Memory map the file
        <span class="cov8" title="1">data, err := syscall.Mmap(int(file.Fd()), 0, int(size), syscall.PROT_READ, syscall.MAP_PRIVATE)
        if err != nil </span><span class="cov0" title="0">{
                file.Close()
                return nil, fmt.Errorf("mmap failed: %w", err)
        }</span>

        // Store data atomically
        <span class="cov8" title="1">mappedFile.data.Store(data)

        return mappedFile, nil</span>
}

// remapFile remaps a file that has grown - now lock-free for readers!
func (r *Reader) remapFile(fileIndex int) error <span class="cov8" title="1">{
        // Only validate file index under read lock
        r.mu.RLock()
        if fileIndex &lt; 0 || fileIndex &gt;= len(r.files) </span><span class="cov0" title="0">{
                r.mu.RUnlock()
                return fmt.Errorf("invalid file index %d", fileIndex)
        }</span>
        <span class="cov8" title="1">mappedFile := r.files[fileIndex]
        r.mu.RUnlock()

        // Use per-file mutex to prevent concurrent remaps of the same file
        // This doesn't block readers of other files or readers using the current mapping
        mappedFile.remapMu.Lock()
        defer mappedFile.remapMu.Unlock()

        // Get current file size
        stat, err := mappedFile.file.Stat()
        if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to stat file: %w", err)
        }</span>

        <span class="cov8" title="1">newSize := stat.Size()

        // Check if we've already been remapped by another goroutine
        if newSize &lt;= mappedFile.lastSize </span><span class="cov0" title="0">{
                return nil // No growth or already remapped
        }</span>

        // Get current data atomically
        <span class="cov8" title="1">oldData := mappedFile.data.Load()
        if oldData != nil &amp;&amp; int64(len(oldData)) &gt;= newSize </span><span class="cov0" title="0">{
                return nil // Already remapped by another goroutine
        }</span>

        // Create new mapping with the current file size
        <span class="cov8" title="1">newData, err := syscall.Mmap(int(mappedFile.file.Fd()), 0, int(newSize), syscall.PROT_READ, syscall.MAP_PRIVATE)
        if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to remap file: %w", err)
        }</span>

        // Atomically update the data pointer - readers will see either old or new mapping
        <span class="cov8" title="1">mappedFile.data.Store(newData)
        mappedFile.lastSize = newSize

        // Unmap old data if it exists
        // Safe to unmap immediately because:
        // 1. We use MAP_PRIVATE (copy-on-write) which protects active readers
        // 2. The atomic.Value ensures readers see either old or new mapping
        // 3. Any active readers have their own memory pages via COW
        if oldData != nil </span><span class="cov8" title="1">{
                go func() </span><span class="cov8" title="1">{
                        // Defer unmapping to avoid blocking the current operation
                        syscall.Munmap(oldData)
                }</span>()
        }

        <span class="cov8" title="1">return nil</span>
}

// ReadEntryAtPosition reads a single entry at the given position
func (r *Reader) ReadEntryAtPosition(pos EntryPosition) ([]byte, error) <span class="cov8" title="1">{
        r.mu.RLock()

        // Validate file index
        if pos.FileIndex &lt; 0 || pos.FileIndex &gt;= len(r.files) </span><span class="cov0" title="0">{
                r.mu.RUnlock()
                return nil, fmt.Errorf("invalid file index %d", pos.FileIndex)
        }</span>

        <span class="cov8" title="1">targetFile := r.files[pos.FileIndex]
        r.mu.RUnlock() // Release read lock early - we'll use atomic operations

        // Check if we need to remap the file due to growth
        currentData := targetFile.data.Load()
        if targetFile.file != nil </span><span class="cov8" title="1">{
                stat, err := targetFile.file.Stat()
                if err == nil &amp;&amp; (currentData == nil || stat.Size() &gt; int64(len(currentData))) </span><span class="cov8" title="1">{
                        // Remap the file with the new size (lock-free!)
                        if err := r.remapFile(pos.FileIndex); err != nil </span><span class="cov0" title="0">{
                                return nil, fmt.Errorf("failed to remap grown file: %w", err)
                        }</span>
                        // Get the potentially updated mapping
                        <span class="cov8" title="1">currentData = targetFile.data.Load()</span>
                }
        }

        // Read from the current mapping (no locks needed!)
        <span class="cov8" title="1">data, err := r.readEntryFromFileData(currentData, pos.ByteOffset)
        if err != nil &amp;&amp; strings.Contains(err.Error(), "extends beyond file") </span><span class="cov8" title="1">{
                // Handle the case where index was updated but data hasn't been flushed yet
                // This can occur during active writes - remap and retry once
                if targetFile.file != nil </span><span class="cov8" title="1">{
                        if stat, statErr := targetFile.file.Stat(); statErr == nil </span><span class="cov8" title="1">{
                                currentSize := stat.Size()
                                if currentSize &gt; int64(len(currentData)) </span><span class="cov0" title="0">{
                                        // File has grown, remap and retry
                                        if remapErr := r.remapFile(pos.FileIndex); remapErr == nil </span><span class="cov0" title="0">{
                                                currentData = targetFile.data.Load()
                                                return r.readEntryFromFileData(currentData, pos.ByteOffset)
                                        }</span>
                                }
                        }
                }
        }
        <span class="cov8" title="1">return data, err</span>
}

// ReadEntry reads a single entry at the given byte offset (legacy method)
func (r *Reader) ReadEntry(offset int64) ([]byte, error) <span class="cov0" title="0">{
        r.mu.RLock()
        defer r.mu.RUnlock()

        // Find the file containing this offset
        var targetFile *MappedFile
        for i := range r.files </span><span class="cov0" title="0">{
                file := r.files[i]
                if offset &gt;= file.StartOffset &amp;&amp; offset &lt; file.EndOffset </span><span class="cov0" title="0">{
                        targetFile = file
                        break</span>
                }
        }

        <span class="cov0" title="0">if targetFile == nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("offset %d not found in any file", offset)
        }</span>

        // Calculate position within the file
        <span class="cov0" title="0">fileOffset := offset - targetFile.StartOffset

        // Get data atomically
        data := targetFile.data.Load()
        if fileOffset+headerSize &gt; int64(len(data)) </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("invalid offset: header extends beyond file")
        }</span>

        // Read header
        <span class="cov0" title="0">header := data[fileOffset : fileOffset+headerSize]
        length := binary.LittleEndian.Uint32(header[0:4])

        // Check bounds
        dataStart := fileOffset + headerSize
        dataEnd := dataStart + int64(length)
        if dataEnd &gt; int64(len(data)) </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("invalid entry: data extends beyond file")
        }</span>

        // Read compressed data
        <span class="cov0" title="0">compressedData := data[dataStart:dataEnd]

        // Get buffer from pool for decompression
        buf := r.bufferPool.Get().([]byte)
        defer func() </span><span class="cov0" title="0">{
                // Reset buffer and return to pool
                buf = buf[:0]
                r.bufferPool.Put(buf) //lint:ignore SA6002 sync.Pool.Put expects interface{}, slice is correct
        }</span>()

        // Check if data is compressed by looking for zstd magic number
        <span class="cov0" title="0">if len(compressedData) &gt;= 4 </span><span class="cov0" title="0">{
                // Try to decompress - if it fails, assume data is uncompressed
                decompressed, err := r.decompressor.DecodeAll(compressedData, buf)
                if err == nil </span><span class="cov0" title="0">{
                        // Successfully decompressed
                        result := make([]byte, len(decompressed))
                        copy(result, decompressed)
                        return result, nil
                }</span>
        }

        // Data is not compressed - return as is
        <span class="cov0" title="0">result := make([]byte, len(compressedData))
        copy(result, compressedData)
        return result, nil</span>
}

// ReadRange reads entries in a byte range
func (r *Reader) ReadRange(startOffset, endOffset int64, callback func(offset int64, data []byte) error) error <span class="cov0" title="0">{
        r.mu.RLock()
        defer r.mu.RUnlock()

        offset := startOffset
        for offset &lt; endOffset </span><span class="cov0" title="0">{
                // Find the file containing this offset
                var targetFile *MappedFile
                for i := range r.files </span><span class="cov0" title="0">{
                        file := r.files[i]
                        if offset &gt;= file.StartOffset &amp;&amp; (i == len(r.files)-1 || offset &lt; r.files[i+1].StartOffset) </span><span class="cov0" title="0">{
                                targetFile = file
                                break</span>
                        }
                }

                <span class="cov0" title="0">if targetFile == nil </span><span class="cov0" title="0">{
                        break</span> // No more data
                }

                // Read entries from this file
                <span class="cov0" title="0">fileOffset := offset - targetFile.StartOffset
                targetData := targetFile.data.Load() // Get data atomically

                for fileOffset &lt; int64(len(targetData)) &amp;&amp; offset &lt; endOffset </span><span class="cov0" title="0">{
                        // Check if we have enough data for header
                        if fileOffset+headerSize &gt; int64(len(targetData)) </span><span class="cov0" title="0">{
                                break</span>
                        }

                        // Read header
                        <span class="cov0" title="0">header := targetData[fileOffset : fileOffset+headerSize]
                        length := binary.LittleEndian.Uint32(header[0:4])

                        // Check bounds
                        dataStart := fileOffset + headerSize
                        dataEnd := dataStart + int64(length)
                        if dataEnd &gt; int64(len(targetData)) </span><span class="cov0" title="0">{
                                break</span>
                        }

                        // Read and decompress data
                        <span class="cov0" title="0">compressedData := targetData[dataStart:dataEnd]

                        // Get buffer from pool for decompression
                        buf := r.bufferPool.Get().([]byte)

                        var dataToCallback []byte
                        if len(compressedData) &gt;= 4 </span><span class="cov0" title="0">{
                                // Try to decompress - if it fails, assume data is uncompressed
                                decompressed, err := r.decompressor.DecodeAll(compressedData, buf)
                                if err == nil </span><span class="cov0" title="0">{
                                        dataToCallback = decompressed
                                }</span> else<span class="cov0" title="0"> {
                                        dataToCallback = compressedData
                                }</span>
                        } else<span class="cov0" title="0"> {
                                dataToCallback = compressedData
                        }</span>

                        // Call callback
                        <span class="cov0" title="0">callbackErr := callback(offset, dataToCallback)

                        // Return buffer to pool
                        buf = buf[:0]
                        r.bufferPool.Put(buf) //lint:ignore SA6002 sync.Pool.Put expects interface{}, slice is correct

                        if callbackErr != nil </span><span class="cov0" title="0">{
                                return callbackErr
                        }</span>

                        // Move to next entry
                        <span class="cov0" title="0">fileOffset = dataEnd
                        offset = targetFile.StartOffset + fileOffset</span>
                }
        }

        <span class="cov0" title="0">return nil</span>
}

// Close unmaps all files and cleans up resources
func (r *Reader) Close() error <span class="cov8" title="1">{
        r.mu.Lock()
        defer r.mu.Unlock()

        var firstErr error
        for i := range r.files </span><span class="cov8" title="1">{
                file := r.files[i]

                // Unmap if mapped
                data := file.data.Load()
                if data != nil </span><span class="cov8" title="1">{
                        if err := syscall.Munmap(data); err != nil &amp;&amp; firstErr == nil </span><span class="cov0" title="0">{
                                firstErr = err
                        }</span>
                }

                // Close file
                <span class="cov8" title="1">if file.file != nil </span><span class="cov8" title="1">{
                        if err := file.file.Close(); err != nil &amp;&amp; firstErr == nil </span><span class="cov0" title="0">{
                                firstErr = err
                        }</span>
                }
        }

        <span class="cov8" title="1">return firstErr</span>
}

// readEntryFromFileData reads a single entry from memory-mapped data at a byte offset
func (r *Reader) readEntryFromFileData(data []byte, byteOffset int64) ([]byte, error) <span class="cov8" title="1">{
        if len(data) == 0 </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("file not memory mapped")
        }</span>

        <span class="cov8" title="1">if byteOffset+headerSize &gt; int64(len(data)) </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("invalid offset: header extends beyond file")
        }</span>

        // Read header
        <span class="cov8" title="1">header := data[byteOffset : byteOffset+headerSize]
        length := binary.LittleEndian.Uint32(header[0:4])

        // Check bounds
        dataStart := byteOffset + headerSize
        dataEnd := dataStart + int64(length)
        if dataEnd &gt; int64(len(data)) </span><span class="cov8" title="1">{
                return nil, fmt.Errorf("invalid entry: data extends beyond file")
        }</span>

        // Read entry data
        <span class="cov8" title="1">entryData := data[dataStart:dataEnd]

        // Check if data is compressed by looking for zstd magic number
        if len(entryData) &gt;= 4 &amp;&amp; binary.LittleEndian.Uint32(entryData[0:4]) == 0xFD2FB528 </span><span class="cov8" title="1">{
                // Data is compressed - decompress it
                decompressed, err := r.decompressor.DecodeAll(entryData, nil)
                if err != nil </span><span class="cov0" title="0">{
                        return nil, fmt.Errorf("failed to decompress: %w", err)
                }</span>
                <span class="cov8" title="1">return decompressed, nil</span>
        }

        // Data is not compressed - return as is
        <span class="cov8" title="1">return entryData, nil</span>
}
</pre>
		
		<pre class="file" id="file6" style="display: none">package comet

import (
        "maps"
        "os"
        "sort"
        "sync/atomic"
        "time"
)

// RetentionStats provides type-safe retention statistics
// Fields ordered for optimal memory alignment
type RetentionStats struct {
        // 8-byte aligned fields first
        TotalSizeBytes int64         `json:"total_size_bytes"`
        TotalSizeGB    float64       `json:"total_size_gb"`
        MaxTotalSizeGB float64       `json:"max_total_size_gb"`
        RetentionAge   time.Duration `json:"retention_age"`
        TotalFiles     int           `json:"total_files"`

        // Pointer fields (8 bytes)
        ShardStats map[uint32]ShardRetentionStats `json:"shard_stats,omitempty"`

        // Larger composite types last (time.Time is 24 bytes)
        OldestData time.Time `json:"oldest_data"`
        NewestData time.Time `json:"newest_data"`
}

// ShardRetentionStats provides retention stats for a single shard
// Fields ordered for optimal memory alignment
type ShardRetentionStats struct {
        // 8-byte aligned fields first
        SizeBytes int64 `json:"size_bytes"`
        Files     int   `json:"files"`

        // Pointer field (8 bytes)
        ConsumerLag map[string]int64 `json:"consumer_lag,omitempty"`

        // Larger composite types last (time.Time is 24 bytes)
        OldestEntry time.Time `json:"oldest_entry"`
        NewestEntry time.Time `json:"newest_entry"`
}

// startRetentionManager starts the background retention process
func (c *Client) startRetentionManager() <span class="cov8" title="1">{
        if c.config.Retention.CleanupInterval &lt;= 0 </span><span class="cov8" title="1">{
                // Retention disabled
                return
        }</span>

        <span class="cov8" title="1">c.retentionWg.Add(1)
        go func() </span><span class="cov8" title="1">{
                defer c.retentionWg.Done()

                ticker := time.NewTicker(c.config.Retention.CleanupInterval)
                defer ticker.Stop()

                // Run initial cleanup after short delay
                initialTimer := time.NewTimer(10 * time.Second)
                select </span>{
                case &lt;-initialTimer.C:<span class="cov0" title="0">
                        c.runRetentionCleanup()</span>
                case &lt;-c.stopCh:<span class="cov8" title="1">
                        initialTimer.Stop()
                        return</span>
                }

                // Run periodic cleanup
                <span class="cov0" title="0">for </span><span class="cov0" title="0">{
                        select </span>{
                        case &lt;-ticker.C:<span class="cov0" title="0">
                                c.runRetentionCleanup()</span>
                        case &lt;-c.stopCh:<span class="cov0" title="0">
                                return</span>
                        }
                }
        }()
}

// runRetentionCleanup performs a single cleanup pass
func (c *Client) runRetentionCleanup() <span class="cov0" title="0">{
        c.mu.RLock()
        shards := make([]*Shard, 0, len(c.shards))
        for _, shard := range c.shards </span><span class="cov0" title="0">{
                shards = append(shards, shard)
        }</span>
        <span class="cov0" title="0">c.mu.RUnlock()

        var totalSize int64
        for _, shard := range shards </span><span class="cov0" title="0">{
                size := c.cleanupShard(shard)
                totalSize += size
        }</span>

        // Check total size limit
        <span class="cov0" title="0">if c.config.Retention.MaxTotalSize &gt; 0 &amp;&amp; totalSize &gt; c.config.Retention.MaxTotalSize </span><span class="cov0" title="0">{
                // Need to delete more files to get under total limit
                c.enforceGlobalSizeLimit(shards, totalSize)
        }</span>
}

// cleanupShard cleans up old files in a single shard
func (c *Client) cleanupShard(shard *Shard) int64 <span class="cov0" title="0">{
        shard.mu.RLock()
        files := make([]FileInfo, len(shard.index.Files))
        copy(files, shard.index.Files)
        currentFile := shard.index.CurrentFile
        consumerOffsets := make(map[string]int64)
        maps.Copy(consumerOffsets, shard.index.ConsumerOffsets)
        shard.mu.RUnlock()

        // Calculate current state
        var shardSize int64
        var oldestProtectedEntry int64 = -1
        now := time.Now()

        // Find oldest consumer position if protecting unconsumed
        if c.config.Retention.ProtectUnconsumed </span><span class="cov0" title="0">{
                for _, offset := range consumerOffsets </span><span class="cov0" title="0">{
                        if oldestProtectedEntry == -1 || offset &lt; oldestProtectedEntry </span><span class="cov0" title="0">{
                                oldestProtectedEntry = offset
                        }</span>
                }
        }

        // Analyze files for deletion
        <span class="cov0" title="0">filesToDelete := []FileInfo{}
        filesToKeep := []FileInfo{}

        for i, file := range files </span><span class="cov0" title="0">{
                fileSize := file.EndOffset - file.StartOffset
                shardSize += fileSize

                // Never delete the current file
                if file.Path == currentFile </span><span class="cov0" title="0">{
                        filesToKeep = append(filesToKeep, file)
                        continue</span>
                }

                // Check if we should delete this file
                <span class="cov0" title="0">shouldDelete := false

                // Time-based deletion
                if c.config.Retention.MaxAge &gt; 0 &amp;&amp; now.Sub(file.EndTime) &gt; c.config.Retention.MaxAge </span><span class="cov0" title="0">{
                        shouldDelete = true
                }</span>

                // Force delete after time
                <span class="cov0" title="0">if c.config.Retention.ForceDeleteAfter &gt; 0 &amp;&amp; now.Sub(file.EndTime) &gt; c.config.Retention.ForceDeleteAfter </span><span class="cov0" title="0">{
                        shouldDelete = true
                        oldestProtectedEntry = -1 // Ignore consumer protection
                }</span>

                // Check if file has active readers
                <span class="cov0" title="0">if shouldDelete &amp;&amp; atomic.LoadInt64(&amp;shard.readerCount) &gt; 0 </span><span class="cov0" title="0">{
                        // Skip files that might have active readers
                        // This is conservative - we could track per-file readers for more precision
                        if i == 0 || i == len(files)-1 </span><span class="cov0" title="0">{
                                shouldDelete = false
                        }</span>
                }

                // Check consumer protection
                <span class="cov0" title="0">if shouldDelete &amp;&amp; oldestProtectedEntry &gt;= 0 </span><span class="cov0" title="0">{
                        // Check if any consumer still needs this file
                        fileLastEntry := file.StartEntry + file.Entries - 1
                        if fileLastEntry &gt;= oldestProtectedEntry </span><span class="cov0" title="0">{
                                shouldDelete = false
                        }</span>
                }

                // Enforce minimum files
                <span class="cov0" title="0">if shouldDelete &amp;&amp; len(filesToKeep) &lt; c.config.Retention.MinFilesToKeep </span><span class="cov0" title="0">{
                        shouldDelete = false
                }</span>

                <span class="cov0" title="0">if shouldDelete </span><span class="cov0" title="0">{
                        filesToDelete = append(filesToDelete, file)
                }</span> else<span class="cov0" title="0"> {
                        filesToKeep = append(filesToKeep, file)
                }</span>
        }

        // Size-based deletion (if we're over the shard limit)
        <span class="cov0" title="0">if c.config.Retention.MaxShardSize &gt; 0 &amp;&amp; shardSize &gt; c.config.Retention.MaxShardSize </span><span class="cov0" title="0">{
                // Sort kept files by age (oldest first)
                sort.Slice(filesToKeep, func(i, j int) bool </span><span class="cov0" title="0">{
                        return filesToKeep[i].EndTime.Before(filesToKeep[j].EndTime)
                }</span>)

                <span class="cov0" title="0">targetSize := shardSize
                for i := 0; i &lt; len(filesToKeep) &amp;&amp; targetSize &gt; c.config.Retention.MaxShardSize; i++ </span><span class="cov0" title="0">{
                        file := filesToKeep[i]

                        // Skip current file and minimum required files
                        if file.Path == currentFile || len(filesToKeep)-i &lt;= c.config.Retention.MinFilesToKeep </span><span class="cov0" title="0">{
                                continue</span>
                        }

                        // Move to delete list
                        <span class="cov0" title="0">filesToDelete = append(filesToDelete, file)
                        targetSize -= (file.EndOffset - file.StartOffset)

                        // Remove from keep list
                        filesToKeep = append(filesToKeep[:i], filesToKeep[i+1:]...)
                        i--</span> // Adjust index after removal
                }
        }

        // Actually delete the files
        <span class="cov0" title="0">if len(filesToDelete) &gt; 0 </span><span class="cov0" title="0">{
                c.deleteFiles(shard, filesToDelete, &amp;c.metrics)
        }</span>

        // Return remaining size
        <span class="cov0" title="0">var remainingSize int64
        for _, file := range filesToKeep </span><span class="cov0" title="0">{
                remainingSize += file.EndOffset - file.StartOffset
        }</span>
        <span class="cov0" title="0">return remainingSize</span>
}

// deleteFiles removes files from disk and updates the shard index
// CRITICAL: Updates index BEFORE deleting files to prevent readers from accessing deleted files
func (c *Client) deleteFiles(shard *Shard, files []FileInfo, metrics *ClientMetrics) <span class="cov0" title="0">{
        if len(files) == 0 </span><span class="cov0" title="0">{
                return
        }</span>

        // STEP 1: Update the shard index FIRST to prevent readers from accessing files we're about to delete
        <span class="cov0" title="0">shard.mu.Lock()

        // Create a map of files to delete for quick lookup
        deletedMap := make(map[string]bool)
        for _, file := range files </span><span class="cov0" title="0">{
                deletedMap[file.Path] = true
        }</span>

        // Filter out files to be deleted from the index
        <span class="cov0" title="0">newFiles := make([]FileInfo, 0, len(shard.index.Files))
        for _, file := range shard.index.Files </span><span class="cov0" title="0">{
                if !deletedMap[file.Path] </span><span class="cov0" title="0">{
                        newFiles = append(newFiles, file)
                }</span>
        }

        <span class="cov0" title="0">shard.index.Files = newFiles
        shard.mu.Unlock()

        // STEP 2: Now delete the physical files - readers can no longer find them in the index
        deletedCount := 0
        for _, file := range files </span><span class="cov0" title="0">{
                err := os.Remove(file.Path)
                if err != nil &amp;&amp; !os.IsNotExist(err) </span><span class="cov0" title="0">{
                        // Log error but continue - file may have been deleted by another process
                        continue</span>
                }
                <span class="cov0" title="0">deletedCount++</span>
        }

        // Update metrics (using existing TotalFiles counter)
        <span class="cov0" title="0">if metrics != nil &amp;&amp; deletedCount &gt; 0 </span>{<span class="cov0" title="0">
                // Note: TotalFiles tracks total files created, not current count
                // Could add a separate metric for files deleted if needed
        }</span>

        // Clean up entry boundaries for deleted files
        // Clean up binary index to remove entries that reference deleted files
        <span class="cov0" title="0">if len(files) &gt; 0 </span><span class="cov0" title="0">{
                minDeletedEntry := files[0].StartEntry

                // Filter binary index nodes
                newNodes := make([]EntryIndexNode, 0)
                for _, node := range shard.index.BinaryIndex.Nodes </span><span class="cov0" title="0">{
                        if node.EntryNumber &lt; minDeletedEntry </span><span class="cov0" title="0">{
                                newNodes = append(newNodes, node)
                        }</span>
                }
                <span class="cov0" title="0">shard.index.BinaryIndex.Nodes = newNodes</span>
        }

        // Persist the updated index
        <span class="cov0" title="0">shard.persistIndex()

        // Update metrics
        if metrics != nil </span><span class="cov0" title="0">{
                metrics.FileRotations.Add(uint64(deletedCount))
        }</span>
}

// enforceGlobalSizeLimit deletes files across all shards to meet total size limit
func (c *Client) enforceGlobalSizeLimit(shards []*Shard, currentTotal int64) <span class="cov0" title="0">{
        if currentTotal &lt;= c.config.Retention.MaxTotalSize </span><span class="cov0" title="0">{
                return
        }</span>

        // Collect all files from all shards with their metadata
        <span class="cov0" title="0">type FileWithShard struct {
                shard *Shard
                file  FileInfo
        }

        var allFiles []FileWithShard
        for _, shard := range shards </span><span class="cov0" title="0">{
                shard.mu.RLock()
                for _, file := range shard.index.Files </span><span class="cov0" title="0">{
                        if file.Path != shard.index.CurrentFile </span><span class="cov0" title="0">{
                                allFiles = append(allFiles, FileWithShard{shard: shard, file: file})
                        }</span>
                }
                <span class="cov0" title="0">shard.mu.RUnlock()</span>
        }

        // Sort by age (oldest first)
        <span class="cov0" title="0">sort.Slice(allFiles, func(i, j int) bool </span><span class="cov0" title="0">{
                return allFiles[i].file.EndTime.Before(allFiles[j].file.EndTime)
        }</span>)

        // Delete oldest files until we're under the limit
        <span class="cov0" title="0">bytesToDelete := currentTotal - c.config.Retention.MaxTotalSize
        deletionMap := make(map[*Shard][]FileInfo)

        for _, fw := range allFiles </span><span class="cov0" title="0">{
                if bytesToDelete &lt;= 0 </span><span class="cov0" title="0">{
                        break</span>
                }

                <span class="cov0" title="0">fileSize := fw.file.EndOffset - fw.file.StartOffset
                deletionMap[fw.shard] = append(deletionMap[fw.shard], fw.file)
                bytesToDelete -= fileSize</span>
        }

        // Perform deletions
        <span class="cov0" title="0">for shard, files := range deletionMap </span><span class="cov0" title="0">{
                c.deleteFiles(shard, files, &amp;c.metrics)
        }</span>
}

// GetRetentionStats returns current retention statistics
func (c *Client) GetRetentionStats() *RetentionStats <span class="cov8" title="1">{
        stats := &amp;RetentionStats{
                RetentionAge:   c.config.Retention.MaxAge,
                MaxTotalSizeGB: float64(c.config.Retention.MaxTotalSize) / (1 &lt;&lt; 30),
                ShardStats:     make(map[uint32]ShardRetentionStats),
        }

        c.mu.RLock()
        defer c.mu.RUnlock()

        for shardID, shard := range c.shards </span><span class="cov8" title="1">{
                shard.mu.RLock()

                shardStat := ShardRetentionStats{
                        Files:       len(shard.index.Files),
                        ConsumerLag: make(map[string]int64),
                }

                // Calculate shard size and time range
                for _, file := range shard.index.Files </span><span class="cov8" title="1">{
                        size := file.EndOffset - file.StartOffset
                        shardStat.SizeBytes += size
                        stats.TotalSizeBytes += size

                        if shardStat.OldestEntry.IsZero() || file.StartTime.Before(shardStat.OldestEntry) </span><span class="cov8" title="1">{
                                shardStat.OldestEntry = file.StartTime
                        }</span>
                        // For the current file, use current time as end time
                        <span class="cov8" title="1">endTime := file.EndTime
                        if file.Path == shard.index.CurrentFile &amp;&amp; (endTime.IsZero() || endTime.Before(file.StartTime)) </span><span class="cov8" title="1">{
                                endTime = time.Now()
                        }</span>

                        <span class="cov8" title="1">if endTime.After(shardStat.NewestEntry) </span><span class="cov8" title="1">{
                                shardStat.NewestEntry = endTime
                        }</span>

                        // Update global oldest/newest
                        <span class="cov8" title="1">if stats.OldestData.IsZero() || file.StartTime.Before(stats.OldestData) </span><span class="cov8" title="1">{
                                stats.OldestData = file.StartTime
                        }</span>
                        <span class="cov8" title="1">if endTime.After(stats.NewestData) </span><span class="cov8" title="1">{
                                stats.NewestData = endTime
                        }</span>
                }

                // Calculate consumer lag for this shard
                <span class="cov8" title="1">for group, offset := range shard.index.ConsumerOffsets </span><span class="cov0" title="0">{
                        lag := shard.index.CurrentEntryNumber - offset
                        if lag &gt; 0 </span><span class="cov0" title="0">{
                                shardStat.ConsumerLag[group] = lag
                        }</span>
                }

                <span class="cov8" title="1">stats.TotalFiles += shardStat.Files
                stats.ShardStats[shardID] = shardStat

                shard.mu.RUnlock()</span>
        }

        <span class="cov8" title="1">stats.TotalSizeGB = float64(stats.TotalSizeBytes) / (1 &lt;&lt; 30)

        return stats</span>
}
</pre>
		
		</div>
	</body>
	<script>
	(function() {
		var files = document.getElementById('files');
		var visible;
		files.addEventListener('change', onChange, false);
		function select(part) {
			if (visible)
				visible.style.display = 'none';
			visible = document.getElementById(part);
			if (!visible)
				return;
			files.value = part;
			visible.style.display = 'block';
			location.hash = part;
		}
		function onChange() {
			select(files.value);
			window.scrollTo(0, 0);
		}
		if (location.hash != "") {
			select(location.hash.substr(1));
		}
		if (!visible) {
			select("file0");
		}
	})();
	</script>
</html>
